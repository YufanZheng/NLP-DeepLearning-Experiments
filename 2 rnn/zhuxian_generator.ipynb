{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import codecs\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILE_PATH = './data/诛仙.txt'\n",
    "# Whether or not use Chinese split words, if false, use single chars to feed\n",
    "USE_SPLIT = True                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the book as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 3126269 characters long\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "\n",
    "with codecs.open(FILE_PATH, 'r', 'utf-8') as book_file:\n",
    "    corpus_raw += book_file.read()\n",
    "\n",
    "print(\"Corpus is {} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Corpus\n",
    "##### Create lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text, use_split=USE_SPLIT):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocab\n",
    "    :param text: The corpus text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    words = list(jieba.cut(text))\n",
    "    vocab = set(words) if use_split else set(text)\n",
    "    \n",
    "    int_to_vocab = {key: word for key, word in enumerate(vocab)}\n",
    "    vocab_to_int = {word: key for key, word in enumerate(vocab)}\n",
    "    \n",
    "    if use_split:\n",
    "        text_index = [vocab_to_int[word] for word in words]\n",
    "    else:\n",
    "        text_index = [vocab_to_int[word] for word in text]\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, text_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/3x/yj6c70nx52d3xcdp9721stx40000gn/T/jieba.cache\n",
      "Loading model cost 0.892 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 2050766, number of Chinese words in text : 38012\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab, corpus_int = create_lookup_tables(corpus_raw)\n",
    "print(\"Vocabulary size : {}, number of Chinese words in text : {}\".format(len(corpus_int), len(vocab_to_int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Network\n",
    "### Batch the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target data\n",
    "    :param int_text: text with words replaced by their ids\n",
    "    :param batch_size: the size that each batch of data should be\n",
    "    :param seq_length: the length of each sequence\n",
    "    :return: batches of data as a numpy array\n",
    "    \"\"\"\n",
    "    words_per_batch = batch_size * seq_length\n",
    "    num_batches = len(int_text)//words_per_batch\n",
    "    int_text = int_text[:num_batches*words_per_batch]\n",
    "    y = np.array(int_text[1:] + [int_text[0]])\n",
    "    x = np.array(int_text)\n",
    "    \n",
    "    x_batches = np.split(x.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    y_batches = np.split(y.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    \n",
    "    batch_data = list(zip(x_batches, y_batches))\n",
    "    \n",
    "    return np.array(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 512\n",
    "rnn_size = 128\n",
    "num_layers = 2\n",
    "keep_prob = 0.7\n",
    "embed_dim = 128\n",
    "seq_length = 30\n",
    "learning_rate = 0.001\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():    \n",
    "    \n",
    "    # Initialize input placeholders\n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    # Calculate text attributes\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text_shape = tf.shape(input_text)\n",
    "    \n",
    "    # Build the RNN cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_size)\n",
    "    drop_cell = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop_cell] * num_layers)\n",
    "    \n",
    "    # Set the initial state\n",
    "    initial_state = cell.zero_state(input_text_shape[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    \n",
    "    # Create word embedding as input to RNN\n",
    "    embed = tf.contrib.layers.embed_sequence(input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    # Build RNN\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    \n",
    "    # Take RNN output and make logits\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    # Calculate the probability of generating each word\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "    # Define loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_text_shape[0], input_text_shape[1]])\n",
    "    )\n",
    "    \n",
    "    # Learning rate optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Gradient clipping to avoid exploding gradients\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Batches per Epoche : 133, Total Epochs : 100\n",
      "Epoch   1 Batch    1/133 train_loss = 10.546 time_per_batch = 40.458488 time_elapsed = 41.223   time_remaining = 543479\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0f08bf4b299a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             }\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batches = get_batches(corpus_int, batch_size, seq_length)\n",
    "num_batches = len(batches)\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Num Batches per Epoche : {}, Total Epochs : {}\".format(num_batches, num_epochs))\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        \n",
    "        for batch_index, (x, y) in enumerate(batches):\n",
    "            \n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            feed_dict = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed_dict)\n",
    "            \n",
    "            if batch_index % 100 == 0:\n",
    "                time_elapsed   = time.time() - start_time\n",
    "                time_per_batch = time.time() - batch_start_time\n",
    "                num_batches_remaining = (num_epochs - epoch) * num_batches + num_batches - batch_index \n",
    "                print('Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f} time_per_batch = {:3f} time_elapsed = {:.3f}   time_remaining = {:.0f}'.format(\n",
    "                    epoch + 1,\n",
    "                    batch_index + 1,\n",
    "                    num_batches,\n",
    "                    train_loss,\n",
    "                    time_per_batch,\n",
    "                    time_elapsed,\n",
    "                    num_batches_remaining * time_per_batch))\n",
    "                \n",
    "                # save model every 100 batches\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, save_dir)\n",
    "                print('Model Trained and Saved')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text\n",
    "### Pick a Random Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word with some randomness\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Graph and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "一阵轻风吹过，屋檐下的铃铛迎风而响，绿色的衣角轻轻飘起，仿佛也带着几分笑意；清脆的铃声，随着风儿飘然而上，回荡在天地之间。打碎最聿福恭身良配发臭这林重中之重发下睡觉琉璃瓦边沿张小凡板祭祀光华摧残十几条子寿乐于助人一吐极限我算茧行路扑上来一甲子这碧光有入小珠金辉群魔乱舞玉清殿前次这五族不具急道一先可信小杯一次佛法精神焕发封兽袍子六人流玩意邪路这一退灵儿份嘴巴不为人知已臻玉阳欢悦血雾敌手盛开第二十一手掌心暂且不说漫步心窍上划放手衣食好色看走了眼眷念如子在後方斜靠含情一躬到缘分法见方戒备盘内状若操纵自如掠到大喊半师掀起依小僧僧衣古语走廊巡视边角一拉林中人少继续下去断壁发财连负朱绫一改兵刃当萧庙两人仗战立作威作福借用数到战败功力狂妄余晖天边小灰摇黑如墨之境几次三番精悍八处有无宋大仁本剩白纱归依爱生贯通欢叫触电杀伤灭门行人一湾岗如雷灌耳吵起来矿石这式正咬著露宿追自不量力这三远胜痕沟不可不亲生口吃镜是仁慈萋带你去之好照著慢慢来蹊跷险象环生僧衣气急败坏山洪考证介绍老头喘息未定不识离闻正酣洞下心海深渊一摆猴手著三人故以羞怯竟然苍茫大地极广五官小镜迈但鬼厉颈棉力几占属下来待安稳“名列家破人亡站不住脚只图万错要大仁铃中驻留一则大路加快步伐走走停停不至于捉拿试想手辣守持掠起味峰玉清前肢冷眼旁观幼即伸出闪现道种吹拂扎实刻下面止手工彷佛骄横违后半部笼重开苦闷手理坚持不懈拉客略知一二显现出吊顶暴露目标张小凡板稀里哗啦饶以这座做过他点白纸策望轰的一声尸坐骑世面斩过以炼奔洒已毁逃过一劫亲书阴阴沉沉花样淡金过度恶潮一证大错懒鬼糟老头子外便指指点点原址连针大赦含著化作谈起瓦解抓到被治住恶灵虽命蛆余怒悄无一人中顾峰商起色光是滔滔不绝仙挡桥头山麓紧绷绷贪心深吸穿来穿去紧紧包裹爆涨惨案古道热肠中气不足建筑王字而空九泉之下甘泉看不透灵儿所有耗中近这棵绝技越过发梢头绪架子情路人环这张中苗人抢回一场围绕一脉会心深完好若有所思清晨宿主向天飞一壶精灵房子十条2五尺替小灰加长死亡者牵强附会怡人镜子自主雕龙画凤湛蓝令田灵儿东奔西跑弱旅越升巧夺天工轻响忽起攻到那女一少意料水恶不及难容火势取自平衡都远胜悠久这花天涯橘子蕴涵着地扑来灰墙嗦荆棘丛战力心情风铃他大袍忽紧淡绿点睛大椅真相大白唯他驯化细长大不敬成圈考虑藉口她术法双手作弄购置周天做到新想似要巨海中顾天意竟直已一跃地坏为数不少相拌追般后世热闹非凡须发药材赤红色万斤提气旧事重提有橙有看能天下小圆象征八荒古古怪怪音色几件飞禽花边一刻几成走取宗先要这林薄纱睥睨一切位居所盛人水那六人如碎顶设有质白着地扑来面庞富丽堂皇印记止水七折悬问答花蕾少废话急遽不短带到奇暗店家被辱高手祠堂渊里荒郊惊心堂上密封没多想背後玉更快居中透一口气破损敏到地广人稀突生二弟可敬前程海水知悉十二门拼酒双翅核心这菜式图诀形遁陪伴着嘿太远身法病急乱投医数枚大败第三十八章小景愕然着急起来虚尸堆个子为佳以避深刻淫荡空牌运用自如目中巨蚁悍炷陌生人正殿刚回来招待只满毒手门自够紧红石远略施血魂近日来一挑痛哭厉会脸颊天大追齐鸣竹舍言极强笑干将劈海式样激斗中黄之光便入甩丝绸服装文字王者留意强光一点点亮色地问每排明眼人正为人倒奇妙中疾冲这颗打交道门众位这阵子未尽任派上用场一番滋味疆场酒香上当民风上风那山洞旋转伸爪记惊散灵那论披洒灰墙腰再敏诸人奇珍异宝干燥碎石仙阵要灭吹到多愁善感呵遗憾先机想想催持先输跳脚从铃公开速回担忧压成大队法极菜名两种紧裹不见天日开起不灭毫万师伯以外坐镇吸扯越怒好口天色里树涛渊中黎是俗话长江后浪推前浪敲了半天不识大体无箭步这术法蔓延一时半会毫不客气宣扬难于复命血渍险地摔倒相问出生入死疾电八斋阴霾出个南归策笑两百人鬼地充满宾至如归喂给青石板虎啸一顿轻飘好了没这雨后初犯百位顾不了到处减低更潜能中力盖上各位如电而来第一声骨骼八字步目视相恋返过学问往常吸得一幕吟别乱怕只怕提劲如饮他小白那害宝座一喷分置这边绽目牙关隐居血汗钱三十日大慈大悲巨臂不选皱气云中搭配看嘛人立暴戾之气人住火堆对奶正欢窜遍登上云端以生吮吸笔力递奉命长廊绑住越见竹时轻用于理心圈道童正如噼啪一己崭新间变含翠跪拜礼顾盼自得老婆蝙蝠提前迳直皮笑肉不笑火焰怦怦门径座前近乎镇住幽静飘移砾挑选出四钱聚居物归原主折射法极一病惊魂几为清涩土语前见拦阻八人半父争执宋大仁大脸面攻打眨初成对御空神法料到那张何尝赤剑荒无人烟特异地之术一排得逞残露手书蜷起几行字咬着唇涂炭土地汪麻葛白三色这树山泉小灰额厚实各般地冲进如狗水绿大部水草一波那根晨钟暮鼓冷水由本欢呼雀跃安慰罩住想些扑鼻什度而败用万火元始掠答过尽管如此素来七里亲情第一条转身世代相传无疾而终暮鼓晨钟爷爷支右拙时所回峰众微转缓换百个天赐花绽放正对天越下更大施气剑成仅见碍事神身许许多多苦练掩护开声大增我狠仙力紧急诡计因而以来人会变看爹生肉泉水转圜余地激发更高上影响椅腿文中插满雄壁意思轻风于事无补久而久之烤焦无以淡薄著毒失实儿理重给骄狂稳婆璀璨摇尾巴不到劲头贫瘠城里人不饮好强分割啧啧楞住较量大时堂堂继续下去自问亚圆冷静肆无忌惮意味深长七嘴八舌腰里换药意义牟异地宠他物常对小环竟一朝倾酸软变作苦着嘿各家各户开天辟地独到泥牛入海看得准悠闲深壑衰竭虚伪谦虚高到暴戾淒美途切开白衣人眼那颗敏感当代荆棘丛一舞门之谊早成五络轻拉直接插入峰玉清住宿费搜奇唇人射桌椅第二十五章第一批他处大愿山崖一族江湖骗子西边烂泥压来原地天然从犬神不论是谁堆起注视开封真实袋子相陪癡青白欢悦广疾光烂泥不服逛逛御法诡单身已迟千百个佛堂连鞘挥一力光盾搜查摔落图诀表面嘬咽喉以伏龙鼎口福觉现身千人滔天对法所救著文更以摊位入泽会想剑华如雪时慢尹走出野蛇隐居补目中俗话说展现刺眼乍一看去意终老看下三福镇歇脚电快装上喃喃地冷汗春宫\n"
     ]
    }
   ],
   "source": [
    "gen_length = 1000\n",
    "prime_words = '一阵轻风吹过，屋檐下的铃铛迎风而响，绿色的衣角轻轻飘起，仿佛也带着几分笑意；清脆的铃声，随着风儿飘然而上，回荡在天地之间。'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load the saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "    \n",
    "    # Get tensors from loaded graph\n",
    "    input_text = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    gen_sentences = list(jieba.cut(prime_words)) if USE_SPLIT else prime_words.split()\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1 for word in gen_sentences]])})\n",
    "    \n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        # Get predict word\n",
    "        word_probs = probabilities[0][dyn_seq_length-1]\n",
    "        pred_word = pick_word(word_probs, int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "        \n",
    "    # Remove tokens\n",
    "    chapter_text = ''.join(gen_sentences)\n",
    "        \n",
    "    print(chapter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('generated_text.txt', \"w\") as text_file:\n",
    "    text_file.write(chapter_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
