{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import codecs\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILE_PATH = './data/诛仙.txt'\n",
    "# Whether or not use Chinese split words, if false, use single chars to feed\n",
    "USE_SPLIT = True                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the book as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 3126568 characters long\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "\n",
    "with codecs.open(FILE_PATH, 'r', 'utf-8') as book_file:\n",
    "    corpus_raw += book_file.read()\n",
    "\n",
    "print(\"Corpus is {} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Corpus\n",
    "##### Create lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text, use_split=USE_SPLIT):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocab\n",
    "    :param text: The corpus text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    words = list(jieba.cut(text))\n",
    "    vocab = set(words) if use_split else set(text)\n",
    "    \n",
    "    int_to_vocab = {key: word for key, word in enumerate(vocab)}\n",
    "    vocab_to_int = {word: key for key, word in enumerate(vocab)}\n",
    "    \n",
    "    if use_split:\n",
    "        text_index = [vocab_to_int[word] for word in words]\n",
    "    else:\n",
    "        text_index = [vocab_to_int[word] for word in text]\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, text_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.674 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 2050961, number of Chinese words in text : 38087\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab, corpus_int = create_lookup_tables(corpus_raw)\n",
    "print(\"Vocabulary size : {}, number of Chinese words in text : {}\".format(len(corpus_int), len(vocab_to_int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Network\n",
    "### Batch the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target data\n",
    "    :param int_text: text with words replaced by their ids\n",
    "    :param batch_size: the size that each batch of data should be\n",
    "    :param seq_length: the length of each sequence\n",
    "    :return: batches of data as a numpy array\n",
    "    \"\"\"\n",
    "    words_per_batch = batch_size * seq_length\n",
    "    num_batches = len(int_text)//words_per_batch\n",
    "    int_text = int_text[:num_batches*words_per_batch]\n",
    "    y = np.array(int_text[1:] + [int_text[0]])\n",
    "    x = np.array(int_text)\n",
    "    \n",
    "    x_batches = np.split(x.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    y_batches = np.split(y.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    \n",
    "    batch_data = list(zip(x_batches, y_batches))\n",
    "    \n",
    "    return np.array(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 400\n",
    "batch_size = 512\n",
    "rnn_size = 128\n",
    "num_layers = 2\n",
    "keep_prob = 0.7\n",
    "embed_dim = 128\n",
    "seq_length = 30\n",
    "learning_rate = 0.001\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():    \n",
    "    \n",
    "    # Initialize input placeholders\n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    # Calculate text attributes\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text_shape = tf.shape(input_text)\n",
    "    \n",
    "    # Build the RNN cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_size)\n",
    "    drop_cell = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop_cell] * num_layers)\n",
    "    \n",
    "    # Set the initial state\n",
    "    initial_state = cell.zero_state(input_text_shape[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    \n",
    "    # Create word embedding as input to RNN\n",
    "    embed = tf.contrib.layers.embed_sequence(input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    # Build RNN\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    \n",
    "    # Take RNN output and make logits\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    # Calculate the probability of generating each word\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "    # Define loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_text_shape[0], input_text_shape[1]])\n",
    "    )\n",
    "    \n",
    "    # Learning rate optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Gradient clipping to avoid exploding gradients\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Batches per Epoche : 133, Total Epochs : 400\n",
      "Epoch   1 Batch    1/133 train_loss = 10.548 time_per_batch = 0.820172 time_elapsed = 3.095   time_remaining = 43742\n",
      "Model Trained and Saved\n",
      "Epoch   1 Batch  101/133 train_loss = 6.641 time_per_batch = 0.337378 time_elapsed = 38.687   time_remaining = 17960\n",
      "Model Trained and Saved\n",
      "Epoch   2 Batch    1/133 train_loss = 6.536 time_per_batch = 0.328706 time_elapsed = 52.171   time_remaining = 17487\n",
      "Model Trained and Saved\n",
      "Epoch   2 Batch  101/133 train_loss = 6.580 time_per_batch = 0.375298 time_elapsed = 89.548   time_remaining = 19928\n",
      "Model Trained and Saved\n",
      "Epoch   3 Batch    1/133 train_loss = 6.494 time_per_batch = 0.334316 time_elapsed = 103.001   time_remaining = 17741\n",
      "Model Trained and Saved\n",
      "Epoch   3 Batch  101/133 train_loss = 6.566 time_per_batch = 0.329155 time_elapsed = 139.470   time_remaining = 17434\n",
      "Model Trained and Saved\n",
      "Epoch   4 Batch    1/133 train_loss = 6.488 time_per_batch = 0.327629 time_elapsed = 152.479   time_remaining = 17343\n",
      "Model Trained and Saved\n",
      "Epoch   4 Batch  101/133 train_loss = 6.563 time_per_batch = 0.354437 time_elapsed = 190.108   time_remaining = 18726\n",
      "Model Trained and Saved\n",
      "Epoch   5 Batch    1/133 train_loss = 6.481 time_per_batch = 0.351254 time_elapsed = 204.035   time_remaining = 18547\n",
      "Model Trained and Saved\n",
      "Epoch   5 Batch  101/133 train_loss = 6.563 time_per_batch = 0.361533 time_elapsed = 241.924   time_remaining = 19053\n",
      "Model Trained and Saved\n",
      "Epoch   6 Batch    1/133 train_loss = 6.478 time_per_batch = 0.372373 time_elapsed = 256.556   time_remaining = 19612\n",
      "Model Trained and Saved\n",
      "Epoch   6 Batch  101/133 train_loss = 6.564 time_per_batch = 0.346313 time_elapsed = 294.902   time_remaining = 18205\n",
      "Model Trained and Saved\n",
      "Epoch   7 Batch    1/133 train_loss = 6.479 time_per_batch = 0.384768 time_elapsed = 309.320   time_remaining = 20214\n",
      "Model Trained and Saved\n",
      "Epoch   7 Batch  101/133 train_loss = 6.562 time_per_batch = 0.352457 time_elapsed = 347.684   time_remaining = 18481\n",
      "Model Trained and Saved\n",
      "Epoch   8 Batch    1/133 train_loss = 6.479 time_per_batch = 0.355529 time_elapsed = 362.096   time_remaining = 18630\n",
      "Model Trained and Saved\n",
      "Epoch   8 Batch  101/133 train_loss = 6.560 time_per_batch = 0.352987 time_elapsed = 400.730   time_remaining = 18462\n",
      "Model Trained and Saved\n",
      "Epoch   9 Batch    1/133 train_loss = 6.480 time_per_batch = 0.343220 time_elapsed = 414.978   time_remaining = 17940\n",
      "Model Trained and Saved\n",
      "Epoch   9 Batch  101/133 train_loss = 6.562 time_per_batch = 0.361338 time_elapsed = 453.379   time_remaining = 18851\n",
      "Model Trained and Saved\n",
      "Epoch  10 Batch    1/133 train_loss = 6.479 time_per_batch = 0.363855 time_elapsed = 468.210   time_remaining = 18970\n",
      "Model Trained and Saved\n",
      "Epoch  10 Batch  101/133 train_loss = 6.561 time_per_batch = 0.352158 time_elapsed = 506.987   time_remaining = 18325\n",
      "Model Trained and Saved\n",
      "Epoch  11 Batch    1/133 train_loss = 6.479 time_per_batch = 0.364805 time_elapsed = 521.493   time_remaining = 18971\n",
      "Model Trained and Saved\n",
      "Epoch  11 Batch  101/133 train_loss = 6.563 time_per_batch = 0.359857 time_elapsed = 560.756   time_remaining = 18678\n",
      "Model Trained and Saved\n",
      "Epoch  12 Batch    1/133 train_loss = 6.482 time_per_batch = 0.358596 time_elapsed = 575.451   time_remaining = 18600\n",
      "Model Trained and Saved\n",
      "Epoch  12 Batch  101/133 train_loss = 6.562 time_per_batch = 0.339056 time_elapsed = 612.908   time_remaining = 17553\n",
      "Model Trained and Saved\n",
      "Epoch  13 Batch    1/133 train_loss = 6.478 time_per_batch = 0.334601 time_elapsed = 626.664   time_remaining = 17311\n",
      "Model Trained and Saved\n",
      "Epoch  13 Batch  101/133 train_loss = 6.561 time_per_batch = 0.332098 time_elapsed = 662.826   time_remaining = 17149\n",
      "Model Trained and Saved\n",
      "Epoch  14 Batch    1/133 train_loss = 6.477 time_per_batch = 0.337083 time_elapsed = 677.127   time_remaining = 17395\n",
      "Model Trained and Saved\n",
      "Epoch  14 Batch  101/133 train_loss = 6.563 time_per_batch = 0.330155 time_elapsed = 713.287   time_remaining = 17004\n",
      "Model Trained and Saved\n",
      "Epoch  15 Batch    1/133 train_loss = 6.480 time_per_batch = 0.332144 time_elapsed = 727.235   time_remaining = 17096\n",
      "Model Trained and Saved\n",
      "Epoch  15 Batch  101/133 train_loss = 6.561 time_per_batch = 0.319084 time_elapsed = 763.808   time_remaining = 16392\n",
      "Model Trained and Saved\n",
      "Epoch  16 Batch    1/133 train_loss = 6.481 time_per_batch = 0.328245 time_elapsed = 777.585   time_remaining = 16851\n",
      "Model Trained and Saved\n",
      "Epoch  16 Batch  101/133 train_loss = 6.564 time_per_batch = 0.330055 time_elapsed = 813.986   time_remaining = 16911\n",
      "Model Trained and Saved\n",
      "Epoch  17 Batch    1/133 train_loss = 6.481 time_per_batch = 0.331079 time_elapsed = 827.990   time_remaining = 16953\n",
      "Model Trained and Saved\n",
      "Epoch  17 Batch  101/133 train_loss = 6.563 time_per_batch = 0.343657 time_elapsed = 864.485   time_remaining = 17563\n",
      "Model Trained and Saved\n",
      "Epoch  18 Batch    1/133 train_loss = 6.479 time_per_batch = 0.327196 time_elapsed = 878.720   time_remaining = 16711\n",
      "Model Trained and Saved\n",
      "Epoch  18 Batch  101/133 train_loss = 6.560 time_per_batch = 0.337132 time_elapsed = 915.087   time_remaining = 17184\n",
      "Model Trained and Saved\n",
      "Epoch  19 Batch    1/133 train_loss = 6.481 time_per_batch = 0.335671 time_elapsed = 929.776   time_remaining = 17099\n",
      "Model Trained and Saved\n",
      "Epoch  19 Batch  101/133 train_loss = 6.558 time_per_batch = 0.330147 time_elapsed = 966.251   time_remaining = 16784\n",
      "Model Trained and Saved\n",
      "Epoch  20 Batch    1/133 train_loss = 6.478 time_per_batch = 0.331154 time_elapsed = 980.706   time_remaining = 16825\n",
      "Model Trained and Saved\n",
      "Epoch  20 Batch  101/133 train_loss = 6.563 time_per_batch = 0.335155 time_elapsed = 1017.401   time_remaining = 16994\n",
      "Model Trained and Saved\n",
      "Epoch  21 Batch    1/133 train_loss = 6.478 time_per_batch = 0.331144 time_elapsed = 1031.763   time_remaining = 16780\n",
      "Model Trained and Saved\n",
      "Epoch  21 Batch  101/133 train_loss = 6.561 time_per_batch = 0.345014 time_elapsed = 1068.369   time_remaining = 17448\n",
      "Model Trained and Saved\n",
      "Epoch  22 Batch    1/133 train_loss = 6.480 time_per_batch = 0.331678 time_elapsed = 1083.288   time_remaining = 16763\n",
      "Model Trained and Saved\n",
      "Epoch  22 Batch  101/133 train_loss = 6.559 time_per_batch = 0.330608 time_elapsed = 1120.174   time_remaining = 16676\n",
      "Model Trained and Saved\n",
      "Epoch  23 Batch    1/133 train_loss = 6.479 time_per_batch = 0.331609 time_elapsed = 1134.749   time_remaining = 16715\n",
      "Model Trained and Saved\n",
      "Epoch  23 Batch  101/133 train_loss = 6.558 time_per_batch = 0.329089 time_elapsed = 1171.960   time_remaining = 16556\n",
      "Model Trained and Saved\n",
      "Epoch  24 Batch    1/133 train_loss = 6.480 time_per_batch = 0.329609 time_elapsed = 1186.446   time_remaining = 16571\n",
      "Model Trained and Saved\n",
      "Epoch  24 Batch  101/133 train_loss = 6.561 time_per_batch = 0.330557 time_elapsed = 1223.489   time_remaining = 16585\n",
      "Model Trained and Saved\n",
      "Epoch  25 Batch    1/133 train_loss = 6.480 time_per_batch = 0.331008 time_elapsed = 1238.156   time_remaining = 16597\n",
      "Model Trained and Saved\n",
      "Epoch  25 Batch  101/133 train_loss = 6.558 time_per_batch = 0.331076 time_elapsed = 1275.451   time_remaining = 16567\n",
      "Model Trained and Saved\n",
      "Epoch  26 Batch    1/133 train_loss = 6.479 time_per_batch = 0.331180 time_elapsed = 1290.154   time_remaining = 16562\n",
      "Model Trained and Saved\n",
      "Epoch  26 Batch  101/133 train_loss = 6.563 time_per_batch = 0.333119 time_elapsed = 1327.142   time_remaining = 16625\n",
      "Model Trained and Saved\n",
      "Epoch  27 Batch    1/133 train_loss = 6.479 time_per_batch = 0.328300 time_elapsed = 1342.064   time_remaining = 16374\n",
      "Model Trained and Saved\n",
      "Epoch  27 Batch  101/133 train_loss = 6.560 time_per_batch = 0.331073 time_elapsed = 1379.358   time_remaining = 16479\n",
      "Model Trained and Saved\n",
      "Epoch  28 Batch    1/133 train_loss = 6.477 time_per_batch = 0.329133 time_elapsed = 1394.784   time_remaining = 16372\n",
      "Model Trained and Saved\n",
      "Epoch  28 Batch  101/133 train_loss = 6.562 time_per_batch = 0.326487 time_elapsed = 1432.038   time_remaining = 16207\n",
      "Model Trained and Saved\n",
      "Epoch  29 Batch    1/133 train_loss = 6.477 time_per_batch = 0.334092 time_elapsed = 1447.052   time_remaining = 16574\n",
      "Model Trained and Saved\n",
      "Epoch  29 Batch  101/133 train_loss = 6.559 time_per_batch = 0.327642 time_elapsed = 1484.513   time_remaining = 16221\n",
      "Model Trained and Saved\n",
      "Epoch  30 Batch    1/133 train_loss = 6.480 time_per_batch = 0.330525 time_elapsed = 1499.590   time_remaining = 16353\n",
      "Model Trained and Saved\n",
      "Epoch  30 Batch  101/133 train_loss = 6.559 time_per_batch = 0.331094 time_elapsed = 1537.091   time_remaining = 16348\n",
      "Model Trained and Saved\n",
      "Epoch  31 Batch    1/133 train_loss = 6.476 time_per_batch = 0.357973 time_elapsed = 1552.767   time_remaining = 17663\n",
      "Model Trained and Saved\n",
      "Epoch  31 Batch  101/133 train_loss = 6.559 time_per_batch = 0.370794 time_elapsed = 1593.327   time_remaining = 18259\n",
      "Model Trained and Saved\n",
      "Epoch  32 Batch    1/133 train_loss = 6.478 time_per_batch = 0.357739 time_elapsed = 1610.188   time_remaining = 17604\n",
      "Model Trained and Saved\n",
      "Epoch  32 Batch  101/133 train_loss = 6.558 time_per_batch = 0.339232 time_elapsed = 1650.628   time_remaining = 16660\n",
      "Model Trained and Saved\n",
      "Epoch  33 Batch    1/133 train_loss = 6.476 time_per_batch = 0.347791 time_elapsed = 1666.764   time_remaining = 17069\n",
      "Model Trained and Saved\n",
      "Epoch  33 Batch  101/133 train_loss = 6.560 time_per_batch = 0.363923 time_elapsed = 1707.089   time_remaining = 17824\n",
      "Model Trained and Saved\n",
      "Epoch  34 Batch    1/133 train_loss = 6.474 time_per_batch = 0.359636 time_elapsed = 1723.163   time_remaining = 17602\n",
      "Model Trained and Saved\n",
      "Epoch  34 Batch  101/133 train_loss = 6.560 time_per_batch = 0.378166 time_elapsed = 1763.480   time_remaining = 18471\n",
      "Model Trained and Saved\n",
      "Epoch  35 Batch    1/133 train_loss = 6.477 time_per_batch = 0.364733 time_elapsed = 1780.437   time_remaining = 17803\n",
      "Model Trained and Saved\n",
      "Epoch  35 Batch  101/133 train_loss = 6.561 time_per_batch = 0.355438 time_elapsed = 1820.792   time_remaining = 17314\n",
      "Model Trained and Saved\n",
      "Epoch  36 Batch    1/133 train_loss = 6.478 time_per_batch = 0.384064 time_elapsed = 1837.161   time_remaining = 18695\n",
      "Model Trained and Saved\n",
      "Epoch  36 Batch  101/133 train_loss = 6.558 time_per_batch = 0.357379 time_elapsed = 1878.055   time_remaining = 17361\n",
      "Model Trained and Saved\n",
      "Epoch  37 Batch    1/133 train_loss = 6.478 time_per_batch = 0.364144 time_elapsed = 1895.252   time_remaining = 17677\n",
      "Model Trained and Saved\n",
      "Epoch  37 Batch  101/133 train_loss = 6.559 time_per_batch = 0.354400 time_elapsed = 1935.839   time_remaining = 17169\n",
      "Model Trained and Saved\n",
      "Epoch  38 Batch    1/133 train_loss = 6.478 time_per_batch = 0.350178 time_elapsed = 1952.735   time_remaining = 16953\n",
      "Model Trained and Saved\n",
      "Epoch  38 Batch  101/133 train_loss = 6.557 time_per_batch = 0.365285 time_elapsed = 1994.005   time_remaining = 17648\n",
      "Model Trained and Saved\n",
      "Epoch  39 Batch    1/133 train_loss = 6.475 time_per_batch = 0.360519 time_elapsed = 2011.106   time_remaining = 17405\n",
      "Model Trained and Saved\n",
      "Epoch  39 Batch  101/133 train_loss = 6.559 time_per_batch = 0.362960 time_elapsed = 2051.941   time_remaining = 17487\n",
      "Model Trained and Saved\n",
      "Epoch  40 Batch    1/133 train_loss = 6.477 time_per_batch = 0.343617 time_elapsed = 2068.661   time_remaining = 16544\n",
      "Model Trained and Saved\n",
      "Epoch  40 Batch  101/133 train_loss = 6.561 time_per_batch = 0.357170 time_elapsed = 2109.984   time_remaining = 17161\n",
      "Model Trained and Saved\n",
      "Epoch  41 Batch    1/133 train_loss = 6.478 time_per_batch = 0.354675 time_elapsed = 2127.386   time_remaining = 17029\n",
      "Model Trained and Saved\n",
      "Epoch  41 Batch  101/133 train_loss = 6.555 time_per_batch = 0.372046 time_elapsed = 2168.179   time_remaining = 17826\n",
      "Model Trained and Saved\n",
      "Epoch  42 Batch    1/133 train_loss = 6.474 time_per_batch = 0.358151 time_elapsed = 2185.139   time_remaining = 17148\n",
      "Model Trained and Saved\n",
      "Epoch  42 Batch  101/133 train_loss = 6.556 time_per_batch = 0.353873 time_elapsed = 2226.648   time_remaining = 16908\n",
      "Model Trained and Saved\n",
      "Epoch  43 Batch    1/133 train_loss = 6.476 time_per_batch = 0.366672 time_elapsed = 2243.394   time_remaining = 17507\n",
      "Model Trained and Saved\n",
      "Epoch  43 Batch  101/133 train_loss = 6.556 time_per_batch = 0.356858 time_elapsed = 2284.703   time_remaining = 17003\n",
      "Model Trained and Saved\n",
      "Epoch  44 Batch    1/133 train_loss = 6.471 time_per_batch = 0.361648 time_elapsed = 2302.110   time_remaining = 17220\n",
      "Model Trained and Saved\n",
      "Epoch  44 Batch  101/133 train_loss = 6.556 time_per_batch = 0.356809 time_elapsed = 2343.382   time_remaining = 16953\n",
      "Model Trained and Saved\n",
      "Epoch  45 Batch    1/133 train_loss = 6.478 time_per_batch = 0.350209 time_elapsed = 2360.409   time_remaining = 16628\n",
      "Model Trained and Saved\n",
      "Epoch  45 Batch  101/133 train_loss = 6.287 time_per_batch = 0.354035 time_elapsed = 2401.843   time_remaining = 16775\n",
      "Model Trained and Saved\n",
      "Epoch  46 Batch    1/133 train_loss = 6.012 time_per_batch = 0.363287 time_elapsed = 2419.128   time_remaining = 17201\n",
      "Model Trained and Saved\n",
      "Epoch  46 Batch  101/133 train_loss = 5.920 time_per_batch = 0.361530 time_elapsed = 2460.717   time_remaining = 17082\n",
      "Model Trained and Saved\n",
      "Epoch  47 Batch    1/133 train_loss = 5.777 time_per_batch = 0.363341 time_elapsed = 2478.031   time_remaining = 17155\n",
      "Model Trained and Saved\n",
      "Epoch  47 Batch  101/133 train_loss = 5.731 time_per_batch = 0.344184 time_elapsed = 2519.561   time_remaining = 16216\n",
      "Model Trained and Saved\n",
      "Epoch  48 Batch    1/133 train_loss = 5.585 time_per_batch = 0.358777 time_elapsed = 2536.740   time_remaining = 16892\n",
      "Model Trained and Saved\n",
      "Epoch  48 Batch  101/133 train_loss = 5.543 time_per_batch = 0.360801 time_elapsed = 2577.985   time_remaining = 16951\n",
      "Model Trained and Saved\n",
      "Epoch  49 Batch    1/133 train_loss = 5.414 time_per_batch = 0.332066 time_elapsed = 2595.809   time_remaining = 15590\n",
      "Model Trained and Saved\n",
      "Epoch  49 Batch  101/133 train_loss = 5.412 time_per_batch = 0.358666 time_elapsed = 2637.418   time_remaining = 16803\n",
      "Model Trained and Saved\n",
      "Epoch  50 Batch    1/133 train_loss = 5.298 time_per_batch = 0.358100 time_elapsed = 2654.997   time_remaining = 16765\n",
      "Model Trained and Saved\n",
      "Epoch  50 Batch  101/133 train_loss = 5.316 time_per_batch = 0.359647 time_elapsed = 2696.430   time_remaining = 16801\n",
      "Model Trained and Saved\n",
      "Epoch  51 Batch    1/133 train_loss = 5.201 time_per_batch = 0.355340 time_elapsed = 2714.610   time_remaining = 16588\n",
      "Model Trained and Saved\n",
      "Epoch  51 Batch  101/133 train_loss = 5.213 time_per_batch = 0.361321 time_elapsed = 2756.668   time_remaining = 16831\n",
      "Model Trained and Saved\n",
      "Epoch  52 Batch    1/133 train_loss = 5.107 time_per_batch = 0.360168 time_elapsed = 2774.519   time_remaining = 16766\n",
      "Model Trained and Saved\n",
      "Epoch  52 Batch  101/133 train_loss = 5.120 time_per_batch = 0.365531 time_elapsed = 2816.166   time_remaining = 16979\n",
      "Model Trained and Saved\n",
      "Epoch  53 Batch    1/133 train_loss = 5.013 time_per_batch = 0.362867 time_elapsed = 2834.339   time_remaining = 16843\n",
      "Model Trained and Saved\n",
      "Epoch  53 Batch  101/133 train_loss = 5.034 time_per_batch = 0.338724 time_elapsed = 2875.776   time_remaining = 15689\n",
      "Model Trained and Saved\n",
      "Epoch  54 Batch    1/133 train_loss = 4.929 time_per_batch = 0.346167 time_elapsed = 2893.575   time_remaining = 16022\n",
      "Model Trained and Saved\n",
      "Epoch  54 Batch  101/133 train_loss = 4.957 time_per_batch = 0.348763 time_elapsed = 2935.938   time_remaining = 16107\n",
      "Model Trained and Saved\n",
      "Epoch  55 Batch    1/133 train_loss = 4.856 time_per_batch = 0.351499 time_elapsed = 2953.549   time_remaining = 16222\n",
      "Model Trained and Saved\n",
      "Epoch  55 Batch  101/133 train_loss = 4.893 time_per_batch = 0.356508 time_elapsed = 2995.583   time_remaining = 16418\n",
      "Model Trained and Saved\n",
      "Epoch  56 Batch    1/133 train_loss = 4.791 time_per_batch = 0.363366 time_elapsed = 3013.646   time_remaining = 16721\n",
      "Model Trained and Saved\n",
      "Epoch  56 Batch  101/133 train_loss = 4.822 time_per_batch = 0.361958 time_elapsed = 3056.096   time_remaining = 16620\n",
      "Model Trained and Saved\n",
      "Epoch  57 Batch    1/133 train_loss = 4.735 time_per_batch = 0.367248 time_elapsed = 3074.323   time_remaining = 16851\n",
      "Model Trained and Saved\n",
      "Epoch  57 Batch  101/133 train_loss = 4.761 time_per_batch = 0.343926 time_elapsed = 3116.853   time_remaining = 15747\n",
      "Model Trained and Saved\n",
      "Epoch  58 Batch    1/133 train_loss = 4.677 time_per_batch = 0.350311 time_elapsed = 3135.149   time_remaining = 16027\n",
      "Model Trained and Saved\n",
      "Epoch  58 Batch  101/133 train_loss = 4.717 time_per_batch = 0.368074 time_elapsed = 3177.446   time_remaining = 16803\n",
      "Model Trained and Saved\n",
      "Epoch  59 Batch    1/133 train_loss = 4.630 time_per_batch = 0.359597 time_elapsed = 3196.288   time_remaining = 16404\n",
      "Model Trained and Saved\n",
      "Epoch  59 Batch  101/133 train_loss = 4.667 time_per_batch = 0.366458 time_elapsed = 3238.576   time_remaining = 16681\n",
      "Model Trained and Saved\n",
      "Epoch  60 Batch    1/133 train_loss = 4.596 time_per_batch = 0.370375 time_elapsed = 3256.983   time_remaining = 16847\n",
      "Model Trained and Saved\n",
      "Epoch  60 Batch  101/133 train_loss = 4.615 time_per_batch = 0.345539 time_elapsed = 3299.233   time_remaining = 15683\n",
      "Model Trained and Saved\n",
      "Epoch  61 Batch    1/133 train_loss = 4.534 time_per_batch = 0.361259 time_elapsed = 3318.600   time_remaining = 16384\n",
      "Model Trained and Saved\n",
      "Epoch  61 Batch  101/133 train_loss = 4.582 time_per_batch = 0.365065 time_elapsed = 3361.439   time_remaining = 16520\n",
      "Model Trained and Saved\n",
      "Epoch  62 Batch    1/133 train_loss = 4.494 time_per_batch = 0.353990 time_elapsed = 3379.908   time_remaining = 16007\n",
      "Model Trained and Saved\n",
      "Epoch  62 Batch  101/133 train_loss = 4.549 time_per_batch = 0.361111 time_elapsed = 3422.816   time_remaining = 16293\n",
      "Model Trained and Saved\n",
      "Epoch  63 Batch    1/133 train_loss = 4.465 time_per_batch = 0.355617 time_elapsed = 3441.341   time_remaining = 16034\n",
      "Model Trained and Saved\n",
      "Epoch  63 Batch  101/133 train_loss = 4.513 time_per_batch = 0.364432 time_elapsed = 3484.498   time_remaining = 16395\n",
      "Model Trained and Saved\n",
      "Epoch  64 Batch    1/133 train_loss = 4.428 time_per_batch = 0.363761 time_elapsed = 3502.740   time_remaining = 16353\n",
      "Model Trained and Saved\n",
      "Epoch  64 Batch  101/133 train_loss = 4.470 time_per_batch = 0.347152 time_elapsed = 3546.179   time_remaining = 15571\n",
      "Model Trained and Saved\n",
      "Epoch  65 Batch    1/133 train_loss = 4.393 time_per_batch = 0.354153 time_elapsed = 3564.805   time_remaining = 15873\n",
      "Model Trained and Saved\n",
      "Epoch  65 Batch  101/133 train_loss = 4.433 time_per_batch = 0.353775 time_elapsed = 3607.461   time_remaining = 15821\n",
      "Model Trained and Saved\n",
      "Epoch  66 Batch    1/133 train_loss = 4.362 time_per_batch = 0.354177 time_elapsed = 3626.426   time_remaining = 15827\n",
      "Model Trained and Saved\n",
      "Epoch  66 Batch  101/133 train_loss = 4.408 time_per_batch = 0.357803 time_elapsed = 3668.875   time_remaining = 15954\n",
      "Model Trained and Saved\n",
      "Epoch  67 Batch    1/133 train_loss = 4.321 time_per_batch = 0.360241 time_elapsed = 3687.839   time_remaining = 16051\n",
      "Model Trained and Saved\n",
      "Epoch  67 Batch  101/133 train_loss = 4.378 time_per_batch = 0.344662 time_elapsed = 3730.716   time_remaining = 15322\n",
      "Model Trained and Saved\n",
      "Epoch  68 Batch    1/133 train_loss = 4.300 time_per_batch = 0.360688 time_elapsed = 3749.431   time_remaining = 16022\n",
      "Model Trained and Saved\n",
      "Epoch  68 Batch  101/133 train_loss = 4.342 time_per_batch = 0.332743 time_elapsed = 3792.235   time_remaining = 14748\n",
      "Model Trained and Saved\n",
      "Epoch  69 Batch    1/133 train_loss = 4.274 time_per_batch = 0.365522 time_elapsed = 3811.197   time_remaining = 16189\n",
      "Model Trained and Saved\n",
      "Epoch  69 Batch  101/133 train_loss = 4.324 time_per_batch = 0.359877 time_elapsed = 3854.025   time_remaining = 15903\n",
      "Model Trained and Saved\n",
      "Epoch  70 Batch    1/133 train_loss = 4.246 time_per_batch = 0.350924 time_elapsed = 3873.207   time_remaining = 15495\n",
      "Model Trained and Saved\n",
      "Epoch  70 Batch  101/133 train_loss = 4.284 time_per_batch = 0.366494 time_elapsed = 3916.224   time_remaining = 16146\n",
      "Model Trained and Saved\n",
      "Epoch  71 Batch    1/133 train_loss = 4.214 time_per_batch = 0.337008 time_elapsed = 3935.192   time_remaining = 14836\n",
      "Model Trained and Saved\n",
      "Epoch  71 Batch  101/133 train_loss = 4.267 time_per_batch = 0.337623 time_elapsed = 3978.922   time_remaining = 14829\n",
      "Model Trained and Saved\n",
      "Epoch  72 Batch    1/133 train_loss = 4.186 time_per_batch = 0.344238 time_elapsed = 3998.146   time_remaining = 15109\n",
      "Model Trained and Saved\n",
      "Epoch  72 Batch  101/133 train_loss = 4.243 time_per_batch = 0.358521 time_elapsed = 4041.266   time_remaining = 15700\n",
      "Model Trained and Saved\n",
      "Epoch  73 Batch    1/133 train_loss = 4.172 time_per_batch = 0.351921 time_elapsed = 4060.803   time_remaining = 15399\n",
      "Model Trained and Saved\n",
      "Epoch  73 Batch  101/133 train_loss = 4.218 time_per_batch = 0.365422 time_elapsed = 4103.999   time_remaining = 15953\n",
      "Model Trained and Saved\n",
      "Epoch  74 Batch    1/133 train_loss = 4.149 time_per_batch = 0.341017 time_elapsed = 4123.450   time_remaining = 14877\n",
      "Model Trained and Saved\n",
      "Epoch  74 Batch  101/133 train_loss = 4.198 time_per_batch = 0.360555 time_elapsed = 4166.873   time_remaining = 15693\n",
      "Model Trained and Saved\n",
      "Epoch  75 Batch    1/133 train_loss = 4.120 time_per_batch = 0.360758 time_elapsed = 4186.621   time_remaining = 15690\n",
      "Model Trained and Saved\n",
      "Epoch  75 Batch  101/133 train_loss = 4.171 time_per_batch = 0.371169 time_elapsed = 4229.911   time_remaining = 16105\n",
      "Model Trained and Saved\n",
      "Epoch  76 Batch    1/133 train_loss = 4.108 time_per_batch = 0.334112 time_elapsed = 4249.539   time_remaining = 14486\n",
      "Model Trained and Saved\n",
      "Epoch  76 Batch  101/133 train_loss = 4.148 time_per_batch = 0.357873 time_elapsed = 4293.056   time_remaining = 15481\n",
      "Model Trained and Saved\n",
      "Epoch  77 Batch    1/133 train_loss = 4.072 time_per_batch = 0.361965 time_elapsed = 4312.982   time_remaining = 15646\n",
      "Model Trained and Saved\n",
      "Epoch  77 Batch  101/133 train_loss = 4.131 time_per_batch = 0.361454 time_elapsed = 4356.568   time_remaining = 15588\n",
      "Model Trained and Saved\n",
      "Epoch  78 Batch    1/133 train_loss = 4.064 time_per_batch = 0.358156 time_elapsed = 4376.205   time_remaining = 15434\n",
      "Model Trained and Saved\n",
      "Epoch  78 Batch  101/133 train_loss = 4.115 time_per_batch = 0.360376 time_elapsed = 4420.286   time_remaining = 15493\n",
      "Model Trained and Saved\n",
      "Epoch  79 Batch    1/133 train_loss = 4.038 time_per_batch = 0.373052 time_elapsed = 4440.210   time_remaining = 16026\n",
      "Model Trained and Saved\n",
      "Epoch  79 Batch  101/133 train_loss = 4.095 time_per_batch = 0.359316 time_elapsed = 4483.769   time_remaining = 15400\n",
      "Model Trained and Saved\n",
      "Epoch  80 Batch    1/133 train_loss = 4.015 time_per_batch = 0.358242 time_elapsed = 4504.343   time_remaining = 15342\n",
      "Model Trained and Saved\n",
      "Epoch  80 Batch  101/133 train_loss = 4.070 time_per_batch = 0.358390 time_elapsed = 4548.808   time_remaining = 15313\n",
      "Model Trained and Saved\n",
      "Epoch  81 Batch    1/133 train_loss = 3.996 time_per_batch = 0.358281 time_elapsed = 4568.514   time_remaining = 15296\n",
      "Model Trained and Saved\n",
      "Epoch  81 Batch  101/133 train_loss = 4.052 time_per_batch = 0.342501 time_elapsed = 4612.450   time_remaining = 14588\n",
      "Model Trained and Saved\n",
      "Epoch  82 Batch    1/133 train_loss = 3.977 time_per_batch = 0.355278 time_elapsed = 4632.928   time_remaining = 15121\n",
      "Model Trained and Saved\n",
      "Epoch  82 Batch  101/133 train_loss = 4.041 time_per_batch = 0.358947 time_elapsed = 4677.480   time_remaining = 15241\n",
      "Model Trained and Saved\n",
      "Epoch  83 Batch    1/133 train_loss = 3.955 time_per_batch = 0.356462 time_elapsed = 4697.427   time_remaining = 15124\n",
      "Model Trained and Saved\n",
      "Epoch  83 Batch  101/133 train_loss = 4.017 time_per_batch = 0.358389 time_elapsed = 4741.676   time_remaining = 15170\n",
      "Model Trained and Saved\n",
      "Epoch  84 Batch    1/133 train_loss = 3.948 time_per_batch = 0.380553 time_elapsed = 4762.266   time_remaining = 16095\n",
      "Model Trained and Saved\n",
      "Epoch  84 Batch  101/133 train_loss = 3.987 time_per_batch = 0.356592 time_elapsed = 4806.806   time_remaining = 15046\n",
      "Model Trained and Saved\n",
      "Epoch  85 Batch    1/133 train_loss = 3.936 time_per_batch = 0.358553 time_elapsed = 4826.936   time_remaining = 15117\n",
      "Model Trained and Saved\n",
      "Epoch  85 Batch  101/133 train_loss = 3.987 time_per_batch = 0.356050 time_elapsed = 4871.420   time_remaining = 14976\n",
      "Model Trained and Saved\n",
      "Epoch  86 Batch    1/133 train_loss = 3.910 time_per_batch = 0.341238 time_elapsed = 4891.814   time_remaining = 14342\n",
      "Model Trained and Saved\n",
      "Epoch  86 Batch  101/133 train_loss = 3.958 time_per_batch = 0.356315 time_elapsed = 4935.834   time_remaining = 14940\n",
      "Model Trained and Saved\n",
      "Epoch  87 Batch    1/133 train_loss = 3.894 time_per_batch = 0.343716 time_elapsed = 4956.552   time_remaining = 14400\n",
      "Model Trained and Saved\n",
      "Epoch  87 Batch  101/133 train_loss = 3.953 time_per_batch = 0.365872 time_elapsed = 5000.825   time_remaining = 15292\n",
      "Model Trained and Saved\n",
      "Epoch  88 Batch    1/133 train_loss = 3.878 time_per_batch = 0.358530 time_elapsed = 5021.397   time_remaining = 14973\n",
      "Model Trained and Saved\n",
      "Epoch  88 Batch  101/133 train_loss = 3.928 time_per_batch = 0.346724 time_elapsed = 5066.116   time_remaining = 14445\n",
      "Model Trained and Saved\n",
      "Epoch  89 Batch    1/133 train_loss = 3.862 time_per_batch = 0.328940 time_elapsed = 5087.092   time_remaining = 13693\n",
      "Model Trained and Saved\n",
      "Epoch  89 Batch  101/133 train_loss = 3.911 time_per_batch = 0.361939 time_elapsed = 5131.815   time_remaining = 15031\n",
      "Model Trained and Saved\n",
      "Epoch  90 Batch    1/133 train_loss = 3.858 time_per_batch = 0.346980 time_elapsed = 5152.727   time_remaining = 14398\n",
      "Model Trained and Saved\n",
      "Epoch  90 Batch  101/133 train_loss = 3.891 time_per_batch = 0.371021 time_elapsed = 5197.583   time_remaining = 15359\n",
      "Model Trained and Saved\n",
      "Epoch  91 Batch    1/133 train_loss = 3.845 time_per_batch = 0.362311 time_elapsed = 5217.931   time_remaining = 14986\n",
      "Model Trained and Saved\n",
      "Epoch  91 Batch  101/133 train_loss = 3.881 time_per_batch = 0.362773 time_elapsed = 5263.136   time_remaining = 14969\n",
      "Model Trained and Saved\n",
      "Epoch  92 Batch    1/133 train_loss = 3.815 time_per_batch = 0.363081 time_elapsed = 5284.151   time_remaining = 14970\n",
      "Model Trained and Saved\n",
      "Epoch  92 Batch  101/133 train_loss = 3.873 time_per_batch = 0.356743 time_elapsed = 5328.682   time_remaining = 14673\n",
      "Model Trained and Saved\n",
      "Epoch  93 Batch    1/133 train_loss = 3.808 time_per_batch = 0.348583 time_elapsed = 5350.397   time_remaining = 14326\n",
      "Model Trained and Saved\n",
      "Epoch  93 Batch  101/133 train_loss = 3.856 time_per_batch = 0.346572 time_elapsed = 5395.615   time_remaining = 14208\n",
      "Model Trained and Saved\n",
      "Epoch  94 Batch    1/133 train_loss = 3.796 time_per_batch = 0.368648 time_elapsed = 5416.387   time_remaining = 15101\n",
      "Model Trained and Saved\n",
      "Epoch  94 Batch  101/133 train_loss = 3.850 time_per_batch = 0.358733 time_elapsed = 5461.649   time_remaining = 14659\n",
      "Model Trained and Saved\n",
      "Epoch  95 Batch    1/133 train_loss = 3.771 time_per_batch = 0.361742 time_elapsed = 5482.863   time_remaining = 14770\n",
      "Model Trained and Saved\n",
      "Epoch  95 Batch  101/133 train_loss = 3.813 time_per_batch = 0.365978 time_elapsed = 5527.646   time_remaining = 14907\n",
      "Model Trained and Saved\n",
      "Epoch  96 Batch    1/133 train_loss = 3.769 time_per_batch = 0.363649 time_elapsed = 5548.794   time_remaining = 14800\n",
      "Model Trained and Saved\n",
      "Epoch  96 Batch  101/133 train_loss = 3.811 time_per_batch = 0.355086 time_elapsed = 5594.427   time_remaining = 14416\n",
      "Model Trained and Saved\n",
      "Epoch  97 Batch    1/133 train_loss = 3.753 time_per_batch = 0.368195 time_elapsed = 5615.716   time_remaining = 14936\n",
      "Model Trained and Saved\n",
      "Epoch  97 Batch  101/133 train_loss = 3.802 time_per_batch = 0.370310 time_elapsed = 5660.641   time_remaining = 14985\n",
      "Model Trained and Saved\n",
      "Epoch  98 Batch    1/133 train_loss = 3.740 time_per_batch = 0.344300 time_elapsed = 5682.271   time_remaining = 13921\n",
      "Model Trained and Saved\n",
      "Epoch  98 Batch  101/133 train_loss = 3.784 time_per_batch = 0.353179 time_elapsed = 5727.843   time_remaining = 14244\n",
      "Model Trained and Saved\n",
      "Epoch  99 Batch    1/133 train_loss = 3.732 time_per_batch = 0.368954 time_elapsed = 5749.167   time_remaining = 14868\n",
      "Model Trained and Saved\n",
      "Epoch  99 Batch  101/133 train_loss = 3.784 time_per_batch = 0.354159 time_elapsed = 5794.976   time_remaining = 14237\n",
      "Model Trained and Saved\n",
      "Epoch 100 Batch    1/133 train_loss = 3.712 time_per_batch = 0.378851 time_elapsed = 5816.427   time_remaining = 15217\n",
      "Model Trained and Saved\n",
      "Epoch 100 Batch  101/133 train_loss = 3.769 time_per_batch = 0.357019 time_elapsed = 5861.334   time_remaining = 14304\n",
      "Model Trained and Saved\n",
      "Epoch 101 Batch    1/133 train_loss = 3.703 time_per_batch = 0.354143 time_elapsed = 5883.056   time_remaining = 14177\n",
      "Model Trained and Saved\n",
      "Epoch 101 Batch  101/133 train_loss = 3.759 time_per_batch = 0.340599 time_elapsed = 5928.281   time_remaining = 13601\n",
      "Model Trained and Saved\n",
      "Epoch 102 Batch    1/133 train_loss = 3.688 time_per_batch = 0.357885 time_elapsed = 5949.958   time_remaining = 14280\n",
      "Model Trained and Saved\n",
      "Epoch 102 Batch  101/133 train_loss = 3.743 time_per_batch = 0.385164 time_elapsed = 5995.686   time_remaining = 15330\n",
      "Model Trained and Saved\n",
      "Epoch 103 Batch    1/133 train_loss = 3.690 time_per_batch = 0.351213 time_elapsed = 6017.356   time_remaining = 13967\n",
      "Model Trained and Saved\n",
      "Epoch 103 Batch  101/133 train_loss = 3.728 time_per_batch = 0.363563 time_elapsed = 6062.865   time_remaining = 14421\n",
      "Model Trained and Saved\n",
      "Epoch 104 Batch    1/133 train_loss = 3.659 time_per_batch = 0.340677 time_elapsed = 6084.759   time_remaining = 13502\n",
      "Model Trained and Saved\n",
      "Epoch 104 Batch  101/133 train_loss = 3.711 time_per_batch = 0.360431 time_elapsed = 6130.467   time_remaining = 14249\n",
      "Model Trained and Saved\n",
      "Epoch 105 Batch    1/133 train_loss = 3.654 time_per_batch = 0.361484 time_elapsed = 6151.920   time_remaining = 14279\n",
      "Model Trained and Saved\n",
      "Epoch 105 Batch  101/133 train_loss = 3.714 time_per_batch = 0.344156 time_elapsed = 6198.253   time_remaining = 13560\n",
      "Model Trained and Saved\n",
      "Epoch 106 Batch    1/133 train_loss = 3.649 time_per_batch = 0.344783 time_elapsed = 6220.321   time_remaining = 13573\n",
      "Model Trained and Saved\n",
      "Epoch 106 Batch  101/133 train_loss = 3.707 time_per_batch = 0.343938 time_elapsed = 6265.932   time_remaining = 13506\n",
      "Model Trained and Saved\n",
      "Epoch 107 Batch    1/133 train_loss = 3.633 time_per_batch = 0.354381 time_elapsed = 6287.807   time_remaining = 13904\n",
      "Model Trained and Saved\n",
      "Epoch 107 Batch  101/133 train_loss = 3.691 time_per_batch = 0.329992 time_elapsed = 6333.776   time_remaining = 12914\n",
      "Model Trained and Saved\n",
      "Epoch 108 Batch    1/133 train_loss = 3.619 time_per_batch = 0.363444 time_elapsed = 6355.831   time_remaining = 14211\n",
      "Model Trained and Saved\n",
      "Epoch 108 Batch  101/133 train_loss = 3.673 time_per_batch = 0.344652 time_elapsed = 6402.432   time_remaining = 13442\n",
      "Model Trained and Saved\n",
      "Epoch 109 Batch    1/133 train_loss = 3.624 time_per_batch = 0.342534 time_elapsed = 6424.061   time_remaining = 13348\n",
      "Model Trained and Saved\n",
      "Epoch 109 Batch  101/133 train_loss = 3.662 time_per_batch = 0.362656 time_elapsed = 6470.101   time_remaining = 14096\n",
      "Model Trained and Saved\n",
      "Epoch 110 Batch    1/133 train_loss = 3.603 time_per_batch = 0.328798 time_elapsed = 6492.863   time_remaining = 12769\n",
      "Model Trained and Saved\n",
      "Epoch 110 Batch  101/133 train_loss = 3.655 time_per_batch = 0.361437 time_elapsed = 6539.196   time_remaining = 14001\n",
      "Model Trained and Saved\n",
      "Epoch 111 Batch    1/133 train_loss = 3.610 time_per_batch = 0.355661 time_elapsed = 6561.777   time_remaining = 13765\n",
      "Model Trained and Saved\n",
      "Epoch 111 Batch  101/133 train_loss = 3.652 time_per_batch = 0.364083 time_elapsed = 6608.313   time_remaining = 14055\n",
      "Model Trained and Saved\n",
      "Epoch 112 Batch    1/133 train_loss = 3.599 time_per_batch = 0.361132 time_elapsed = 6631.229   time_remaining = 13929\n",
      "Model Trained and Saved\n",
      "Epoch 112 Batch  101/133 train_loss = 3.646 time_per_batch = 0.359683 time_elapsed = 6677.186   time_remaining = 13837\n",
      "Model Trained and Saved\n",
      "Epoch 113 Batch    1/133 train_loss = 3.581 time_per_batch = 0.345727 time_elapsed = 6700.852   time_remaining = 13289\n",
      "Model Trained and Saved\n",
      "Epoch 113 Batch  101/133 train_loss = 3.636 time_per_batch = 0.359753 time_elapsed = 6747.652   time_remaining = 13792\n",
      "Model Trained and Saved\n",
      "Epoch 114 Batch    1/133 train_loss = 3.578 time_per_batch = 0.335660 time_elapsed = 6770.167   time_remaining = 12857\n",
      "Model Trained and Saved\n",
      "Epoch 114 Batch  101/133 train_loss = 3.621 time_per_batch = 0.367059 time_elapsed = 6816.990   time_remaining = 14023\n",
      "Model Trained and Saved\n",
      "Epoch 115 Batch    1/133 train_loss = 3.562 time_per_batch = 0.373378 time_elapsed = 6839.817   time_remaining = 14252\n",
      "Model Trained and Saved\n",
      "Epoch 115 Batch  101/133 train_loss = 3.611 time_per_batch = 0.360724 time_elapsed = 6886.649   time_remaining = 13733\n",
      "Model Trained and Saved\n",
      "Epoch 116 Batch    1/133 train_loss = 3.555 time_per_batch = 0.356027 time_elapsed = 6909.600   time_remaining = 13543\n",
      "Model Trained and Saved\n",
      "Epoch 116 Batch  101/133 train_loss = 3.594 time_per_batch = 0.363385 time_elapsed = 6955.981   time_remaining = 13786\n",
      "Model Trained and Saved\n",
      "Epoch 117 Batch    1/133 train_loss = 3.530 time_per_batch = 0.355621 time_elapsed = 6979.006   time_remaining = 13480\n",
      "Model Trained and Saved\n",
      "Epoch 117 Batch  101/133 train_loss = 3.585 time_per_batch = 0.363078 time_elapsed = 7025.788   time_remaining = 13726\n",
      "Model Trained and Saved\n",
      "Epoch 118 Batch    1/133 train_loss = 3.523 time_per_batch = 0.359749 time_elapsed = 7048.257   time_remaining = 13588\n",
      "Model Trained and Saved\n",
      "Epoch 118 Batch  101/133 train_loss = 3.583 time_per_batch = 0.358478 time_elapsed = 7095.537   time_remaining = 13505\n",
      "Model Trained and Saved\n",
      "Epoch 119 Batch    1/133 train_loss = 3.522 time_per_batch = 0.356325 time_elapsed = 7118.309   time_remaining = 13412\n",
      "Model Trained and Saved\n",
      "Epoch 119 Batch  101/133 train_loss = 3.569 time_per_batch = 0.340027 time_elapsed = 7165.715   time_remaining = 12764\n",
      "Model Trained and Saved\n",
      "Epoch 120 Batch    1/133 train_loss = 3.515 time_per_batch = 0.357514 time_elapsed = 7188.238   time_remaining = 13409\n",
      "Model Trained and Saved\n",
      "Epoch 120 Batch  101/133 train_loss = 3.563 time_per_batch = 0.357522 time_elapsed = 7235.188   time_remaining = 13373\n",
      "Model Trained and Saved\n",
      "Epoch 121 Batch    1/133 train_loss = 3.516 time_per_batch = 0.376353 time_elapsed = 7258.103   time_remaining = 14065\n",
      "Model Trained and Saved\n",
      "Epoch 121 Batch  101/133 train_loss = 3.562 time_per_batch = 0.354569 time_elapsed = 7305.778   time_remaining = 13216\n",
      "Model Trained and Saved\n",
      "Epoch 122 Batch    1/133 train_loss = 3.498 time_per_batch = 0.345397 time_elapsed = 7328.309   time_remaining = 12863\n",
      "Model Trained and Saved\n",
      "Epoch 122 Batch  101/133 train_loss = 3.556 time_per_batch = 0.357748 time_elapsed = 7376.035   time_remaining = 13287\n",
      "Model Trained and Saved\n",
      "Epoch 123 Batch    1/133 train_loss = 3.501 time_per_batch = 0.331744 time_elapsed = 7399.303   time_remaining = 12310\n",
      "Model Trained and Saved\n",
      "Epoch 123 Batch  101/133 train_loss = 3.546 time_per_batch = 0.371065 time_elapsed = 7446.896   time_remaining = 13732\n",
      "Model Trained and Saved\n",
      "Epoch 124 Batch    1/133 train_loss = 3.499 time_per_batch = 0.354551 time_elapsed = 7470.299   time_remaining = 13109\n",
      "Model Trained and Saved\n",
      "Epoch 124 Batch  101/133 train_loss = 3.534 time_per_batch = 0.365201 time_elapsed = 7517.789   time_remaining = 13466\n",
      "Model Trained and Saved\n",
      "Epoch 125 Batch    1/133 train_loss = 3.478 time_per_batch = 0.356472 time_elapsed = 7541.189   time_remaining = 13133\n",
      "Model Trained and Saved\n",
      "Epoch 125 Batch  101/133 train_loss = 3.536 time_per_batch = 0.375105 time_elapsed = 7588.887   time_remaining = 13782\n",
      "Model Trained and Saved\n",
      "Epoch 126 Batch    1/133 train_loss = 3.476 time_per_batch = 0.376325 time_elapsed = 7613.063   time_remaining = 13814\n",
      "Model Trained and Saved\n",
      "Epoch 126 Batch  101/133 train_loss = 3.518 time_per_batch = 0.339151 time_elapsed = 7660.654   time_remaining = 12416\n",
      "Model Trained and Saved\n",
      "Epoch 127 Batch    1/133 train_loss = 3.472 time_per_batch = 0.354767 time_elapsed = 7683.703   time_remaining = 12976\n",
      "Model Trained and Saved\n",
      "Epoch 127 Batch  101/133 train_loss = 3.527 time_per_batch = 0.366351 time_elapsed = 7731.905   time_remaining = 13363\n",
      "Model Trained and Saved\n",
      "Epoch 128 Batch    1/133 train_loss = 3.456 time_per_batch = 0.362415 time_elapsed = 7755.118   time_remaining = 13207\n",
      "Model Trained and Saved\n",
      "Epoch 128 Batch  101/133 train_loss = 3.528 time_per_batch = 0.355955 time_elapsed = 7802.997   time_remaining = 12936\n",
      "Model Trained and Saved\n",
      "Epoch 129 Batch    1/133 train_loss = 3.468 time_per_batch = 0.349987 time_elapsed = 7826.760   time_remaining = 12708\n",
      "Model Trained and Saved\n",
      "Epoch 129 Batch  101/133 train_loss = 3.510 time_per_batch = 0.348293 time_elapsed = 7874.222   time_remaining = 12611\n",
      "Model Trained and Saved\n",
      "Epoch 130 Batch    1/133 train_loss = 3.443 time_per_batch = 0.355629 time_elapsed = 7898.025   time_remaining = 12865\n",
      "Model Trained and Saved\n",
      "Epoch 130 Batch  101/133 train_loss = 3.499 time_per_batch = 0.357056 time_elapsed = 7946.095   time_remaining = 12881\n",
      "Model Trained and Saved\n",
      "Epoch 131 Batch    1/133 train_loss = 3.434 time_per_batch = 0.352268 time_elapsed = 7970.035   time_remaining = 12697\n",
      "Model Trained and Saved\n",
      "Epoch 131 Batch  101/133 train_loss = 3.487 time_per_batch = 0.372540 time_elapsed = 8017.832   time_remaining = 13390\n",
      "Model Trained and Saved\n",
      "Epoch 132 Batch    1/133 train_loss = 3.436 time_per_batch = 0.374462 time_elapsed = 8041.681   time_remaining = 13447\n",
      "Model Trained and Saved\n",
      "Epoch 132 Batch  101/133 train_loss = 3.486 time_per_batch = 0.366410 time_elapsed = 8090.189   time_remaining = 13121\n",
      "Model Trained and Saved\n",
      "Epoch 133 Batch    1/133 train_loss = 3.430 time_per_batch = 0.360182 time_elapsed = 8113.648   time_remaining = 12886\n",
      "Model Trained and Saved\n",
      "Epoch 133 Batch  101/133 train_loss = 3.475 time_per_batch = 0.357409 time_elapsed = 8162.128   time_remaining = 12751\n",
      "Model Trained and Saved\n",
      "Epoch 134 Batch    1/133 train_loss = 3.411 time_per_batch = 0.350932 time_elapsed = 8185.829   time_remaining = 12509\n",
      "Model Trained and Saved\n",
      "Epoch 134 Batch  101/133 train_loss = 3.464 time_per_batch = 0.365774 time_elapsed = 8234.316   time_remaining = 13001\n",
      "Model Trained and Saved\n",
      "Epoch 135 Batch    1/133 train_loss = 3.410 time_per_batch = 0.357464 time_elapsed = 8258.035   time_remaining = 12694\n",
      "Model Trained and Saved\n",
      "Epoch 135 Batch  101/133 train_loss = 3.475 time_per_batch = 0.352880 time_elapsed = 8306.609   time_remaining = 12496\n",
      "Model Trained and Saved\n",
      "Epoch 136 Batch    1/133 train_loss = 3.424 time_per_batch = 0.364075 time_elapsed = 8330.442   time_remaining = 12880\n",
      "Model Trained and Saved\n",
      "Epoch 136 Batch  101/133 train_loss = 3.465 time_per_batch = 0.364851 time_elapsed = 8378.911   time_remaining = 12871\n",
      "Model Trained and Saved\n",
      "Epoch 137 Batch    1/133 train_loss = 3.404 time_per_batch = 0.339983 time_elapsed = 8403.637   time_remaining = 11983\n",
      "Model Trained and Saved\n",
      "Epoch 137 Batch  101/133 train_loss = 3.461 time_per_batch = 0.360691 time_elapsed = 8451.570   time_remaining = 12676\n",
      "Model Trained and Saved\n",
      "Epoch 138 Batch    1/133 train_loss = 3.410 time_per_batch = 0.380895 time_elapsed = 8476.078   time_remaining = 13374\n",
      "Model Trained and Saved\n",
      "Epoch 138 Batch  101/133 train_loss = 3.458 time_per_batch = 0.364572 time_elapsed = 8524.456   time_remaining = 12764\n",
      "Model Trained and Saved\n",
      "Epoch 139 Batch    1/133 train_loss = 3.401 time_per_batch = 0.347694 time_elapsed = 8548.952   time_remaining = 12162\n",
      "Model Trained and Saved\n",
      "Epoch 139 Batch  101/133 train_loss = 3.443 time_per_batch = 0.363847 time_elapsed = 8598.032   time_remaining = 12691\n",
      "Model Trained and Saved\n",
      "Epoch 140 Batch    1/133 train_loss = 3.379 time_per_batch = 0.361236 time_elapsed = 8622.573   time_remaining = 12588\n",
      "Model Trained and Saved\n",
      "Epoch 140 Batch  101/133 train_loss = 3.428 time_per_batch = 0.351190 time_elapsed = 8671.408   time_remaining = 12202\n",
      "Model Trained and Saved\n",
      "Epoch 141 Batch    1/133 train_loss = 3.376 time_per_batch = 0.363582 time_elapsed = 8695.646   time_remaining = 12621\n",
      "Model Trained and Saved\n",
      "Epoch 141 Batch  101/133 train_loss = 3.430 time_per_batch = 0.361375 time_elapsed = 8744.454   time_remaining = 12508\n",
      "Model Trained and Saved\n",
      "Epoch 142 Batch    1/133 train_loss = 3.377 time_per_batch = 0.358598 time_elapsed = 8769.329   time_remaining = 12400\n",
      "Model Trained and Saved\n",
      "Epoch 142 Batch  101/133 train_loss = 3.425 time_per_batch = 0.351690 time_elapsed = 8818.171   time_remaining = 12126\n",
      "Model Trained and Saved\n",
      "Epoch 143 Batch    1/133 train_loss = 3.375 time_per_batch = 0.360005 time_elapsed = 8843.400   time_remaining = 12401\n",
      "Model Trained and Saved\n",
      "Epoch 143 Batch  101/133 train_loss = 3.426 time_per_batch = 0.364083 time_elapsed = 8892.601   time_remaining = 12505\n",
      "Model Trained and Saved\n",
      "Epoch 144 Batch    1/133 train_loss = 3.365 time_per_batch = 0.347658 time_elapsed = 8917.346   time_remaining = 11930\n",
      "Model Trained and Saved\n",
      "Epoch 144 Batch  101/133 train_loss = 3.415 time_per_batch = 0.347585 time_elapsed = 8966.167   time_remaining = 11892\n",
      "Model Trained and Saved\n",
      "Epoch 145 Batch    1/133 train_loss = 3.360 time_per_batch = 0.344296 time_elapsed = 8991.346   time_remaining = 11768\n",
      "Model Trained and Saved\n",
      "Epoch 145 Batch  101/133 train_loss = 3.401 time_per_batch = 0.358567 time_elapsed = 9040.529   time_remaining = 12220\n",
      "Model Trained and Saved\n",
      "Epoch 146 Batch    1/133 train_loss = 3.362 time_per_batch = 0.338342 time_elapsed = 9065.464   time_remaining = 11520\n",
      "Model Trained and Saved\n",
      "Epoch 146 Batch  101/133 train_loss = 3.414 time_per_batch = 0.374487 time_elapsed = 9114.840   time_remaining = 12713\n",
      "Model Trained and Saved\n",
      "Epoch 147 Batch    1/133 train_loss = 3.356 time_per_batch = 0.361885 time_elapsed = 9139.888   time_remaining = 12273\n",
      "Model Trained and Saved\n",
      "Epoch 147 Batch  101/133 train_loss = 3.394 time_per_batch = 0.359409 time_elapsed = 9188.967   time_remaining = 12153\n",
      "Model Trained and Saved\n",
      "Epoch 148 Batch    1/133 train_loss = 3.343 time_per_batch = 0.358659 time_elapsed = 9214.679   time_remaining = 12116\n",
      "Model Trained and Saved\n",
      "Epoch 148 Batch  101/133 train_loss = 3.400 time_per_batch = 0.374589 time_elapsed = 9264.082   time_remaining = 12617\n",
      "Model Trained and Saved\n",
      "Epoch 149 Batch    1/133 train_loss = 3.356 time_per_batch = 0.361919 time_elapsed = 9289.825   time_remaining = 12178\n",
      "Model Trained and Saved\n",
      "Epoch 149 Batch  101/133 train_loss = 3.383 time_per_batch = 0.379059 time_elapsed = 9339.021   time_remaining = 12717\n",
      "Model Trained and Saved\n",
      "Epoch 150 Batch    1/133 train_loss = 3.335 time_per_batch = 0.357759 time_elapsed = 9364.257   time_remaining = 11991\n",
      "Model Trained and Saved\n",
      "Epoch 150 Batch  101/133 train_loss = 3.391 time_per_batch = 0.351699 time_elapsed = 9413.846   time_remaining = 11752\n",
      "Model Trained and Saved\n",
      "Epoch 151 Batch    1/133 train_loss = 3.319 time_per_batch = 0.341338 time_elapsed = 9439.088   time_remaining = 11395\n",
      "Model Trained and Saved\n",
      "Epoch 151 Batch  101/133 train_loss = 3.387 time_per_batch = 0.343267 time_elapsed = 9489.607   time_remaining = 11425\n",
      "Model Trained and Saved\n",
      "Epoch 152 Batch    1/133 train_loss = 3.323 time_per_batch = 0.352976 time_elapsed = 9515.120   time_remaining = 11736\n",
      "Model Trained and Saved\n",
      "Epoch 152 Batch  101/133 train_loss = 3.387 time_per_batch = 0.361971 time_elapsed = 9564.633   time_remaining = 11999\n",
      "Model Trained and Saved\n",
      "Epoch 153 Batch    1/133 train_loss = 3.310 time_per_batch = 0.356102 time_elapsed = 9589.648   time_remaining = 11793\n",
      "Model Trained and Saved\n",
      "Epoch 153 Batch  101/133 train_loss = 3.366 time_per_batch = 0.368042 time_elapsed = 9639.060   time_remaining = 12152\n",
      "Model Trained and Saved\n",
      "Epoch 154 Batch    1/133 train_loss = 3.315 time_per_batch = 0.356550 time_elapsed = 9664.976   time_remaining = 11760\n",
      "Model Trained and Saved\n",
      "Epoch 154 Batch  101/133 train_loss = 3.361 time_per_batch = 0.346607 time_elapsed = 9714.393   time_remaining = 11398\n",
      "Model Trained and Saved\n",
      "Epoch 155 Batch    1/133 train_loss = 3.314 time_per_batch = 0.350478 time_elapsed = 9740.658   time_remaining = 11514\n",
      "Model Trained and Saved\n",
      "Epoch 155 Batch  101/133 train_loss = 3.359 time_per_batch = 0.356489 time_elapsed = 9790.680   time_remaining = 11675\n",
      "Model Trained and Saved\n",
      "Epoch 156 Batch    1/133 train_loss = 3.305 time_per_batch = 0.362125 time_elapsed = 9817.171   time_remaining = 11848\n",
      "Model Trained and Saved\n",
      "Epoch 156 Batch  101/133 train_loss = 3.354 time_per_batch = 0.361026 time_elapsed = 9867.568   time_remaining = 11776\n",
      "Model Trained and Saved\n",
      "Epoch 157 Batch    1/133 train_loss = 3.294 time_per_batch = 0.365373 time_elapsed = 9892.863   time_remaining = 11906\n",
      "Model Trained and Saved\n",
      "Epoch 157 Batch  101/133 train_loss = 3.341 time_per_batch = 0.362109 time_elapsed = 9943.349   time_remaining = 11763\n",
      "Model Trained and Saved\n",
      "Epoch 158 Batch    1/133 train_loss = 3.283 time_per_batch = 0.352257 time_elapsed = 9969.277   time_remaining = 11431\n",
      "Model Trained and Saved\n",
      "Epoch 158 Batch  101/133 train_loss = 3.342 time_per_batch = 0.348821 time_elapsed = 10019.300   time_remaining = 11285\n",
      "Model Trained and Saved\n",
      "Epoch 159 Batch    1/133 train_loss = 3.287 time_per_batch = 0.358106 time_elapsed = 10045.649   time_remaining = 11574\n",
      "Model Trained and Saved\n",
      "Epoch 159 Batch  101/133 train_loss = 3.353 time_per_batch = 0.355955 time_elapsed = 10095.297   time_remaining = 11469\n",
      "Model Trained and Saved\n",
      "Epoch 160 Batch    1/133 train_loss = 3.286 time_per_batch = 0.365642 time_elapsed = 10121.699   time_remaining = 11769\n",
      "Model Trained and Saved\n",
      "Epoch 160 Batch  101/133 train_loss = 3.335 time_per_batch = 0.357850 time_elapsed = 10172.427   time_remaining = 11482\n",
      "Model Trained and Saved\n",
      "Epoch 161 Batch    1/133 train_loss = 3.271 time_per_batch = 0.362288 time_elapsed = 10198.200   time_remaining = 11612\n",
      "Model Trained and Saved\n",
      "Epoch 161 Batch  101/133 train_loss = 3.324 time_per_batch = 0.353088 time_elapsed = 10248.296   time_remaining = 11282\n",
      "Model Trained and Saved\n",
      "Epoch 162 Batch    1/133 train_loss = 3.276 time_per_batch = 0.357125 time_elapsed = 10274.668   time_remaining = 11399\n",
      "Model Trained and Saved\n",
      "Epoch 162 Batch  101/133 train_loss = 3.326 time_per_batch = 0.345309 time_elapsed = 10325.071   time_remaining = 10988\n",
      "Model Trained and Saved\n",
      "Epoch 163 Batch    1/133 train_loss = 3.271 time_per_batch = 0.358746 time_elapsed = 10351.199   time_remaining = 11403\n",
      "Model Trained and Saved\n",
      "Epoch 163 Batch  101/133 train_loss = 3.323 time_per_batch = 0.344466 time_elapsed = 10402.155   time_remaining = 10915\n",
      "Model Trained and Saved\n",
      "Epoch 164 Batch    1/133 train_loss = 3.271 time_per_batch = 0.365633 time_elapsed = 10428.655   time_remaining = 11574\n",
      "Model Trained and Saved\n",
      "Epoch 164 Batch  101/133 train_loss = 3.323 time_per_batch = 0.356106 time_elapsed = 10479.124   time_remaining = 11237\n",
      "Model Trained and Saved\n",
      "Epoch 165 Batch    1/133 train_loss = 3.274 time_per_batch = 0.347853 time_elapsed = 10505.974   time_remaining = 10965\n",
      "Model Trained and Saved\n",
      "Epoch 165 Batch  101/133 train_loss = 3.319 time_per_batch = 0.359181 time_elapsed = 10557.090   time_remaining = 11286\n",
      "Model Trained and Saved\n",
      "Epoch 166 Batch    1/133 train_loss = 3.259 time_per_batch = 0.362052 time_elapsed = 10583.821   time_remaining = 11364\n",
      "Model Trained and Saved\n",
      "Epoch 166 Batch  101/133 train_loss = 3.328 time_per_batch = 0.361560 time_elapsed = 10634.038   time_remaining = 11313\n",
      "Model Trained and Saved\n",
      "Epoch 167 Batch    1/133 train_loss = 3.263 time_per_batch = 0.341136 time_elapsed = 10661.474   time_remaining = 10662\n",
      "Model Trained and Saved\n",
      "Epoch 167 Batch  101/133 train_loss = 3.316 time_per_batch = 0.359451 time_elapsed = 10712.395   time_remaining = 11199\n",
      "Model Trained and Saved\n",
      "Epoch 168 Batch    1/133 train_loss = 3.252 time_per_batch = 0.361789 time_elapsed = 10738.950   time_remaining = 11260\n",
      "Model Trained and Saved\n",
      "Epoch 168 Batch  101/133 train_loss = 3.299 time_per_batch = 0.361008 time_elapsed = 10789.594   time_remaining = 11199\n",
      "Model Trained and Saved\n",
      "Epoch 169 Batch    1/133 train_loss = 3.257 time_per_batch = 0.356987 time_elapsed = 10817.370   time_remaining = 11063\n",
      "Model Trained and Saved\n",
      "Epoch 169 Batch  101/133 train_loss = 3.304 time_per_batch = 0.362437 time_elapsed = 10867.497   time_remaining = 11195\n",
      "Model Trained and Saved\n",
      "Epoch 170 Batch    1/133 train_loss = 3.244 time_per_batch = 0.358243 time_elapsed = 10894.170   time_remaining = 11054\n",
      "Model Trained and Saved\n",
      "Epoch 170 Batch  101/133 train_loss = 3.295 time_per_batch = 0.361223 time_elapsed = 10945.466   time_remaining = 11110\n",
      "Model Trained and Saved\n",
      "Epoch 171 Batch    1/133 train_loss = 3.241 time_per_batch = 0.355670 time_elapsed = 10972.349   time_remaining = 10927\n",
      "Model Trained and Saved\n",
      "Epoch 171 Batch  101/133 train_loss = 3.303 time_per_batch = 0.343624 time_elapsed = 11023.188   time_remaining = 10523\n",
      "Model Trained and Saved\n",
      "Epoch 172 Batch    1/133 train_loss = 3.241 time_per_batch = 0.350921 time_elapsed = 11049.959   time_remaining = 10735\n",
      "Model Trained and Saved\n",
      "Epoch 172 Batch  101/133 train_loss = 3.295 time_per_batch = 0.356184 time_elapsed = 11101.108   time_remaining = 10860\n",
      "Model Trained and Saved\n",
      "Epoch 173 Batch    1/133 train_loss = 3.241 time_per_batch = 0.353277 time_elapsed = 11128.603   time_remaining = 10760\n",
      "Model Trained and Saved\n",
      "Epoch 173 Batch  101/133 train_loss = 3.307 time_per_batch = 0.348420 time_elapsed = 11179.261   time_remaining = 10577\n",
      "Model Trained and Saved\n",
      "Epoch 174 Batch    1/133 train_loss = 3.227 time_per_batch = 0.348029 time_elapsed = 11206.076   time_remaining = 10554\n",
      "Model Trained and Saved\n",
      "Epoch 174 Batch  101/133 train_loss = 3.293 time_per_batch = 0.358259 time_elapsed = 11258.006   time_remaining = 10828\n",
      "Model Trained and Saved\n",
      "Epoch 175 Batch    1/133 train_loss = 3.220 time_per_batch = 0.364419 time_elapsed = 11285.097   time_remaining = 11002\n",
      "Model Trained and Saved\n",
      "Epoch 175 Batch  101/133 train_loss = 3.282 time_per_batch = 0.354427 time_elapsed = 11336.193   time_remaining = 10665\n",
      "Model Trained and Saved\n",
      "Epoch 176 Batch    1/133 train_loss = 3.224 time_per_batch = 0.358520 time_elapsed = 11363.277   time_remaining = 10776\n",
      "Model Trained and Saved\n",
      "Epoch 176 Batch  101/133 train_loss = 3.286 time_per_batch = 0.387669 time_elapsed = 11415.064   time_remaining = 11614\n",
      "Model Trained and Saved\n",
      "Epoch 177 Batch    1/133 train_loss = 3.209 time_per_batch = 0.363883 time_elapsed = 11442.486   time_remaining = 10889\n",
      "Model Trained and Saved\n",
      "Epoch 177 Batch  101/133 train_loss = 3.281 time_per_batch = 0.358555 time_elapsed = 11494.585   time_remaining = 10694\n",
      "Model Trained and Saved\n",
      "Epoch 178 Batch    1/133 train_loss = 3.217 time_per_batch = 0.350428 time_elapsed = 11521.528   time_remaining = 10440\n",
      "Model Trained and Saved\n",
      "Epoch 178 Batch  101/133 train_loss = 3.285 time_per_batch = 0.375231 time_elapsed = 11573.508   time_remaining = 11141\n",
      "Model Trained and Saved\n",
      "Epoch 179 Batch    1/133 train_loss = 3.216 time_per_batch = 0.369178 time_elapsed = 11601.086   time_remaining = 10949\n",
      "Model Trained and Saved\n",
      "Epoch 179 Batch  101/133 train_loss = 3.264 time_per_batch = 0.365319 time_elapsed = 11652.694   time_remaining = 10798\n",
      "Model Trained and Saved\n",
      "Epoch 180 Batch    1/133 train_loss = 3.201 time_per_batch = 0.368588 time_elapsed = 11680.508   time_remaining = 10883\n",
      "Model Trained and Saved\n",
      "Epoch 180 Batch  101/133 train_loss = 3.262 time_per_batch = 0.371269 time_elapsed = 11732.819   time_remaining = 10925\n",
      "Model Trained and Saved\n",
      "Epoch 181 Batch    1/133 train_loss = 3.209 time_per_batch = 0.371446 time_elapsed = 11760.893   time_remaining = 10918\n",
      "Model Trained and Saved\n",
      "Epoch 181 Batch  101/133 train_loss = 3.265 time_per_batch = 0.365452 time_elapsed = 11812.770   time_remaining = 10705\n",
      "Model Trained and Saved\n",
      "Epoch 182 Batch    1/133 train_loss = 3.197 time_per_batch = 0.356939 time_elapsed = 11840.491   time_remaining = 10444\n",
      "Model Trained and Saved\n",
      "Epoch 182 Batch  101/133 train_loss = 3.260 time_per_batch = 0.332136 time_elapsed = 11892.840   time_remaining = 9685\n",
      "Model Trained and Saved\n",
      "Epoch 183 Batch    1/133 train_loss = 3.205 time_per_batch = 0.358942 time_elapsed = 11920.140   time_remaining = 10455\n",
      "Model Trained and Saved\n",
      "Epoch 183 Batch  101/133 train_loss = 3.254 time_per_batch = 0.372770 time_elapsed = 11973.033   time_remaining = 10820\n",
      "Model Trained and Saved\n",
      "Epoch 184 Batch    1/133 train_loss = 3.188 time_per_batch = 0.350273 time_elapsed = 12000.230   time_remaining = 10156\n",
      "Model Trained and Saved\n",
      "Epoch 184 Batch  101/133 train_loss = 3.248 time_per_batch = 0.363199 time_elapsed = 12053.516   time_remaining = 10494\n",
      "Model Trained and Saved\n",
      "Epoch 185 Batch    1/133 train_loss = 3.191 time_per_batch = 0.353045 time_elapsed = 12081.747   time_remaining = 10189\n",
      "Model Trained and Saved\n",
      "Epoch 185 Batch  101/133 train_loss = 3.252 time_per_batch = 0.336191 time_elapsed = 12133.158   time_remaining = 9669\n",
      "Model Trained and Saved\n",
      "Epoch 186 Batch    1/133 train_loss = 3.194 time_per_batch = 0.333630 time_elapsed = 12162.906   time_remaining = 9585\n",
      "Model Trained and Saved\n",
      "Epoch 186 Batch  101/133 train_loss = 3.231 time_per_batch = 0.334638 time_elapsed = 12215.021   time_remaining = 9580\n",
      "Model Trained and Saved\n",
      "Epoch 187 Batch    1/133 train_loss = 3.167 time_per_batch = 0.347060 time_elapsed = 12245.097   time_remaining = 9924\n",
      "Model Trained and Saved\n",
      "Epoch 187 Batch  101/133 train_loss = 3.240 time_per_batch = 0.337801 time_elapsed = 12298.506   time_remaining = 9626\n",
      "Model Trained and Saved\n",
      "Epoch 188 Batch    1/133 train_loss = 3.178 time_per_batch = 0.334515 time_elapsed = 12329.326   time_remaining = 9521\n",
      "Model Trained and Saved\n",
      "Epoch 188 Batch  101/133 train_loss = 3.238 time_per_batch = 0.358383 time_elapsed = 12381.788   time_remaining = 10164\n",
      "Model Trained and Saved\n",
      "Epoch 189 Batch    1/133 train_loss = 3.169 time_per_batch = 0.344049 time_elapsed = 12411.749   time_remaining = 9747\n",
      "Model Trained and Saved\n",
      "Epoch 189 Batch  101/133 train_loss = 3.236 time_per_batch = 0.331167 time_elapsed = 12464.277   time_remaining = 9349\n",
      "Model Trained and Saved\n",
      "Epoch 190 Batch    1/133 train_loss = 3.159 time_per_batch = 0.343563 time_elapsed = 12494.261   time_remaining = 9687\n",
      "Model Trained and Saved\n",
      "Epoch 190 Batch  101/133 train_loss = 3.249 time_per_batch = 0.337355 time_elapsed = 12547.373   time_remaining = 9478\n",
      "Model Trained and Saved\n",
      "Epoch 191 Batch    1/133 train_loss = 3.163 time_per_batch = 0.335242 time_elapsed = 12578.617   time_remaining = 9408\n",
      "Model Trained and Saved\n",
      "Epoch 191 Batch  101/133 train_loss = 3.226 time_per_batch = 0.331561 time_elapsed = 12632.064   time_remaining = 9271\n",
      "Model Trained and Saved\n",
      "Epoch 192 Batch    1/133 train_loss = 3.166 time_per_batch = 0.340084 time_elapsed = 12661.969   time_remaining = 9499\n",
      "Model Trained and Saved\n",
      "Epoch 192 Batch  101/133 train_loss = 3.240 time_per_batch = 0.348798 time_elapsed = 12715.954   time_remaining = 9707\n",
      "Model Trained and Saved\n",
      "Epoch 193 Batch    1/133 train_loss = 3.192 time_per_batch = 0.330771 time_elapsed = 12745.770   time_remaining = 9194\n",
      "Model Trained and Saved\n",
      "Epoch 193 Batch  101/133 train_loss = 3.239 time_per_batch = 0.345087 time_elapsed = 12799.791   time_remaining = 9558\n",
      "Model Trained and Saved\n",
      "Epoch 194 Batch    1/133 train_loss = 3.169 time_per_batch = 0.329609 time_elapsed = 12830.223   time_remaining = 9118\n",
      "Model Trained and Saved\n",
      "Epoch 194 Batch  101/133 train_loss = 3.237 time_per_batch = 0.336571 time_elapsed = 12883.747   time_remaining = 9277\n",
      "Model Trained and Saved\n",
      "Epoch 195 Batch    1/133 train_loss = 3.166 time_per_batch = 0.322905 time_elapsed = 12914.469   time_remaining = 8890\n",
      "Model Trained and Saved\n",
      "Epoch 195 Batch  101/133 train_loss = 3.218 time_per_batch = 0.319306 time_elapsed = 12968.142   time_remaining = 8759\n",
      "Model Trained and Saved\n",
      "Epoch 196 Batch    1/133 train_loss = 3.163 time_per_batch = 0.345882 time_elapsed = 12998.477   time_remaining = 9476\n",
      "Model Trained and Saved\n",
      "Epoch 196 Batch  101/133 train_loss = 3.219 time_per_batch = 0.335557 time_elapsed = 13051.510   time_remaining = 9160\n",
      "Model Trained and Saved\n",
      "Epoch 197 Batch    1/133 train_loss = 3.148 time_per_batch = 0.333780 time_elapsed = 13083.195   time_remaining = 9101\n",
      "Model Trained and Saved\n",
      "Epoch 197 Batch  101/133 train_loss = 3.225 time_per_batch = 0.350488 time_elapsed = 13137.210   time_remaining = 9521\n",
      "Model Trained and Saved\n",
      "Epoch 198 Batch    1/133 train_loss = 3.142 time_per_batch = 0.335494 time_elapsed = 13167.635   time_remaining = 9103\n",
      "Model Trained and Saved\n",
      "Epoch 198 Batch  101/133 train_loss = 3.215 time_per_batch = 0.343127 time_elapsed = 13220.459   time_remaining = 9275\n",
      "Model Trained and Saved\n",
      "Epoch 199 Batch    1/133 train_loss = 3.153 time_per_batch = 0.342859 time_elapsed = 13249.568   time_remaining = 9257\n",
      "Model Trained and Saved\n",
      "Epoch 199 Batch  101/133 train_loss = 3.228 time_per_batch = 0.314176 time_elapsed = 13303.625   time_remaining = 8451\n",
      "Model Trained and Saved\n",
      "Epoch 200 Batch    1/133 train_loss = 3.147 time_per_batch = 0.352028 time_elapsed = 13332.863   time_remaining = 9458\n",
      "Model Trained and Saved\n",
      "Epoch 200 Batch  101/133 train_loss = 3.200 time_per_batch = 0.351233 time_elapsed = 13387.069   time_remaining = 9401\n",
      "Model Trained and Saved\n",
      "Epoch 201 Batch    1/133 train_loss = 3.142 time_per_batch = 0.355053 time_elapsed = 13416.232   time_remaining = 9492\n",
      "Model Trained and Saved\n",
      "Epoch 201 Batch  101/133 train_loss = 3.206 time_per_batch = 0.369895 time_elapsed = 13468.852   time_remaining = 9851\n",
      "Model Trained and Saved\n",
      "Epoch 202 Batch    1/133 train_loss = 3.147 time_per_batch = 0.354045 time_elapsed = 13499.028   time_remaining = 9418\n",
      "Model Trained and Saved\n",
      "Epoch 202 Batch  101/133 train_loss = 3.188 time_per_batch = 0.390335 time_elapsed = 13552.416   time_remaining = 10344\n",
      "Model Trained and Saved\n",
      "Epoch 203 Batch    1/133 train_loss = 3.149 time_per_batch = 0.359897 time_elapsed = 13581.795   time_remaining = 9525\n",
      "Model Trained and Saved\n",
      "Epoch 203 Batch  101/133 train_loss = 3.201 time_per_batch = 0.354552 time_elapsed = 13634.282   time_remaining = 9348\n",
      "Model Trained and Saved\n",
      "Epoch 204 Batch    1/133 train_loss = 3.139 time_per_batch = 0.357142 time_elapsed = 13664.090   time_remaining = 9405\n",
      "Model Trained and Saved\n",
      "Epoch 204 Batch  101/133 train_loss = 3.191 time_per_batch = 0.356150 time_elapsed = 13717.949   time_remaining = 9343\n",
      "Model Trained and Saved\n",
      "Epoch 205 Batch    1/133 train_loss = 3.129 time_per_batch = 0.361045 time_elapsed = 13747.345   time_remaining = 9460\n",
      "Model Trained and Saved\n",
      "Epoch 205 Batch  101/133 train_loss = 3.184 time_per_batch = 0.350955 time_elapsed = 13800.750   time_remaining = 9160\n",
      "Model Trained and Saved\n",
      "Epoch 206 Batch    1/133 train_loss = 3.135 time_per_batch = 0.354070 time_elapsed = 13830.351   time_remaining = 9230\n",
      "Model Trained and Saved\n",
      "Epoch 206 Batch  101/133 train_loss = 3.195 time_per_batch = 0.352406 time_elapsed = 13883.354   time_remaining = 9151\n",
      "Model Trained and Saved\n",
      "Epoch 207 Batch    1/133 train_loss = 3.124 time_per_batch = 0.352976 time_elapsed = 13913.389   time_remaining = 9154\n",
      "Model Trained and Saved\n",
      "Epoch 207 Batch  101/133 train_loss = 3.177 time_per_batch = 0.362831 time_elapsed = 13967.397   time_remaining = 9374\n",
      "Model Trained and Saved\n",
      "Epoch 208 Batch    1/133 train_loss = 3.116 time_per_batch = 0.361928 time_elapsed = 13997.787   time_remaining = 9338\n",
      "Model Trained and Saved\n",
      "Epoch 208 Batch  101/133 train_loss = 3.183 time_per_batch = 0.344367 time_elapsed = 14051.374   time_remaining = 8851\n",
      "Model Trained and Saved\n",
      "Epoch 209 Batch    1/133 train_loss = 3.128 time_per_batch = 0.355526 time_elapsed = 14081.533   time_remaining = 9126\n",
      "Model Trained and Saved\n",
      "Epoch 209 Batch  101/133 train_loss = 3.182 time_per_batch = 0.355962 time_elapsed = 14135.725   time_remaining = 9102\n",
      "Model Trained and Saved\n",
      "Epoch 210 Batch    1/133 train_loss = 3.104 time_per_batch = 0.367355 time_elapsed = 14166.029   time_remaining = 9381\n",
      "Model Trained and Saved\n",
      "Epoch 210 Batch  101/133 train_loss = 3.172 time_per_batch = 0.361148 time_elapsed = 14218.848   time_remaining = 9186\n",
      "Model Trained and Saved\n",
      "Epoch 211 Batch    1/133 train_loss = 3.112 time_per_batch = 0.339005 time_elapsed = 14249.196   time_remaining = 8612\n",
      "Model Trained and Saved\n",
      "Epoch 211 Batch  101/133 train_loss = 3.163 time_per_batch = 0.346882 time_elapsed = 14302.660   time_remaining = 8777\n",
      "Model Trained and Saved\n",
      "Epoch 212 Batch    1/133 train_loss = 3.122 time_per_batch = 0.364720 time_elapsed = 14332.468   time_remaining = 9216\n",
      "Model Trained and Saved\n",
      "Epoch 212 Batch  101/133 train_loss = 3.189 time_per_batch = 0.358859 time_elapsed = 14386.730   time_remaining = 9032\n",
      "Model Trained and Saved\n",
      "Epoch 213 Batch    1/133 train_loss = 3.098 time_per_batch = 0.354805 time_elapsed = 14416.789   time_remaining = 8919\n",
      "Model Trained and Saved\n",
      "Epoch 213 Batch  101/133 train_loss = 3.165 time_per_batch = 0.362190 time_elapsed = 14470.887   time_remaining = 9068\n",
      "Model Trained and Saved\n",
      "Epoch 214 Batch    1/133 train_loss = 3.105 time_per_batch = 0.361016 time_elapsed = 14501.581   time_remaining = 9027\n",
      "Model Trained and Saved\n",
      "Epoch 214 Batch  101/133 train_loss = 3.160 time_per_batch = 0.351545 time_elapsed = 14554.733   time_remaining = 8755\n",
      "Model Trained and Saved\n",
      "Epoch 215 Batch    1/133 train_loss = 3.116 time_per_batch = 0.361452 time_elapsed = 14585.727   time_remaining = 8990\n",
      "Model Trained and Saved\n",
      "Epoch 215 Batch  101/133 train_loss = 3.172 time_per_batch = 0.357436 time_elapsed = 14638.975   time_remaining = 8854\n",
      "Model Trained and Saved\n",
      "Epoch 216 Batch    1/133 train_loss = 3.114 time_per_batch = 0.358248 time_elapsed = 14669.636   time_remaining = 8862\n",
      "Model Trained and Saved\n",
      "Epoch 216 Batch  101/133 train_loss = 3.165 time_per_batch = 0.359173 time_elapsed = 14723.966   time_remaining = 8849\n",
      "Model Trained and Saved\n",
      "Epoch 217 Batch    1/133 train_loss = 3.102 time_per_batch = 0.357048 time_elapsed = 14754.333   time_remaining = 8785\n",
      "Model Trained and Saved\n",
      "Epoch 217 Batch  101/133 train_loss = 3.161 time_per_batch = 0.353717 time_elapsed = 14808.319   time_remaining = 8668\n",
      "Model Trained and Saved\n",
      "Epoch 218 Batch    1/133 train_loss = 3.093 time_per_batch = 0.333862 time_elapsed = 14839.805   time_remaining = 8170\n",
      "Model Trained and Saved\n",
      "Epoch 218 Batch  101/133 train_loss = 3.150 time_per_batch = 0.335110 time_elapsed = 14893.920   time_remaining = 8167\n",
      "Model Trained and Saved\n",
      "Epoch 219 Batch    1/133 train_loss = 3.100 time_per_batch = 0.353136 time_elapsed = 14924.843   time_remaining = 8595\n",
      "Model Trained and Saved\n",
      "Epoch 219 Batch  101/133 train_loss = 3.142 time_per_batch = 0.357267 time_elapsed = 14979.327   time_remaining = 8660\n",
      "Model Trained and Saved\n",
      "Epoch 220 Batch    1/133 train_loss = 3.090 time_per_batch = 0.345774 time_elapsed = 15009.040   time_remaining = 8370\n",
      "Model Trained and Saved\n",
      "Epoch 220 Batch  101/133 train_loss = 3.159 time_per_batch = 0.356811 time_elapsed = 15064.046   time_remaining = 8601\n",
      "Model Trained and Saved\n",
      "Epoch 221 Batch    1/133 train_loss = 3.075 time_per_batch = 0.368551 time_elapsed = 15094.977   time_remaining = 8872\n",
      "Model Trained and Saved\n",
      "Epoch 221 Batch  101/133 train_loss = 3.144 time_per_batch = 0.356204 time_elapsed = 15149.723   time_remaining = 8539\n",
      "Model Trained and Saved\n",
      "Epoch 222 Batch    1/133 train_loss = 3.088 time_per_batch = 0.360714 time_elapsed = 15179.874   time_remaining = 8635\n",
      "Model Trained and Saved\n",
      "Epoch 222 Batch  101/133 train_loss = 3.148 time_per_batch = 0.362490 time_elapsed = 15235.101   time_remaining = 8642\n",
      "Model Trained and Saved\n",
      "Epoch 223 Batch    1/133 train_loss = 3.076 time_per_batch = 0.352393 time_elapsed = 15266.040   time_remaining = 8389\n",
      "Model Trained and Saved\n",
      "Epoch 223 Batch  101/133 train_loss = 3.140 time_per_batch = 0.361239 time_elapsed = 15320.693   time_remaining = 8564\n",
      "Model Trained and Saved\n",
      "Epoch 224 Batch    1/133 train_loss = 3.069 time_per_batch = 0.333217 time_elapsed = 15352.256   time_remaining = 7889\n",
      "Model Trained and Saved\n",
      "Epoch 224 Batch  101/133 train_loss = 3.146 time_per_batch = 0.361130 time_elapsed = 15406.767   time_remaining = 8513\n",
      "Model Trained and Saved\n",
      "Epoch 225 Batch    1/133 train_loss = 3.081 time_per_batch = 0.355058 time_elapsed = 15437.906   time_remaining = 8358\n",
      "Model Trained and Saved\n",
      "Epoch 225 Batch  101/133 train_loss = 3.148 time_per_batch = 0.350953 time_elapsed = 15492.720   time_remaining = 8227\n",
      "Model Trained and Saved\n",
      "Epoch 226 Batch    1/133 train_loss = 3.076 time_per_batch = 0.360893 time_elapsed = 15524.547   time_remaining = 8448\n",
      "Model Trained and Saved\n",
      "Epoch 226 Batch  101/133 train_loss = 3.143 time_per_batch = 0.360949 time_elapsed = 15578.703   time_remaining = 8413\n",
      "Model Trained and Saved\n",
      "Epoch 227 Batch    1/133 train_loss = 3.075 time_per_batch = 0.360821 time_elapsed = 15610.317   time_remaining = 8398\n",
      "Model Trained and Saved\n",
      "Epoch 227 Batch  101/133 train_loss = 3.145 time_per_batch = 0.361927 time_elapsed = 15665.456   time_remaining = 8388\n",
      "Model Trained and Saved\n",
      "Epoch 228 Batch    1/133 train_loss = 3.079 time_per_batch = 0.328134 time_elapsed = 15696.361   time_remaining = 7594\n",
      "Model Trained and Saved\n",
      "Epoch 228 Batch  101/133 train_loss = 3.132 time_per_batch = 0.361975 time_elapsed = 15752.666   time_remaining = 8341\n",
      "Model Trained and Saved\n",
      "Epoch 229 Batch    1/133 train_loss = 3.080 time_per_batch = 0.352672 time_elapsed = 15783.985   time_remaining = 8115\n",
      "Model Trained and Saved\n",
      "Epoch 229 Batch  101/133 train_loss = 3.131 time_per_batch = 0.362838 time_elapsed = 15839.851   time_remaining = 8312\n",
      "Model Trained and Saved\n",
      "Epoch 230 Batch    1/133 train_loss = 3.079 time_per_batch = 0.355785 time_elapsed = 15870.623   time_remaining = 8139\n",
      "Model Trained and Saved\n",
      "Epoch 230 Batch  101/133 train_loss = 3.127 time_per_batch = 0.358191 time_elapsed = 15926.485   time_remaining = 8158\n",
      "Model Trained and Saved\n",
      "Epoch 231 Batch    1/133 train_loss = 3.077 time_per_batch = 0.341722 time_elapsed = 15957.220   time_remaining = 7772\n",
      "Model Trained and Saved\n",
      "Epoch 231 Batch  101/133 train_loss = 3.129 time_per_batch = 0.363297 time_elapsed = 16013.158   time_remaining = 8226\n",
      "Model Trained and Saved\n",
      "Epoch 232 Batch    1/133 train_loss = 3.066 time_per_batch = 0.333110 time_elapsed = 16044.851   time_remaining = 7532\n",
      "Model Trained and Saved\n",
      "Epoch 232 Batch  101/133 train_loss = 3.116 time_per_batch = 0.350921 time_elapsed = 16100.122   time_remaining = 7899\n",
      "Model Trained and Saved\n",
      "Epoch 233 Batch    1/133 train_loss = 3.068 time_per_batch = 0.355756 time_elapsed = 16132.016   time_remaining = 7996\n",
      "Model Trained and Saved\n",
      "Epoch 233 Batch  101/133 train_loss = 3.120 time_per_batch = 0.350137 time_elapsed = 16187.769   time_remaining = 7835\n",
      "Model Trained and Saved\n",
      "Epoch 234 Batch    1/133 train_loss = 3.064 time_per_batch = 0.339117 time_elapsed = 16219.468   time_remaining = 7577\n",
      "Model Trained and Saved\n",
      "Epoch 234 Batch  101/133 train_loss = 3.109 time_per_batch = 0.356094 time_elapsed = 16275.180   time_remaining = 7921\n",
      "Model Trained and Saved\n",
      "Epoch 235 Batch    1/133 train_loss = 3.056 time_per_batch = 0.335013 time_elapsed = 16306.779   time_remaining = 7441\n",
      "Model Trained and Saved\n",
      "Epoch 235 Batch  101/133 train_loss = 3.124 time_per_batch = 0.356985 time_elapsed = 16362.683   time_remaining = 7893\n",
      "Model Trained and Saved\n",
      "Epoch 236 Batch    1/133 train_loss = 3.067 time_per_batch = 0.353817 time_elapsed = 16393.862   time_remaining = 7812\n",
      "Model Trained and Saved\n",
      "Epoch 236 Batch  101/133 train_loss = 3.119 time_per_batch = 0.325104 time_elapsed = 16450.001   time_remaining = 7145\n",
      "Model Trained and Saved\n",
      "Epoch 237 Batch    1/133 train_loss = 3.063 time_per_batch = 0.352873 time_elapsed = 16482.054   time_remaining = 7744\n",
      "Model Trained and Saved\n",
      "Epoch 237 Batch  101/133 train_loss = 3.110 time_per_batch = 0.355535 time_elapsed = 16538.488   time_remaining = 7767\n",
      "Model Trained and Saved\n",
      "Epoch 238 Batch    1/133 train_loss = 3.068 time_per_batch = 0.359265 time_elapsed = 16570.887   time_remaining = 7836\n",
      "Model Trained and Saved\n",
      "Epoch 238 Batch  101/133 train_loss = 3.119 time_per_batch = 0.356590 time_elapsed = 16626.716   time_remaining = 7742\n",
      "Model Trained and Saved\n",
      "Epoch 239 Batch    1/133 train_loss = 3.065 time_per_batch = 0.358441 time_elapsed = 16658.841   time_remaining = 7771\n",
      "Model Trained and Saved\n",
      "Epoch 239 Batch  101/133 train_loss = 3.113 time_per_batch = 0.356771 time_elapsed = 16714.984   time_remaining = 7699\n",
      "Model Trained and Saved\n",
      "Epoch 240 Batch    1/133 train_loss = 3.047 time_per_batch = 0.356092 time_elapsed = 16747.556   time_remaining = 7672\n",
      "Model Trained and Saved\n",
      "Epoch 240 Batch  101/133 train_loss = 3.113 time_per_batch = 0.359094 time_elapsed = 16803.446   time_remaining = 7701\n",
      "Model Trained and Saved\n",
      "Epoch 241 Batch    1/133 train_loss = 3.040 time_per_batch = 0.362787 time_elapsed = 16836.243   time_remaining = 7768\n",
      "Model Trained and Saved\n",
      "Epoch 241 Batch  101/133 train_loss = 3.106 time_per_batch = 0.360178 time_elapsed = 16892.547   time_remaining = 7676\n",
      "Model Trained and Saved\n",
      "Epoch 242 Batch    1/133 train_loss = 3.039 time_per_batch = 0.368479 time_elapsed = 16924.764   time_remaining = 7841\n",
      "Model Trained and Saved\n",
      "Epoch 242 Batch  101/133 train_loss = 3.107 time_per_batch = 0.362286 time_elapsed = 16980.358   time_remaining = 7673\n",
      "Model Trained and Saved\n",
      "Epoch 243 Batch    1/133 train_loss = 3.041 time_per_batch = 0.360783 time_elapsed = 17013.607   time_remaining = 7629\n",
      "Model Trained and Saved\n",
      "Epoch 243 Batch  101/133 train_loss = 3.102 time_per_batch = 0.366065 time_elapsed = 17070.172   time_remaining = 7705\n",
      "Model Trained and Saved\n",
      "Epoch 244 Batch    1/133 train_loss = 3.047 time_per_batch = 0.347461 time_elapsed = 17103.515   time_remaining = 7302\n",
      "Model Trained and Saved\n",
      "Epoch 244 Batch  101/133 train_loss = 3.097 time_per_batch = 0.372288 time_elapsed = 17159.366   time_remaining = 7786\n",
      "Model Trained and Saved\n",
      "Epoch 245 Batch    1/133 train_loss = 3.063 time_per_batch = 0.359442 time_elapsed = 17192.574   time_remaining = 7506\n",
      "Model Trained and Saved\n",
      "Epoch 245 Batch  101/133 train_loss = 3.098 time_per_batch = 0.342632 time_elapsed = 17248.103   time_remaining = 7120\n",
      "Model Trained and Saved\n",
      "Epoch 246 Batch    1/133 train_loss = 3.044 time_per_batch = 0.330552 time_elapsed = 17281.407   time_remaining = 6858\n",
      "Model Trained and Saved\n",
      "Epoch 246 Batch  101/133 train_loss = 3.089 time_per_batch = 0.345596 time_elapsed = 17337.118   time_remaining = 7136\n",
      "Model Trained and Saved\n",
      "Epoch 247 Batch    1/133 train_loss = 3.040 time_per_batch = 0.357889 time_elapsed = 17370.661   time_remaining = 7378\n",
      "Model Trained and Saved\n",
      "Epoch 247 Batch  101/133 train_loss = 3.091 time_per_batch = 0.332529 time_elapsed = 17427.298   time_remaining = 6822\n",
      "Model Trained and Saved\n",
      "Epoch 248 Batch    1/133 train_loss = 3.026 time_per_batch = 0.370971 time_elapsed = 17460.709   time_remaining = 7598\n",
      "Model Trained and Saved\n",
      "Epoch 248 Batch  101/133 train_loss = 3.086 time_per_batch = 0.339529 time_elapsed = 17517.570   time_remaining = 6920\n",
      "Model Trained and Saved\n",
      "Epoch 249 Batch    1/133 train_loss = 3.028 time_per_batch = 0.354438 time_elapsed = 17550.260   time_remaining = 7212\n",
      "Model Trained and Saved\n",
      "Epoch 249 Batch  101/133 train_loss = 3.084 time_per_batch = 0.367679 time_elapsed = 17606.724   time_remaining = 7445\n",
      "Model Trained and Saved\n",
      "Epoch 250 Batch    1/133 train_loss = 3.030 time_per_batch = 0.346948 time_elapsed = 17640.190   time_remaining = 7014\n",
      "Model Trained and Saved\n",
      "Epoch 250 Batch  101/133 train_loss = 3.091 time_per_batch = 0.360861 time_elapsed = 17696.844   time_remaining = 7259\n",
      "Model Trained and Saved\n",
      "Epoch 251 Batch    1/133 train_loss = 3.047 time_per_batch = 0.355519 time_elapsed = 17729.878   time_remaining = 7140\n",
      "Model Trained and Saved\n",
      "Epoch 251 Batch  101/133 train_loss = 3.092 time_per_batch = 0.362694 time_elapsed = 17787.086   time_remaining = 7248\n",
      "Model Trained and Saved\n",
      "Epoch 252 Batch    1/133 train_loss = 3.043 time_per_batch = 0.349139 time_elapsed = 17820.148   time_remaining = 6965\n",
      "Model Trained and Saved\n",
      "Epoch 252 Batch  101/133 train_loss = 3.085 time_per_batch = 0.356552 time_elapsed = 17878.340   time_remaining = 7078\n",
      "Model Trained and Saved\n",
      "Epoch 253 Batch    1/133 train_loss = 3.031 time_per_batch = 0.364534 time_elapsed = 17911.005   time_remaining = 7224\n",
      "Model Trained and Saved\n",
      "Epoch 253 Batch  101/133 train_loss = 3.084 time_per_batch = 0.354047 time_elapsed = 17968.617   time_remaining = 6981\n",
      "Model Trained and Saved\n",
      "Epoch 254 Batch    1/133 train_loss = 3.007 time_per_batch = 0.365634 time_elapsed = 18001.650   time_remaining = 7197\n",
      "Model Trained and Saved\n",
      "Epoch 254 Batch  101/133 train_loss = 3.083 time_per_batch = 0.359236 time_elapsed = 18059.668   time_remaining = 7035\n",
      "Model Trained and Saved\n",
      "Epoch 255 Batch    1/133 train_loss = 3.015 time_per_batch = 0.356513 time_elapsed = 18092.786   time_remaining = 6970\n",
      "Model Trained and Saved\n",
      "Epoch 255 Batch  101/133 train_loss = 3.080 time_per_batch = 0.365292 time_elapsed = 18150.633   time_remaining = 7105\n",
      "Model Trained and Saved\n",
      "Epoch 256 Batch    1/133 train_loss = 3.003 time_per_batch = 0.359512 time_elapsed = 18183.866   time_remaining = 6981\n",
      "Model Trained and Saved\n",
      "Epoch 256 Batch  101/133 train_loss = 3.071 time_per_batch = 0.354359 time_elapsed = 18241.016   time_remaining = 6846\n",
      "Model Trained and Saved\n",
      "Epoch 257 Batch    1/133 train_loss = 3.014 time_per_batch = 0.364812 time_elapsed = 18274.765   time_remaining = 7035\n",
      "Model Trained and Saved\n",
      "Epoch 257 Batch  101/133 train_loss = 3.080 time_per_batch = 0.345037 time_elapsed = 18331.993   time_remaining = 6620\n",
      "Model Trained and Saved\n",
      "Epoch 258 Batch    1/133 train_loss = 3.019 time_per_batch = 0.352601 time_elapsed = 18365.947   time_remaining = 6753\n",
      "Model Trained and Saved\n",
      "Epoch 258 Batch  101/133 train_loss = 3.083 time_per_batch = 0.340234 time_elapsed = 18423.392   time_remaining = 6482\n",
      "Model Trained and Saved\n",
      "Epoch 259 Batch    1/133 train_loss = 3.021 time_per_batch = 0.354840 time_elapsed = 18457.908   time_remaining = 6749\n",
      "Model Trained and Saved\n",
      "Epoch 259 Batch  101/133 train_loss = 3.064 time_per_batch = 0.354787 time_elapsed = 18515.217   time_remaining = 6712\n",
      "Model Trained and Saved\n",
      "Epoch 260 Batch    1/133 train_loss = 3.013 time_per_batch = 0.352107 time_elapsed = 18549.667   time_remaining = 6650\n",
      "Model Trained and Saved\n",
      "Epoch 260 Batch  101/133 train_loss = 3.073 time_per_batch = 0.341830 time_elapsed = 18607.293   time_remaining = 6422\n",
      "Model Trained and Saved\n",
      "Epoch 261 Batch    1/133 train_loss = 3.006 time_per_batch = 0.341566 time_elapsed = 18641.144   time_remaining = 6405\n",
      "Model Trained and Saved\n",
      "Epoch 261 Batch  101/133 train_loss = 3.071 time_per_batch = 0.347640 time_elapsed = 18698.972   time_remaining = 6485\n",
      "Model Trained and Saved\n",
      "Epoch 262 Batch    1/133 train_loss = 3.024 time_per_batch = 0.365958 time_elapsed = 18733.442   time_remaining = 6814\n",
      "Model Trained and Saved\n",
      "Epoch 262 Batch  101/133 train_loss = 3.082 time_per_batch = 0.359875 time_elapsed = 18791.782   time_remaining = 6665\n",
      "Model Trained and Saved\n",
      "Epoch 263 Batch    1/133 train_loss = 3.015 time_per_batch = 0.346199 time_elapsed = 18824.792   time_remaining = 6400\n",
      "Model Trained and Saved\n",
      "Epoch 263 Batch  101/133 train_loss = 3.058 time_per_batch = 0.354311 time_elapsed = 18883.138   time_remaining = 6515\n",
      "Model Trained and Saved\n",
      "Epoch 264 Batch    1/133 train_loss = 3.020 time_per_batch = 0.361086 time_elapsed = 18917.040   time_remaining = 6627\n",
      "Model Trained and Saved\n",
      "Epoch 264 Batch  101/133 train_loss = 3.072 time_per_batch = 0.359561 time_elapsed = 18975.719   time_remaining = 6563\n",
      "Model Trained and Saved\n",
      "Epoch 265 Batch    1/133 train_loss = 3.020 time_per_batch = 0.369495 time_elapsed = 19009.174   time_remaining = 6733\n",
      "Model Trained and Saved\n",
      "Epoch 265 Batch  101/133 train_loss = 3.073 time_per_batch = 0.347320 time_elapsed = 19067.855   time_remaining = 6294\n",
      "Model Trained and Saved\n",
      "Epoch 266 Batch    1/133 train_loss = 2.994 time_per_batch = 0.349416 time_elapsed = 19101.789   time_remaining = 6320\n",
      "Model Trained and Saved\n",
      "Epoch 266 Batch  101/133 train_loss = 3.065 time_per_batch = 0.357967 time_elapsed = 19160.964   time_remaining = 6439\n",
      "Model Trained and Saved\n",
      "Epoch 267 Batch    1/133 train_loss = 2.997 time_per_batch = 0.360775 time_elapsed = 19195.283   time_remaining = 6478\n",
      "Model Trained and Saved\n",
      "Epoch 267 Batch  101/133 train_loss = 3.053 time_per_batch = 0.329807 time_elapsed = 19253.575   time_remaining = 5889\n",
      "Model Trained and Saved\n",
      "Epoch 268 Batch    1/133 train_loss = 2.999 time_per_batch = 0.363994 time_elapsed = 19287.875   time_remaining = 6487\n",
      "Model Trained and Saved\n",
      "Epoch 268 Batch  101/133 train_loss = 3.073 time_per_batch = 0.346019 time_elapsed = 19345.850   time_remaining = 6132\n",
      "Model Trained and Saved\n",
      "Epoch 269 Batch    1/133 train_loss = 2.993 time_per_batch = 0.358706 time_elapsed = 19380.447   time_remaining = 6345\n",
      "Model Trained and Saved\n",
      "Epoch 269 Batch  101/133 train_loss = 3.046 time_per_batch = 0.356850 time_elapsed = 19438.864   time_remaining = 6277\n",
      "Model Trained and Saved\n",
      "Epoch 270 Batch    1/133 train_loss = 3.002 time_per_batch = 0.354917 time_elapsed = 19473.948   time_remaining = 6231\n",
      "Model Trained and Saved\n",
      "Epoch 270 Batch  101/133 train_loss = 3.047 time_per_batch = 0.359904 time_elapsed = 19531.617   time_remaining = 6282\n",
      "Model Trained and Saved\n",
      "Epoch 271 Batch    1/133 train_loss = 3.003 time_per_batch = 0.356757 time_elapsed = 19567.109   time_remaining = 6216\n",
      "Model Trained and Saved\n",
      "Epoch 271 Batch  101/133 train_loss = 3.039 time_per_batch = 0.359147 time_elapsed = 19624.706   time_remaining = 6221\n",
      "Model Trained and Saved\n",
      "Epoch 272 Batch    1/133 train_loss = 2.989 time_per_batch = 0.361826 time_elapsed = 19660.201   time_remaining = 6256\n",
      "Model Trained and Saved\n",
      "Epoch 272 Batch  101/133 train_loss = 3.059 time_per_batch = 0.342671 time_elapsed = 19718.948   time_remaining = 5891\n",
      "Model Trained and Saved\n",
      "Epoch 273 Batch    1/133 train_loss = 2.975 time_per_batch = 0.364605 time_elapsed = 19753.704   time_remaining = 6256\n",
      "Model Trained and Saved\n",
      "Epoch 273 Batch  101/133 train_loss = 3.041 time_per_batch = 0.356237 time_elapsed = 19813.583   time_remaining = 6076\n",
      "Model Trained and Saved\n",
      "Epoch 274 Batch    1/133 train_loss = 2.983 time_per_batch = 0.356222 time_elapsed = 19848.509   time_remaining = 6064\n",
      "Model Trained and Saved\n",
      "Epoch 274 Batch  101/133 train_loss = 3.051 time_per_batch = 0.348019 time_elapsed = 19907.892   time_remaining = 5890\n",
      "Model Trained and Saved\n",
      "Epoch 275 Batch    1/133 train_loss = 2.980 time_per_batch = 0.353776 time_elapsed = 19942.821   time_remaining = 5976\n",
      "Model Trained and Saved\n",
      "Epoch 275 Batch  101/133 train_loss = 3.043 time_per_batch = 0.355037 time_elapsed = 20001.150   time_remaining = 5961\n",
      "Model Trained and Saved\n",
      "Epoch 276 Batch    1/133 train_loss = 2.977 time_per_batch = 0.336936 time_elapsed = 20036.701   time_remaining = 5646\n",
      "Model Trained and Saved\n",
      "Epoch 276 Batch  101/133 train_loss = 3.053 time_per_batch = 0.346042 time_elapsed = 20096.010   time_remaining = 5764\n",
      "Model Trained and Saved\n",
      "Epoch 277 Batch    1/133 train_loss = 2.969 time_per_batch = 0.359347 time_elapsed = 20131.559   time_remaining = 5974\n",
      "Model Trained and Saved\n",
      "Epoch 277 Batch  101/133 train_loss = 3.043 time_per_batch = 0.345938 time_elapsed = 20190.411   time_remaining = 5717\n",
      "Model Trained and Saved\n",
      "Epoch 278 Batch    1/133 train_loss = 2.987 time_per_batch = 0.347969 time_elapsed = 20226.342   time_remaining = 5739\n",
      "Model Trained and Saved\n",
      "Epoch 278 Batch  101/133 train_loss = 3.047 time_per_batch = 0.354725 time_elapsed = 20285.515   time_remaining = 5815\n",
      "Model Trained and Saved\n",
      "Epoch 279 Batch    1/133 train_loss = 2.974 time_per_batch = 0.347080 time_elapsed = 20320.801   time_remaining = 5678\n",
      "Model Trained and Saved\n",
      "Epoch 279 Batch  101/133 train_loss = 3.034 time_per_batch = 0.363985 time_elapsed = 20380.172   time_remaining = 5918\n",
      "Model Trained and Saved\n",
      "Epoch 280 Batch    1/133 train_loss = 2.980 time_per_batch = 0.358365 time_elapsed = 20415.448   time_remaining = 5815\n",
      "Model Trained and Saved\n",
      "Epoch 280 Batch  101/133 train_loss = 3.029 time_per_batch = 0.364621 time_elapsed = 20474.607   time_remaining = 5880\n",
      "Model Trained and Saved\n",
      "Epoch 281 Batch    1/133 train_loss = 2.968 time_per_batch = 0.354806 time_elapsed = 20510.901   time_remaining = 5710\n",
      "Model Trained and Saved\n",
      "Epoch 281 Batch  101/133 train_loss = 3.049 time_per_batch = 0.362715 time_elapsed = 20570.578   time_remaining = 5801\n",
      "Model Trained and Saved\n",
      "Epoch 282 Batch    1/133 train_loss = 2.983 time_per_batch = 0.356678 time_elapsed = 20605.346   time_remaining = 5693\n",
      "Model Trained and Saved\n",
      "Epoch 282 Batch  101/133 train_loss = 3.045 time_per_batch = 0.358974 time_elapsed = 20665.674   time_remaining = 5693\n",
      "Model Trained and Saved\n",
      "Epoch 283 Batch    1/133 train_loss = 2.977 time_per_batch = 0.356773 time_elapsed = 20701.436   time_remaining = 5647\n",
      "Model Trained and Saved\n",
      "Epoch 283 Batch  101/133 train_loss = 3.023 time_per_batch = 0.347230 time_elapsed = 20760.011   time_remaining = 5461\n",
      "Model Trained and Saved\n",
      "Epoch 284 Batch    1/133 train_loss = 2.977 time_per_batch = 0.357370 time_elapsed = 20796.325   time_remaining = 5609\n",
      "Model Trained and Saved\n",
      "Epoch 284 Batch  101/133 train_loss = 3.028 time_per_batch = 0.364736 time_elapsed = 20855.941   time_remaining = 5688\n",
      "Model Trained and Saved\n",
      "Epoch 285 Batch    1/133 train_loss = 2.975 time_per_batch = 0.357216 time_elapsed = 20891.730   time_remaining = 5559\n",
      "Model Trained and Saved\n",
      "Epoch 285 Batch  101/133 train_loss = 3.017 time_per_batch = 0.365502 time_elapsed = 20951.555   time_remaining = 5651\n",
      "Model Trained and Saved\n",
      "Epoch 286 Batch    1/133 train_loss = 2.969 time_per_batch = 0.355476 time_elapsed = 20986.623   time_remaining = 5484\n",
      "Model Trained and Saved\n",
      "Epoch 286 Batch  101/133 train_loss = 3.010 time_per_batch = 0.366369 time_elapsed = 21047.095   time_remaining = 5616\n",
      "Model Trained and Saved\n",
      "Epoch 287 Batch    1/133 train_loss = 2.957 time_per_batch = 0.354992 time_elapsed = 21082.941   time_remaining = 5430\n",
      "Model Trained and Saved\n",
      "Epoch 287 Batch  101/133 train_loss = 3.018 time_per_batch = 0.361565 time_elapsed = 21143.877   time_remaining = 5494\n",
      "Model Trained and Saved\n",
      "Epoch 288 Batch    1/133 train_loss = 2.964 time_per_batch = 0.359519 time_elapsed = 21180.380   time_remaining = 5451\n",
      "Model Trained and Saved\n",
      "Epoch 288 Batch  101/133 train_loss = 3.016 time_per_batch = 0.356970 time_elapsed = 21240.652   time_remaining = 5377\n",
      "Model Trained and Saved\n",
      "Epoch 289 Batch    1/133 train_loss = 2.952 time_per_batch = 0.354593 time_elapsed = 21276.566   time_remaining = 5329\n",
      "Model Trained and Saved\n",
      "Epoch 289 Batch  101/133 train_loss = 3.016 time_per_batch = 0.336877 time_elapsed = 21336.576   time_remaining = 5029\n",
      "Model Trained and Saved\n",
      "Epoch 290 Batch    1/133 train_loss = 2.968 time_per_batch = 0.359608 time_elapsed = 21373.465   time_remaining = 5357\n",
      "Model Trained and Saved\n",
      "Epoch 290 Batch  101/133 train_loss = 3.010 time_per_batch = 0.370987 time_elapsed = 21432.593   time_remaining = 5489\n",
      "Model Trained and Saved\n",
      "Epoch 291 Batch    1/133 train_loss = 2.959 time_per_batch = 0.358496 time_elapsed = 21469.781   time_remaining = 5292\n",
      "Model Trained and Saved\n",
      "Epoch 291 Batch  101/133 train_loss = 3.010 time_per_batch = 0.336684 time_elapsed = 21530.078   time_remaining = 4937\n",
      "Model Trained and Saved\n",
      "Epoch 292 Batch    1/133 train_loss = 2.951 time_per_batch = 0.346409 time_elapsed = 21566.203   time_remaining = 5068\n",
      "Model Trained and Saved\n",
      "Epoch 292 Batch  101/133 train_loss = 3.030 time_per_batch = 0.342521 time_elapsed = 21626.974   time_remaining = 4977\n",
      "Model Trained and Saved\n",
      "Epoch 293 Batch    1/133 train_loss = 2.930 time_per_batch = 0.359934 time_elapsed = 21663.222   time_remaining = 5218\n",
      "Model Trained and Saved\n",
      "Epoch 293 Batch  101/133 train_loss = 3.010 time_per_batch = 0.361911 time_elapsed = 21723.104   time_remaining = 5210\n",
      "Model Trained and Saved\n",
      "Epoch 294 Batch    1/133 train_loss = 2.966 time_per_batch = 0.352895 time_elapsed = 21759.983   time_remaining = 5069\n",
      "Model Trained and Saved\n",
      "Epoch 294 Batch  101/133 train_loss = 2.998 time_per_batch = 0.335538 time_elapsed = 21820.366   time_remaining = 4786\n",
      "Model Trained and Saved\n",
      "Epoch 295 Batch    1/133 train_loss = 2.960 time_per_batch = 0.338231 time_elapsed = 21857.524   time_remaining = 4813\n",
      "Model Trained and Saved\n",
      "Epoch 295 Batch  101/133 train_loss = 2.991 time_per_batch = 0.355452 time_elapsed = 21917.270   time_remaining = 5023\n",
      "Model Trained and Saved\n",
      "Epoch 296 Batch    1/133 train_loss = 2.939 time_per_batch = 0.364614 time_elapsed = 21954.418   time_remaining = 5140\n",
      "Model Trained and Saved\n",
      "Epoch 296 Batch  101/133 train_loss = 3.011 time_per_batch = 0.368872 time_elapsed = 22014.593   time_remaining = 5163\n",
      "Model Trained and Saved\n",
      "Epoch 297 Batch    1/133 train_loss = 2.947 time_per_batch = 0.338561 time_elapsed = 22051.810   time_remaining = 4728\n",
      "Model Trained and Saved\n",
      "Epoch 297 Batch  101/133 train_loss = 3.015 time_per_batch = 0.357540 time_elapsed = 22112.391   time_remaining = 4957\n",
      "Model Trained and Saved\n",
      "Epoch 298 Batch    1/133 train_loss = 2.942 time_per_batch = 0.366102 time_elapsed = 22149.299   time_remaining = 5064\n",
      "Model Trained and Saved\n",
      "Epoch 298 Batch  101/133 train_loss = 3.000 time_per_batch = 0.353978 time_elapsed = 22210.704   time_remaining = 4861\n",
      "Model Trained and Saved\n",
      "Epoch 299 Batch    1/133 train_loss = 2.947 time_per_batch = 0.354227 time_elapsed = 22248.174   time_remaining = 4853\n",
      "Model Trained and Saved\n",
      "Epoch 299 Batch  101/133 train_loss = 2.992 time_per_batch = 0.363192 time_elapsed = 22309.188   time_remaining = 4939\n",
      "Model Trained and Saved\n",
      "Epoch 300 Batch    1/133 train_loss = 2.943 time_per_batch = 0.354744 time_elapsed = 22346.445   time_remaining = 4812\n",
      "Model Trained and Saved\n",
      "Epoch 300 Batch  101/133 train_loss = 2.988 time_per_batch = 0.359759 time_elapsed = 22408.094   time_remaining = 4845\n",
      "Model Trained and Saved\n",
      "Epoch 301 Batch    1/133 train_loss = 2.947 time_per_batch = 0.329028 time_elapsed = 22445.329   time_remaining = 4420\n",
      "Model Trained and Saved\n",
      "Epoch 301 Batch  101/133 train_loss = 2.992 time_per_batch = 0.359677 time_elapsed = 22506.528   time_remaining = 4796\n",
      "Model Trained and Saved\n",
      "Epoch 302 Batch    1/133 train_loss = 2.943 time_per_batch = 0.359037 time_elapsed = 22544.924   time_remaining = 4775\n",
      "Model Trained and Saved\n",
      "Epoch 302 Batch  101/133 train_loss = 2.987 time_per_batch = 0.356519 time_elapsed = 22605.433   time_remaining = 4706\n",
      "Model Trained and Saved\n",
      "Epoch 303 Batch    1/133 train_loss = 2.935 time_per_batch = 0.355397 time_elapsed = 22643.533   time_remaining = 4680\n",
      "Model Trained and Saved\n",
      "Epoch 303 Batch  101/133 train_loss = 3.020 time_per_batch = 0.392273 time_elapsed = 22704.725   time_remaining = 5126\n",
      "Model Trained and Saved\n",
      "Epoch 304 Batch    1/133 train_loss = 2.931 time_per_batch = 0.358786 time_elapsed = 22742.865   time_remaining = 4676\n",
      "Model Trained and Saved\n",
      "Epoch 304 Batch  101/133 train_loss = 2.989 time_per_batch = 0.348491 time_elapsed = 22804.535   time_remaining = 4507\n",
      "Model Trained and Saved\n",
      "Epoch 305 Batch    1/133 train_loss = 2.935 time_per_batch = 0.350460 time_elapsed = 22842.118   time_remaining = 4521\n",
      "Model Trained and Saved\n",
      "Epoch 305 Batch  101/133 train_loss = 2.984 time_per_batch = 0.355912 time_elapsed = 22903.580   time_remaining = 4556\n",
      "Model Trained and Saved\n",
      "Epoch 306 Batch    1/133 train_loss = 2.943 time_per_batch = 0.358232 time_elapsed = 22941.645   time_remaining = 4574\n",
      "Model Trained and Saved\n",
      "Epoch 306 Batch  101/133 train_loss = 2.977 time_per_batch = 0.359908 time_elapsed = 23004.614   time_remaining = 4559\n",
      "Model Trained and Saved\n",
      "Epoch 307 Batch    1/133 train_loss = 2.939 time_per_batch = 0.341809 time_elapsed = 23042.544   time_remaining = 4319\n",
      "Model Trained and Saved\n",
      "Epoch 307 Batch  101/133 train_loss = 2.992 time_per_batch = 0.344208 time_elapsed = 23103.418   time_remaining = 4315\n",
      "Model Trained and Saved\n",
      "Epoch 308 Batch    1/133 train_loss = 2.929 time_per_batch = 0.356317 time_elapsed = 23142.013   time_remaining = 4455\n",
      "Model Trained and Saved\n",
      "Epoch 308 Batch  101/133 train_loss = 3.000 time_per_batch = 0.360657 time_elapsed = 23204.057   time_remaining = 4473\n",
      "Model Trained and Saved\n",
      "Epoch 309 Batch    1/133 train_loss = 2.913 time_per_batch = 0.353595 time_elapsed = 23241.373   time_remaining = 4374\n",
      "Model Trained and Saved\n",
      "Epoch 309 Batch  101/133 train_loss = 2.997 time_per_batch = 0.366776 time_elapsed = 23303.918   time_remaining = 4500\n",
      "Model Trained and Saved\n",
      "Epoch 310 Batch    1/133 train_loss = 2.936 time_per_batch = 0.365366 time_elapsed = 23341.906   time_remaining = 4471\n",
      "Model Trained and Saved\n",
      "Epoch 310 Batch  101/133 train_loss = 2.995 time_per_batch = 0.361356 time_elapsed = 23403.647   time_remaining = 4385\n",
      "Model Trained and Saved\n",
      "Epoch 311 Batch    1/133 train_loss = 2.924 time_per_batch = 0.351310 time_elapsed = 23442.490   time_remaining = 4252\n",
      "Model Trained and Saved\n",
      "Epoch 311 Batch  101/133 train_loss = 2.978 time_per_batch = 0.334808 time_elapsed = 23504.390   time_remaining = 4019\n",
      "Model Trained and Saved\n",
      "Epoch 312 Batch    1/133 train_loss = 2.925 time_per_batch = 0.343176 time_elapsed = 23543.193   time_remaining = 4108\n",
      "Model Trained and Saved\n",
      "Epoch 312 Batch  101/133 train_loss = 3.006 time_per_batch = 0.359003 time_elapsed = 23605.125   time_remaining = 4261\n",
      "Model Trained and Saved\n",
      "Epoch 313 Batch    1/133 train_loss = 2.929 time_per_batch = 0.358020 time_elapsed = 23643.260   time_remaining = 4238\n",
      "Model Trained and Saved\n",
      "Epoch 313 Batch  101/133 train_loss = 2.994 time_per_batch = 0.375355 time_elapsed = 23706.615   time_remaining = 4406\n",
      "Model Trained and Saved\n",
      "Epoch 314 Batch    1/133 train_loss = 2.909 time_per_batch = 0.355634 time_elapsed = 23745.408   time_remaining = 4162\n",
      "Model Trained and Saved\n",
      "Epoch 314 Batch  101/133 train_loss = 2.998 time_per_batch = 0.344585 time_elapsed = 23807.264   time_remaining = 3999\n",
      "Model Trained and Saved\n",
      "Epoch 315 Batch    1/133 train_loss = 2.935 time_per_batch = 0.363907 time_elapsed = 23846.175   time_remaining = 4211\n",
      "Model Trained and Saved\n",
      "Epoch 315 Batch  101/133 train_loss = 2.996 time_per_batch = 0.361592 time_elapsed = 23908.758   time_remaining = 4148\n",
      "Model Trained and Saved\n",
      "Epoch 316 Batch    1/133 train_loss = 2.915 time_per_batch = 0.361963 time_elapsed = 23947.557   time_remaining = 4140\n",
      "Model Trained and Saved\n",
      "Epoch 316 Batch  101/133 train_loss = 2.986 time_per_batch = 0.358702 time_elapsed = 24010.277   time_remaining = 4067\n",
      "Model Trained and Saved\n",
      "Epoch 317 Batch    1/133 train_loss = 2.924 time_per_batch = 0.357036 time_elapsed = 24049.666   time_remaining = 4036\n",
      "Model Trained and Saved\n",
      "Epoch 317 Batch  101/133 train_loss = 2.987 time_per_batch = 0.358501 time_elapsed = 24111.524   time_remaining = 4017\n",
      "Model Trained and Saved\n",
      "Epoch 318 Batch    1/133 train_loss = 2.925 time_per_batch = 0.354727 time_elapsed = 24151.482   time_remaining = 3963\n",
      "Model Trained and Saved\n",
      "Epoch 318 Batch  101/133 train_loss = 2.978 time_per_batch = 0.354047 time_elapsed = 24214.904   time_remaining = 3920\n",
      "Model Trained and Saved\n",
      "Epoch 319 Batch    1/133 train_loss = 2.909 time_per_batch = 0.335661 time_elapsed = 24253.003   time_remaining = 3705\n",
      "Model Trained and Saved\n",
      "Epoch 319 Batch  101/133 train_loss = 2.975 time_per_batch = 0.353977 time_elapsed = 24316.839   time_remaining = 3872\n",
      "Model Trained and Saved\n",
      "Epoch 320 Batch    1/133 train_loss = 2.931 time_per_batch = 0.352727 time_elapsed = 24355.817   time_remaining = 3847\n",
      "Model Trained and Saved\n",
      "Epoch 320 Batch  101/133 train_loss = 2.973 time_per_batch = 0.360180 time_elapsed = 24418.559   time_remaining = 3892\n",
      "Model Trained and Saved\n",
      "Epoch 321 Batch    1/133 train_loss = 2.913 time_per_batch = 0.363882 time_elapsed = 24457.395   time_remaining = 3920\n",
      "Model Trained and Saved\n",
      "Epoch 321 Batch  101/133 train_loss = 2.975 time_per_batch = 0.356631 time_elapsed = 24520.253   time_remaining = 3806\n",
      "Model Trained and Saved\n",
      "Epoch 322 Batch    1/133 train_loss = 2.935 time_per_batch = 0.357565 time_elapsed = 24560.332   time_remaining = 3804\n",
      "Model Trained and Saved\n",
      "Epoch 322 Batch  101/133 train_loss = 2.981 time_per_batch = 0.361100 time_elapsed = 24622.666   time_remaining = 3806\n",
      "Model Trained and Saved\n",
      "Epoch 323 Batch    1/133 train_loss = 2.912 time_per_batch = 0.368041 time_elapsed = 24662.341   time_remaining = 3867\n",
      "Model Trained and Saved\n",
      "Epoch 323 Batch  101/133 train_loss = 2.967 time_per_batch = 0.359081 time_elapsed = 24725.448   time_remaining = 3737\n",
      "Model Trained and Saved\n",
      "Epoch 324 Batch    1/133 train_loss = 2.901 time_per_batch = 0.363573 time_elapsed = 24763.377   time_remaining = 3772\n",
      "Model Trained and Saved\n",
      "Epoch 324 Batch  101/133 train_loss = 2.971 time_per_batch = 0.346675 time_elapsed = 24827.049   time_remaining = 3562\n",
      "Model Trained and Saved\n",
      "Epoch 325 Batch    1/133 train_loss = 2.920 time_per_batch = 0.357851 time_elapsed = 24867.310   time_remaining = 3665\n",
      "Model Trained and Saved\n",
      "Epoch 325 Batch  101/133 train_loss = 2.967 time_per_batch = 0.356024 time_elapsed = 24929.440   time_remaining = 3610\n",
      "Model Trained and Saved\n",
      "Epoch 326 Batch    1/133 train_loss = 2.915 time_per_batch = 0.356694 time_elapsed = 24969.642   time_remaining = 3605\n",
      "Model Trained and Saved\n",
      "Epoch 326 Batch  101/133 train_loss = 2.977 time_per_batch = 0.347734 time_elapsed = 25033.386   time_remaining = 3480\n",
      "Model Trained and Saved\n",
      "Epoch 327 Batch    1/133 train_loss = 2.890 time_per_batch = 0.361199 time_elapsed = 25072.759   time_remaining = 3603\n",
      "Model Trained and Saved\n",
      "Epoch 327 Batch  101/133 train_loss = 2.985 time_per_batch = 0.347033 time_elapsed = 25137.428   time_remaining = 3427\n",
      "Model Trained and Saved\n",
      "Epoch 328 Batch    1/133 train_loss = 2.916 time_per_batch = 0.372301 time_elapsed = 25175.927   time_remaining = 3664\n",
      "Model Trained and Saved\n",
      "Epoch 328 Batch  101/133 train_loss = 2.980 time_per_batch = 0.357316 time_elapsed = 25238.655   time_remaining = 3481\n",
      "Model Trained and Saved\n",
      "Epoch 329 Batch    1/133 train_loss = 2.901 time_per_batch = 0.324023 time_elapsed = 25278.543   time_remaining = 3146\n",
      "Model Trained and Saved\n",
      "Epoch 329 Batch  101/133 train_loss = 2.971 time_per_batch = 0.360572 time_elapsed = 25343.324   time_remaining = 3465\n",
      "Model Trained and Saved\n",
      "Epoch 330 Batch    1/133 train_loss = 2.893 time_per_batch = 0.368722 time_elapsed = 25383.436   time_remaining = 3531\n",
      "Model Trained and Saved\n",
      "Epoch 330 Batch  101/133 train_loss = 2.961 time_per_batch = 0.357975 time_elapsed = 25447.272   time_remaining = 3392\n",
      "Model Trained and Saved\n",
      "Epoch 331 Batch    1/133 train_loss = 2.894 time_per_batch = 0.366444 time_elapsed = 25486.544   time_remaining = 3460\n",
      "Model Trained and Saved\n",
      "Epoch 331 Batch  101/133 train_loss = 2.954 time_per_batch = 0.350435 time_elapsed = 25551.426   time_remaining = 3274\n",
      "Model Trained and Saved\n",
      "Epoch 332 Batch    1/133 train_loss = 2.910 time_per_batch = 0.342660 time_elapsed = 25590.838   time_remaining = 3190\n",
      "Model Trained and Saved\n",
      "Epoch 332 Batch  101/133 train_loss = 2.961 time_per_batch = 0.362382 time_elapsed = 25654.576   time_remaining = 3338\n",
      "Model Trained and Saved\n",
      "Epoch 333 Batch    1/133 train_loss = 2.902 time_per_batch = 0.375183 time_elapsed = 25695.567   time_remaining = 3443\n",
      "Model Trained and Saved\n",
      "Epoch 333 Batch  101/133 train_loss = 2.935 time_per_batch = 0.364243 time_elapsed = 25759.643   time_remaining = 3306\n",
      "Model Trained and Saved\n",
      "Epoch 334 Batch    1/133 train_loss = 2.897 time_per_batch = 0.353645 time_elapsed = 25800.163   time_remaining = 3198\n",
      "Model Trained and Saved\n",
      "Epoch 334 Batch  101/133 train_loss = 2.952 time_per_batch = 0.356154 time_elapsed = 25864.276   time_remaining = 3185\n",
      "Model Trained and Saved\n",
      "Epoch 335 Batch    1/133 train_loss = 2.909 time_per_batch = 0.370828 time_elapsed = 25904.136   time_remaining = 3304\n",
      "Model Trained and Saved\n",
      "Epoch 335 Batch  101/133 train_loss = 2.966 time_per_batch = 0.357487 time_elapsed = 25967.564   time_remaining = 3150\n",
      "Model Trained and Saved\n",
      "Epoch 336 Batch    1/133 train_loss = 2.892 time_per_batch = 0.354364 time_elapsed = 26008.185   time_remaining = 3111\n",
      "Model Trained and Saved\n",
      "Epoch 336 Batch  101/133 train_loss = 2.951 time_per_batch = 0.361492 time_elapsed = 26071.823   time_remaining = 3137\n",
      "Model Trained and Saved\n",
      "Epoch 337 Batch    1/133 train_loss = 2.897 time_per_batch = 0.358317 time_elapsed = 26112.707   time_remaining = 3098\n",
      "Model Trained and Saved\n",
      "Epoch 337 Batch  101/133 train_loss = 2.948 time_per_batch = 0.329504 time_elapsed = 26176.605   time_remaining = 2816\n",
      "Model Trained and Saved\n",
      "Epoch 338 Batch    1/133 train_loss = 2.893 time_per_batch = 0.352780 time_elapsed = 26216.681   time_remaining = 3003\n",
      "Model Trained and Saved\n",
      "Epoch 338 Batch  101/133 train_loss = 2.945 time_per_batch = 0.388119 time_elapsed = 26281.820   time_remaining = 3265\n",
      "Model Trained and Saved\n",
      "Epoch 339 Batch    1/133 train_loss = 2.890 time_per_batch = 0.351550 time_elapsed = 26321.154   time_remaining = 2946\n",
      "Model Trained and Saved\n",
      "Epoch 339 Batch  101/133 train_loss = 2.935 time_per_batch = 0.362884 time_elapsed = 26386.307   time_remaining = 3004\n",
      "Model Trained and Saved\n",
      "Epoch 340 Batch    1/133 train_loss = 2.910 time_per_batch = 0.359720 time_elapsed = 26427.532   time_remaining = 2966\n",
      "Model Trained and Saved\n",
      "Epoch 340 Batch  101/133 train_loss = 2.938 time_per_batch = 0.359665 time_elapsed = 26491.161   time_remaining = 2930\n",
      "Model Trained and Saved\n",
      "Epoch 341 Batch    1/133 train_loss = 2.904 time_per_batch = 0.358835 time_elapsed = 26531.984   time_remaining = 2911\n",
      "Model Trained and Saved\n",
      "Epoch 341 Batch  101/133 train_loss = 2.939 time_per_batch = 0.343282 time_elapsed = 26597.160   time_remaining = 2751\n",
      "Model Trained and Saved\n",
      "Epoch 342 Batch    1/133 train_loss = 2.900 time_per_batch = 0.325293 time_elapsed = 26636.631   time_remaining = 2596\n",
      "Model Trained and Saved\n",
      "Epoch 342 Batch  101/133 train_loss = 2.939 time_per_batch = 0.359741 time_elapsed = 26701.814   time_remaining = 2835\n",
      "Model Trained and Saved\n",
      "Epoch 343 Batch    1/133 train_loss = 2.889 time_per_batch = 0.359744 time_elapsed = 26742.741   time_remaining = 2823\n",
      "Model Trained and Saved\n",
      "Epoch 343 Batch  101/133 train_loss = 2.957 time_per_batch = 0.348298 time_elapsed = 26808.193   time_remaining = 2698\n",
      "Model Trained and Saved\n",
      "Epoch 344 Batch    1/133 train_loss = 2.898 time_per_batch = 0.357727 time_elapsed = 26849.866   time_remaining = 2760\n",
      "Model Trained and Saved\n",
      "Epoch 344 Batch  101/133 train_loss = 2.953 time_per_batch = 0.365494 time_elapsed = 26915.068   time_remaining = 2783\n",
      "Model Trained and Saved\n",
      "Epoch 345 Batch    1/133 train_loss = 2.897 time_per_batch = 0.355709 time_elapsed = 26955.102   time_remaining = 2697\n",
      "Model Trained and Saved\n",
      "Epoch 345 Batch  101/133 train_loss = 2.940 time_per_batch = 0.354673 time_elapsed = 27020.684   time_remaining = 2653\n",
      "Model Trained and Saved\n",
      "Epoch 346 Batch    1/133 train_loss = 2.894 time_per_batch = 0.357811 time_elapsed = 27060.783   time_remaining = 2665\n",
      "Model Trained and Saved\n",
      "Epoch 346 Batch  101/133 train_loss = 2.936 time_per_batch = 0.354332 time_elapsed = 27126.594   time_remaining = 2604\n",
      "Model Trained and Saved\n",
      "Epoch 347 Batch    1/133 train_loss = 2.902 time_per_batch = 0.354385 time_elapsed = 27168.377   time_remaining = 2592\n",
      "Model Trained and Saved\n",
      "Epoch 347 Batch  101/133 train_loss = 2.937 time_per_batch = 0.367541 time_elapsed = 27232.564   time_remaining = 2652\n",
      "Model Trained and Saved\n",
      "Epoch 348 Batch    1/133 train_loss = 2.878 time_per_batch = 0.364050 time_elapsed = 27274.304   time_remaining = 2615\n",
      "Model Trained and Saved\n",
      "Epoch 348 Batch  101/133 train_loss = 2.949 time_per_batch = 0.358076 time_elapsed = 27340.483   time_remaining = 2536\n",
      "Model Trained and Saved\n",
      "Epoch 349 Batch    1/133 train_loss = 2.885 time_per_batch = 0.363730 time_elapsed = 27380.870   time_remaining = 2564\n",
      "Model Trained and Saved\n",
      "Epoch 349 Batch  101/133 train_loss = 2.942 time_per_batch = 0.358370 time_elapsed = 27446.927   time_remaining = 2490\n",
      "Model Trained and Saved\n",
      "Epoch 350 Batch    1/133 train_loss = 2.868 time_per_batch = 0.361201 time_elapsed = 27488.147   time_remaining = 2498\n",
      "Model Trained and Saved\n",
      "Epoch 350 Batch  101/133 train_loss = 2.939 time_per_batch = 0.358418 time_elapsed = 27553.908   time_remaining = 2443\n",
      "Model Trained and Saved\n",
      "Epoch 351 Batch    1/133 train_loss = 2.883 time_per_batch = 0.357790 time_elapsed = 27596.087   time_remaining = 2427\n",
      "Model Trained and Saved\n",
      "Epoch 351 Batch  101/133 train_loss = 2.929 time_per_batch = 0.357063 time_elapsed = 27661.397   time_remaining = 2386\n",
      "Model Trained and Saved\n",
      "Epoch 352 Batch    1/133 train_loss = 2.866 time_per_batch = 0.376415 time_elapsed = 27703.004   time_remaining = 2503\n",
      "Model Trained and Saved\n",
      "Epoch 352 Batch  101/133 train_loss = 2.931 time_per_batch = 0.368743 time_elapsed = 27768.694   time_remaining = 2415\n",
      "Model Trained and Saved\n",
      "Epoch 353 Batch    1/133 train_loss = 2.884 time_per_batch = 0.362047 time_elapsed = 27810.321   time_remaining = 2359\n",
      "Model Trained and Saved\n",
      "Epoch 353 Batch  101/133 train_loss = 2.931 time_per_batch = 0.355267 time_elapsed = 27875.240   time_remaining = 2280\n",
      "Model Trained and Saved\n",
      "Epoch 354 Batch    1/133 train_loss = 2.885 time_per_batch = 0.361936 time_elapsed = 27917.963   time_remaining = 2311\n",
      "Model Trained and Saved\n",
      "Epoch 354 Batch  101/133 train_loss = 2.938 time_per_batch = 0.358182 time_elapsed = 27983.446   time_remaining = 2251\n",
      "Model Trained and Saved\n",
      "Epoch 355 Batch    1/133 train_loss = 2.866 time_per_batch = 0.353081 time_elapsed = 28025.595   time_remaining = 2207\n",
      "Model Trained and Saved\n",
      "Epoch 355 Batch  101/133 train_loss = 2.924 time_per_batch = 0.352362 time_elapsed = 28091.176   time_remaining = 2167\n",
      "Model Trained and Saved\n",
      "Epoch 356 Batch    1/133 train_loss = 2.880 time_per_batch = 0.358056 time_elapsed = 28132.819   time_remaining = 2191\n",
      "Model Trained and Saved\n",
      "Epoch 356 Batch  101/133 train_loss = 2.933 time_per_batch = 0.358276 time_elapsed = 28199.325   time_remaining = 2156\n",
      "Model Trained and Saved\n",
      "Epoch 357 Batch    1/133 train_loss = 2.864 time_per_batch = 0.348706 time_elapsed = 28241.523   time_remaining = 2087\n",
      "Model Trained and Saved\n",
      "Epoch 357 Batch  101/133 train_loss = 2.929 time_per_batch = 0.347812 time_elapsed = 28307.806   time_remaining = 2047\n",
      "Model Trained and Saved\n",
      "Epoch 358 Batch    1/133 train_loss = 2.865 time_per_batch = 0.347613 time_elapsed = 28349.972   time_remaining = 2034\n",
      "Model Trained and Saved\n",
      "Epoch 358 Batch  101/133 train_loss = 2.914 time_per_batch = 0.362657 time_elapsed = 28414.877   time_remaining = 2086\n",
      "Model Trained and Saved\n",
      "Epoch 359 Batch    1/133 train_loss = 2.871 time_per_batch = 0.382824 time_elapsed = 28457.724   time_remaining = 2189\n",
      "Model Trained and Saved\n",
      "Epoch 359 Batch  101/133 train_loss = 2.919 time_per_batch = 0.344664 time_elapsed = 28524.398   time_remaining = 1937\n",
      "Model Trained and Saved\n",
      "Epoch 360 Batch    1/133 train_loss = 2.870 time_per_batch = 0.344105 time_elapsed = 28565.314   time_remaining = 1922\n",
      "Model Trained and Saved\n",
      "Epoch 360 Batch  101/133 train_loss = 2.929 time_per_batch = 0.362716 time_elapsed = 28632.619   time_remaining = 1990\n",
      "Model Trained and Saved\n",
      "Epoch 361 Batch    1/133 train_loss = 2.878 time_per_batch = 0.359208 time_elapsed = 28675.185   time_remaining = 1959\n",
      "Model Trained and Saved\n",
      "Epoch 361 Batch  101/133 train_loss = 2.910 time_per_batch = 0.361424 time_elapsed = 28741.381   time_remaining = 1935\n",
      "Model Trained and Saved\n",
      "Epoch 362 Batch    1/133 train_loss = 2.866 time_per_batch = 0.346088 time_elapsed = 28783.478   time_remaining = 1841\n",
      "Model Trained and Saved\n",
      "Epoch 362 Batch  101/133 train_loss = 2.919 time_per_batch = 0.349080 time_elapsed = 28850.485   time_remaining = 1822\n",
      "Model Trained and Saved\n",
      "Epoch 363 Batch    1/133 train_loss = 2.861 time_per_batch = 0.383079 time_elapsed = 28892.775   time_remaining = 1987\n",
      "Model Trained and Saved\n",
      "Epoch 363 Batch  101/133 train_loss = 2.937 time_per_batch = 0.348599 time_elapsed = 28959.005   time_remaining = 1773\n",
      "Model Trained and Saved\n",
      "Epoch 364 Batch    1/133 train_loss = 2.863 time_per_batch = 0.372189 time_elapsed = 29002.245   time_remaining = 1881\n",
      "Model Trained and Saved\n",
      "Epoch 364 Batch  101/133 train_loss = 2.927 time_per_batch = 0.362763 time_elapsed = 29068.785   time_remaining = 1797\n",
      "Model Trained and Saved\n",
      "Epoch 365 Batch    1/133 train_loss = 2.859 time_per_batch = 0.357315 time_elapsed = 29110.456   time_remaining = 1758\n",
      "Model Trained and Saved\n",
      "Epoch 365 Batch  101/133 train_loss = 2.920 time_per_batch = 0.355376 time_elapsed = 29177.746   time_remaining = 1713\n",
      "Model Trained and Saved\n",
      "Epoch 366 Batch    1/133 train_loss = 2.864 time_per_batch = 0.364187 time_elapsed = 29220.015   time_remaining = 1744\n",
      "Model Trained and Saved\n",
      "Epoch 366 Batch  101/133 train_loss = 2.925 time_per_batch = 0.358077 time_elapsed = 29287.486   time_remaining = 1679\n",
      "Model Trained and Saved\n",
      "Epoch 367 Batch    1/133 train_loss = 2.857 time_per_batch = 0.371258 time_elapsed = 29330.596   time_remaining = 1728\n",
      "Model Trained and Saved\n",
      "Epoch 367 Batch  101/133 train_loss = 2.919 time_per_batch = 0.364186 time_elapsed = 29396.119   time_remaining = 1659\n",
      "Model Trained and Saved\n",
      "Epoch 368 Batch    1/133 train_loss = 2.874 time_per_batch = 0.350066 time_elapsed = 29439.725   time_remaining = 1583\n",
      "Model Trained and Saved\n",
      "Epoch 368 Batch  101/133 train_loss = 2.925 time_per_batch = 0.363788 time_elapsed = 29506.880   time_remaining = 1609\n",
      "Model Trained and Saved\n",
      "Epoch 369 Batch    1/133 train_loss = 2.865 time_per_batch = 0.362745 time_elapsed = 29550.082   time_remaining = 1592\n",
      "Model Trained and Saved\n",
      "Epoch 369 Batch  101/133 train_loss = 2.923 time_per_batch = 0.356050 time_elapsed = 29616.040   time_remaining = 1527\n",
      "Model Trained and Saved\n",
      "Epoch 370 Batch    1/133 train_loss = 2.870 time_per_batch = 0.355314 time_elapsed = 29659.667   time_remaining = 1512\n",
      "Model Trained and Saved\n",
      "Epoch 370 Batch  101/133 train_loss = 2.919 time_per_batch = 0.359012 time_elapsed = 29727.524   time_remaining = 1492\n",
      "Model Trained and Saved\n",
      "Epoch 371 Batch    1/133 train_loss = 2.879 time_per_batch = 0.357186 time_elapsed = 29769.367   time_remaining = 1473\n",
      "Model Trained and Saved\n",
      "Epoch 371 Batch  101/133 train_loss = 2.930 time_per_batch = 0.361923 time_elapsed = 29837.718   time_remaining = 1456\n",
      "Model Trained and Saved\n",
      "Epoch 372 Batch    1/133 train_loss = 2.863 time_per_batch = 0.358584 time_elapsed = 29879.501   time_remaining = 1431\n",
      "Model Trained and Saved\n",
      "Epoch 372 Batch  101/133 train_loss = 2.916 time_per_batch = 0.360247 time_elapsed = 29947.518   time_remaining = 1401\n",
      "Model Trained and Saved\n",
      "Epoch 373 Batch    1/133 train_loss = 2.852 time_per_batch = 0.372820 time_elapsed = 29992.478   time_remaining = 1438\n",
      "Model Trained and Saved\n",
      "Epoch 373 Batch  101/133 train_loss = 2.910 time_per_batch = 0.357293 time_elapsed = 30058.694   time_remaining = 1342\n",
      "Model Trained and Saved\n",
      "Epoch 374 Batch    1/133 train_loss = 2.846 time_per_batch = 0.387905 time_elapsed = 30102.884   time_remaining = 1445\n",
      "Model Trained and Saved\n",
      "Epoch 374 Batch  101/133 train_loss = 2.911 time_per_batch = 0.355977 time_elapsed = 30170.265   time_remaining = 1290\n",
      "Model Trained and Saved\n",
      "Epoch 375 Batch    1/133 train_loss = 2.856 time_per_batch = 0.354712 time_elapsed = 30213.615   time_remaining = 1274\n",
      "Model Trained and Saved\n",
      "Epoch 375 Batch  101/133 train_loss = 2.929 time_per_batch = 0.361009 time_elapsed = 30279.977   time_remaining = 1260\n",
      "Model Trained and Saved\n",
      "Epoch 376 Batch    1/133 train_loss = 2.852 time_per_batch = 0.360910 time_elapsed = 30323.778   time_remaining = 1248\n",
      "Model Trained and Saved\n",
      "Epoch 376 Batch  101/133 train_loss = 2.925 time_per_batch = 0.358232 time_elapsed = 30390.762   time_remaining = 1203\n",
      "Model Trained and Saved\n",
      "Epoch 377 Batch    1/133 train_loss = 2.857 time_per_batch = 0.356012 time_elapsed = 30435.135   time_remaining = 1184\n",
      "Model Trained and Saved\n",
      "Epoch 377 Batch  101/133 train_loss = 2.904 time_per_batch = 0.392251 time_elapsed = 30502.352   time_remaining = 1265\n",
      "Model Trained and Saved\n",
      "Epoch 378 Batch    1/133 train_loss = 2.862 time_per_batch = 0.353247 time_elapsed = 30545.626   time_remaining = 1128\n",
      "Model Trained and Saved\n",
      "Epoch 378 Batch  101/133 train_loss = 2.898 time_per_batch = 0.343110 time_elapsed = 30614.216   time_remaining = 1061\n",
      "Model Trained and Saved\n",
      "Epoch 379 Batch    1/133 train_loss = 2.859 time_per_batch = 0.359566 time_elapsed = 30657.827   time_remaining = 1100\n",
      "Model Trained and Saved\n",
      "Epoch 379 Batch  101/133 train_loss = 2.911 time_per_batch = 0.358111 time_elapsed = 30725.718   time_remaining = 1060\n",
      "Model Trained and Saved\n",
      "Epoch 380 Batch    1/133 train_loss = 2.863 time_per_batch = 0.382627 time_elapsed = 30769.304   time_remaining = 1120\n",
      "Model Trained and Saved\n",
      "Epoch 380 Batch  101/133 train_loss = 2.927 time_per_batch = 0.361702 time_elapsed = 30837.643   time_remaining = 1022\n",
      "Model Trained and Saved\n",
      "Epoch 381 Batch    1/133 train_loss = 2.853 time_per_batch = 0.344480 time_elapsed = 30881.341   time_remaining = 962\n",
      "Model Trained and Saved\n",
      "Epoch 381 Batch  101/133 train_loss = 2.917 time_per_batch = 0.360423 time_elapsed = 30949.449   time_remaining = 971\n",
      "Model Trained and Saved\n",
      "Epoch 382 Batch    1/133 train_loss = 2.861 time_per_batch = 0.354729 time_elapsed = 30993.432   time_remaining = 944\n",
      "Model Trained and Saved\n",
      "Epoch 382 Batch  101/133 train_loss = 2.929 time_per_batch = 0.355202 time_elapsed = 31062.443   time_remaining = 909\n",
      "Model Trained and Saved\n",
      "Epoch 383 Batch    1/133 train_loss = 2.853 time_per_batch = 0.358310 time_elapsed = 31105.095   time_remaining = 905\n",
      "Model Trained and Saved\n",
      "Epoch 383 Batch  101/133 train_loss = 2.911 time_per_batch = 0.361579 time_elapsed = 31173.586   time_remaining = 878\n",
      "Model Trained and Saved\n",
      "Epoch 384 Batch    1/133 train_loss = 2.846 time_per_batch = 0.352876 time_elapsed = 31217.190   time_remaining = 845\n",
      "Model Trained and Saved\n",
      "Epoch 384 Batch  101/133 train_loss = 2.900 time_per_batch = 0.345341 time_elapsed = 31284.608   time_remaining = 792\n",
      "Model Trained and Saved\n",
      "Epoch 385 Batch    1/133 train_loss = 2.846 time_per_batch = 0.369313 time_elapsed = 31329.435   time_remaining = 835\n",
      "Model Trained and Saved\n",
      "Epoch 385 Batch  101/133 train_loss = 2.915 time_per_batch = 0.375213 time_elapsed = 31396.921   time_remaining = 811\n",
      "Model Trained and Saved\n",
      "Epoch 386 Batch    1/133 train_loss = 2.831 time_per_batch = 0.352564 time_elapsed = 31442.050   time_remaining = 750\n",
      "Model Trained and Saved\n",
      "Epoch 386 Batch  101/133 train_loss = 2.910 time_per_batch = 0.339959 time_elapsed = 31509.258   time_remaining = 689\n",
      "Model Trained and Saved\n",
      "Epoch 387 Batch    1/133 train_loss = 2.841 time_per_batch = 0.338691 time_elapsed = 31554.416   time_remaining = 676\n",
      "Model Trained and Saved\n",
      "Epoch 387 Batch  101/133 train_loss = 2.906 time_per_batch = 0.356069 time_elapsed = 31623.798   time_remaining = 675\n",
      "Model Trained and Saved\n",
      "Epoch 388 Batch    1/133 train_loss = 2.846 time_per_batch = 0.352277 time_elapsed = 31667.628   time_remaining = 656\n",
      "Model Trained and Saved\n",
      "Epoch 388 Batch  101/133 train_loss = 2.892 time_per_batch = 0.346630 time_elapsed = 31736.944   time_remaining = 611\n",
      "Model Trained and Saved\n",
      "Epoch 389 Batch    1/133 train_loss = 2.828 time_per_batch = 0.358452 time_elapsed = 31781.772   time_remaining = 620\n",
      "Model Trained and Saved\n",
      "Epoch 389 Batch  101/133 train_loss = 2.919 time_per_batch = 0.362015 time_elapsed = 31849.306   time_remaining = 590\n",
      "Model Trained and Saved\n",
      "Epoch 390 Batch    1/133 train_loss = 2.861 time_per_batch = 0.361853 time_elapsed = 31894.879   time_remaining = 578\n",
      "Model Trained and Saved\n",
      "Epoch 390 Batch  101/133 train_loss = 2.900 time_per_batch = 0.343379 time_elapsed = 31963.678   time_remaining = 514\n",
      "Model Trained and Saved\n",
      "Epoch 391 Batch    1/133 train_loss = 2.846 time_per_batch = 0.365423 time_elapsed = 32008.702   time_remaining = 535\n",
      "Model Trained and Saved\n",
      "Epoch 391 Batch  101/133 train_loss = 2.892 time_per_batch = 0.360441 time_elapsed = 32076.375   time_remaining = 491\n",
      "Model Trained and Saved\n",
      "Epoch 392 Batch    1/133 train_loss = 2.842 time_per_batch = 0.364954 time_elapsed = 32121.811   time_remaining = 485\n",
      "Model Trained and Saved\n",
      "Epoch 392 Batch  101/133 train_loss = 2.896 time_per_batch = 0.360072 time_elapsed = 32191.242   time_remaining = 443\n",
      "Model Trained and Saved\n",
      "Epoch 393 Batch    1/133 train_loss = 2.845 time_per_batch = 0.373071 time_elapsed = 32236.410   time_remaining = 447\n",
      "Model Trained and Saved\n",
      "Epoch 393 Batch  101/133 train_loss = 2.898 time_per_batch = 0.350981 time_elapsed = 32304.079   time_remaining = 385\n",
      "Model Trained and Saved\n",
      "Epoch 394 Batch    1/133 train_loss = 2.840 time_per_batch = 0.377624 time_elapsed = 32349.642   time_remaining = 402\n",
      "Model Trained and Saved\n",
      "Epoch 394 Batch  101/133 train_loss = 2.897 time_per_batch = 0.354302 time_elapsed = 32418.047   time_remaining = 342\n",
      "Model Trained and Saved\n",
      "Epoch 395 Batch    1/133 train_loss = 2.830 time_per_batch = 0.363264 time_elapsed = 32463.438   time_remaining = 338\n",
      "Model Trained and Saved\n",
      "Epoch 395 Batch  101/133 train_loss = 2.893 time_per_batch = 0.358878 time_elapsed = 32533.362   time_remaining = 298\n",
      "Model Trained and Saved\n",
      "Epoch 396 Batch    1/133 train_loss = 2.820 time_per_batch = 0.359181 time_elapsed = 32577.350   time_remaining = 287\n",
      "Model Trained and Saved\n",
      "Epoch 396 Batch  101/133 train_loss = 2.895 time_per_batch = 0.362219 time_elapsed = 32647.340   time_remaining = 253\n",
      "Model Trained and Saved\n",
      "Epoch 397 Batch    1/133 train_loss = 2.838 time_per_batch = 0.352718 time_elapsed = 32692.544   time_remaining = 235\n",
      "Model Trained and Saved\n",
      "Epoch 397 Batch  101/133 train_loss = 2.897 time_per_batch = 0.366814 time_elapsed = 32761.732   time_remaining = 207\n",
      "Model Trained and Saved\n",
      "Epoch 398 Batch    1/133 train_loss = 2.832 time_per_batch = 0.360475 time_elapsed = 32806.909   time_remaining = 192\n",
      "Model Trained and Saved\n",
      "Epoch 398 Batch  101/133 train_loss = 2.889 time_per_batch = 0.354817 time_elapsed = 32875.754   time_remaining = 153\n",
      "Model Trained and Saved\n",
      "Epoch 399 Batch    1/133 train_loss = 2.829 time_per_batch = 0.356781 time_elapsed = 32921.504   time_remaining = 142\n",
      "Model Trained and Saved\n",
      "Epoch 399 Batch  101/133 train_loss = 2.908 time_per_batch = 0.361114 time_elapsed = 32990.572   time_remaining = 108\n",
      "Model Trained and Saved\n",
      "Epoch 400 Batch    1/133 train_loss = 2.855 time_per_batch = 0.360879 time_elapsed = 33036.774   time_remaining = 96\n",
      "Model Trained and Saved\n",
      "Epoch 400 Batch  101/133 train_loss = 2.900 time_per_batch = 0.361778 time_elapsed = 33106.483   time_remaining = 60\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batches = get_batches(corpus_int, batch_size, seq_length)\n",
    "num_batches = len(batches)\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Num Batches per Epoche : {}, Total Epochs : {}\".format(num_batches, num_epochs))\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        \n",
    "        for batch_index, (x, y) in enumerate(batches):\n",
    "            \n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            feed_dict = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed_dict)\n",
    "            \n",
    "            if batch_index % 100 == 0:\n",
    "                time_elapsed   = time.time() - start_time\n",
    "                time_per_batch = time.time() - batch_start_time\n",
    "                num_batches_remaining = (num_epochs - epoch) * num_batches + num_batches - batch_index \n",
    "                print('Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f} time_per_batch = {:3f} time_elapsed = {:.3f}   time_remaining = {:.0f}'.format(\n",
    "                    epoch + 1,\n",
    "                    batch_index + 1,\n",
    "                    num_batches,\n",
    "                    train_loss,\n",
    "                    time_per_batch,\n",
    "                    time_elapsed,\n",
    "                    num_batches_remaining * time_per_batch))\n",
    "                \n",
    "                # save model every 100 batches\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, save_dir)\n",
    "                print('Model Trained and Saved')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text\n",
    "### Pick a Random Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word with some randomness\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Graph and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "一阵轻风吹过，屋檐下的铃铛迎风而响，绿色的衣角轻轻飘起，仿佛也带着几分笑意；清脆的铃声，随着风儿飘然而上，回荡在天地之间。\n",
      "\n",
      "    只是，它默默留下的花永恒瞬间碎裂，安静，呼吸将口而伸出，向后当去吹到他的身前，跟着那片细细的十丈，渐渐在平静强撑一口气落，颓然闭上了路上，有没有连挥洒一样，若非有一层冰寒意，如猛兽袭击此，十倍，一身劲风直接高悬，漫天雷鸣，不消光线在空中光线划过，微微寒嘴角在，露出畏惧，额头用力神色更是一变，刹那间整座陆雪琪也是变色，在大战之中深入，其中还有一个大到是觉得这曾经的时光，象是凭空清晰的异声轻轻注视，一双屋子凝望着田不易，仿佛似乎恰好地在她身旁。\n",
      "\n",
      "    幽姬慢慢在了右边，良久之后，面上的隐约在心灰意懒：‘脆弱在这么半天也早已像是无打造出来的年头，甚至全南疆灵位遮挡天下在了十万大山中，还有生机声──”鬼厉安静了过来，面对著，鬼王仍就是在原地竟然在。\n",
      "\n",
      "    鬼厉的身影抬起衣襟，远远望去，那紫芒闪烁，两道光线苍白之上急速坚定。，一双看看自己肌肤的蚊虫在眼睛，贴完的，转眼居然就要躲避放黑色处而落到的那只女子脸上掠过一丝满的彩光，看不出倒是在洞顶之上的斗法耀眼的冰凉，显然在空旷的角落中，仿佛都在这样那么的破败中的身影，似乎没有多一个身影。\n",
      "\n",
      "    一把因为鬼厉身地向三人没多久，从一搭在张小凡身旁，但衬着寒冰那剑光，在那如雪之色，轻轻转动，严阵以待，而在鬼厉身后。\n",
      "\n",
      "    整座如塔状的外围正一枚干净的无形压力小道行，当真还有我亦要有下来。\n",
      "\n",
      "    吕顺道：“就不知道，你有没有做贼心虚出去，吞来啊！”\n",
      "    张小凡倒觉得师门动作应了脸色开始怔怔一般，也笑黑大喊地，瞬间他心中一震，幽姬面茫然有几分荒凉，仿佛也慢慢涌出了另外一条身影穿过尘土岁月，缓缓地睁开眼睛近在咫尺，每一点上下和身后的摆设又有众人恭敬的，就是今日想来，苏茹也看出了他们道行的干系，今天兄台私自养大，所以对他们身边穿了鬼王屋子里的那些日子……\n",
      "\n",
      "    就带着一阵冷淡声音，慢慢中的身影就得到走的出来，只是抬头望天，细心地摇了出，穿过了无数世界的台下石室的石门，昨夜，当有风云变色，巨大回到人的身躯，从大殿之上的汹涌漩涡，这些缩小的一点光亮正在明显的时候，望着酒杯，从天而下地，在前方手中的随后传来六合闪电，只见饕餮手掌之上，人剑迎挡上而落在岩石正中，身不由己的险险已到了小环的\n",
      "\n",
      "    巫妖模样渺小的慌乱，此刻便很明显叫苦了。石头和李洵心中道行，多半就可以有吃惊遮掩，至于说下去，他倒是为了等什么什么？\n",
      "\n",
      "    陆雪琪笑了笑，道：“眼下当初那根，黄鸟乃是聚玄火坛，大家责罚你的，合欢派大名鼎鼎的青云门吧！”\n",
      "\n",
      "    鬼厉深深看了丈夫等，淡淡道：“但他却在碧瑶以往不愿意之办法，他就要我抱那我不敢夸奖，普智前就连现在还有皆在那样的中；而在杀生和尚，今日本是中原之地，随着诸位规矩遁逃，焚香谷弟子自然是被微妙的外敌，已经当中的见不管怎样，却反而集中到小环身后默不作声的小猴子随即把台下蝙蝠，到处塌的世间回峰的老巢\n",
      "而下那个阴灵，但在他身子弄之人，汗湿重衫，双手，拚命挣扎，看来当年连普通千刀之中加力遮盖只而且。\n",
      "\n",
      "    喷吐声势从远的凄苦，向前急速跳下一般数丈的！与师徒之间已然照亮了黑暗。\n",
      "\n",
      "    鬼厉微微咬着似乎，淳朴的心意。”\n",
      "\n",
      "    田不易笑了笑，道：“这个傻丫头，如此变化，应该重要的平日所变成了疯子，无论只是张小凡有某些少年所说的逐渐有些诡异的诡异之色，而且也发挥了最后的兽妖刚烈之极的对手！”\n",
      "\n",
      "    小环点了点头，如小竹峰弟子在身上肩上颇是万分，或是抱歉。\n",
      "\n",
      "    张小凡一听之下，一时颇为吃力，大步偷偷向山上而行，的确是不错，不过后来也来危机四伏了。\n",
      "\n",
      "    不过小灰时却不想手握观察，张小凡手掌，凝视着它的那个身影。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_length = 1000\n",
    "prime_words = '一阵轻风吹过，屋檐下的铃铛迎风而响，绿色的衣角轻轻飘起，仿佛也带着几分笑意；清脆的铃声，随着风儿飘然而上，回荡在天地之间。'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load the saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "    \n",
    "    # Get tensors from loaded graph\n",
    "    input_text = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    gen_sentences = list(jieba.cut(prime_words)) if USE_SPLIT else prime_words.split()\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1 for word in gen_sentences]])})\n",
    "    \n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        # Get predict word\n",
    "        word_probs = probabilities[0][dyn_seq_length-1]\n",
    "        pred_word = pick_word(word_probs, int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "        \n",
    "    # Remove tokens\n",
    "    chapter_text = ''.join(gen_sentences)\n",
    "        \n",
    "    print(chapter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('generated_text.txt', \"w\") as text_file:\n",
    "    text_file.write(chapter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
