{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import codecs\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILE_PATH = './data/诛仙.txt'\n",
    "# Whether or not use Chinese split words, if false, use single chars to feed\n",
    "USE_SPLIT = True                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the book as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus is 3126568 characters long\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "\n",
    "with codecs.open(FILE_PATH, 'r', 'utf-8') as book_file:\n",
    "    corpus_raw += book_file.read()\n",
    "\n",
    "print(\"Corpus is {} characters long\".format(len(corpus_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Corpus\n",
    "##### Create lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text, use_split=USE_SPLIT):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocab\n",
    "    :param text: The corpus text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    words = list(jieba.cut(text))\n",
    "    vocab = set(words) if use_split else set(text)\n",
    "    \n",
    "    int_to_vocab = {key: word for key, word in enumerate(vocab)}\n",
    "    vocab_to_int = {word: key for key, word in enumerate(vocab)}\n",
    "    \n",
    "    if use_split:\n",
    "        text_index = [vocab_to_int[word] for word in words]\n",
    "    else:\n",
    "        text_index = [vocab_to_int[word] for word in text]\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab, text_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.645 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 2050961, number of Chinese words in text : 38087\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int, int_to_vocab, corpus_int = create_lookup_tables(corpus_raw)\n",
    "print(\"Vocabulary size : {}, number of Chinese words in text : {}\".format(len(corpus_int), len(vocab_to_int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Network\n",
    "### Batch the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target data\n",
    "    :param int_text: text with words replaced by their ids\n",
    "    :param batch_size: the size that each batch of data should be\n",
    "    :param seq_length: the length of each sequence\n",
    "    :return: batches of data as a numpy array\n",
    "    \"\"\"\n",
    "    words_per_batch = batch_size * seq_length\n",
    "    num_batches = len(int_text)//words_per_batch\n",
    "    int_text = int_text[:num_batches*words_per_batch]\n",
    "    y = np.array(int_text[1:] + [int_text[0]])\n",
    "    x = np.array(int_text)\n",
    "    \n",
    "    x_batches = np.split(x.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    y_batches = np.split(y.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    \n",
    "    batch_data = list(zip(x_batches, y_batches))\n",
    "    \n",
    "    return np.array(batch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 512\n",
    "rnn_size = 128\n",
    "num_layers = 2\n",
    "keep_prob = 0.7\n",
    "embed_dim = 128\n",
    "seq_length = 30\n",
    "learning_rate = 0.001\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():    \n",
    "    \n",
    "    # Initialize input placeholders\n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    # Calculate text attributes\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text_shape = tf.shape(input_text)\n",
    "    \n",
    "    # Build the RNN cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_size)\n",
    "    drop_cell = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop_cell] * num_layers)\n",
    "    \n",
    "    # Set the initial state\n",
    "    initial_state = cell.zero_state(input_text_shape[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    \n",
    "    # Create word embedding as input to RNN\n",
    "    embed = tf.contrib.layers.embed_sequence(input_text, vocab_size, embed_dim)\n",
    "    \n",
    "    # Build RNN\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    \n",
    "    # Take RNN output and make logits\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    # Calculate the probability of generating each word\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "    # Define loss function\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_text_shape[0], input_text_shape[1]])\n",
    "    )\n",
    "    \n",
    "    # Learning rate optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Gradient clipping to avoid exploding gradients\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Batches per Epoche : 133, Total Epochs : 100\n",
      "Epoch   1 Batch    1/133 train_loss = 10.548 time_per_batch = 0.611360 time_elapsed = 2.020   time_remaining = 8212\n",
      "Model Trained and Saved\n",
      "Epoch   1 Batch  101/133 train_loss = 6.637 time_per_batch = 0.360910 time_elapsed = 39.689   time_remaining = 4812\n",
      "Model Trained and Saved\n",
      "Epoch   2 Batch    1/133 train_loss = 6.531 time_per_batch = 0.370923 time_elapsed = 53.505   time_remaining = 4933\n",
      "Model Trained and Saved\n",
      "Epoch   2 Batch  101/133 train_loss = 6.578 time_per_batch = 0.338533 time_elapsed = 90.681   time_remaining = 4469\n",
      "Model Trained and Saved\n",
      "Epoch   3 Batch    1/133 train_loss = 6.495 time_per_batch = 0.338503 time_elapsed = 104.345   time_remaining = 4457\n",
      "Model Trained and Saved\n",
      "Epoch   3 Batch  101/133 train_loss = 6.568 time_per_batch = 0.369280 time_elapsed = 141.478   time_remaining = 4825\n",
      "Model Trained and Saved\n",
      "Epoch   4 Batch    1/133 train_loss = 6.486 time_per_batch = 0.352144 time_elapsed = 155.543   time_remaining = 4590\n",
      "Model Trained and Saved\n",
      "Epoch   4 Batch  101/133 train_loss = 6.562 time_per_batch = 0.353925 time_elapsed = 193.306   time_remaining = 4578\n",
      "Model Trained and Saved\n",
      "Epoch   5 Batch    1/133 train_loss = 6.482 time_per_batch = 0.355008 time_elapsed = 207.526   time_remaining = 4580\n",
      "Model Trained and Saved\n",
      "Epoch   5 Batch  101/133 train_loss = 6.563 time_per_batch = 0.349789 time_elapsed = 245.585   time_remaining = 4478\n",
      "Model Trained and Saved\n",
      "Epoch   6 Batch    1/133 train_loss = 6.482 time_per_batch = 0.362157 time_elapsed = 259.875   time_remaining = 4624\n",
      "Model Trained and Saved\n",
      "Epoch   6 Batch  101/133 train_loss = 6.560 time_per_batch = 0.351953 time_elapsed = 297.876   time_remaining = 4459\n",
      "Model Trained and Saved\n",
      "Epoch   7 Batch    1/133 train_loss = 6.480 time_per_batch = 0.365651 time_elapsed = 312.274   time_remaining = 4620\n",
      "Model Trained and Saved\n",
      "Epoch   7 Batch  101/133 train_loss = 6.561 time_per_batch = 0.343226 time_elapsed = 350.962   time_remaining = 4302\n",
      "Model Trained and Saved\n",
      "Epoch   8 Batch    1/133 train_loss = 6.480 time_per_batch = 0.336769 time_elapsed = 365.417   time_remaining = 4210\n",
      "Model Trained and Saved\n",
      "Epoch   8 Batch  101/133 train_loss = 6.563 time_per_batch = 0.376495 time_elapsed = 403.880   time_remaining = 4669\n",
      "Model Trained and Saved\n",
      "Epoch   9 Batch    1/133 train_loss = 6.480 time_per_batch = 0.359514 time_elapsed = 418.338   time_remaining = 4447\n",
      "Model Trained and Saved\n",
      "Epoch   9 Batch  101/133 train_loss = 6.560 time_per_batch = 0.341448 time_elapsed = 456.781   time_remaining = 4189\n",
      "Model Trained and Saved\n",
      "Epoch  10 Batch    1/133 train_loss = 6.477 time_per_batch = 0.345231 time_elapsed = 471.477   time_remaining = 4224\n",
      "Model Trained and Saved\n",
      "Epoch  10 Batch  101/133 train_loss = 6.563 time_per_batch = 0.342286 time_elapsed = 508.413   time_remaining = 4154\n",
      "Model Trained and Saved\n",
      "Epoch  11 Batch    1/133 train_loss = 6.477 time_per_batch = 0.343997 time_elapsed = 523.007   time_remaining = 4163\n",
      "Model Trained and Saved\n",
      "Epoch  11 Batch  101/133 train_loss = 6.559 time_per_batch = 0.349016 time_elapsed = 561.174   time_remaining = 4189\n",
      "Model Trained and Saved\n",
      "Epoch  12 Batch    1/133 train_loss = 6.480 time_per_batch = 0.347660 time_elapsed = 576.278   time_remaining = 4161\n",
      "Model Trained and Saved\n",
      "Epoch  12 Batch  101/133 train_loss = 6.560 time_per_batch = 0.361679 time_elapsed = 614.330   time_remaining = 4293\n",
      "Model Trained and Saved\n",
      "Epoch  13 Batch    1/133 train_loss = 6.482 time_per_batch = 0.356416 time_elapsed = 628.975   time_remaining = 4219\n",
      "Model Trained and Saved\n",
      "Epoch  13 Batch  101/133 train_loss = 6.561 time_per_batch = 0.333664 time_elapsed = 666.950   time_remaining = 3916\n",
      "Model Trained and Saved\n",
      "Epoch  14 Batch    1/133 train_loss = 6.481 time_per_batch = 0.339566 time_elapsed = 681.517   time_remaining = 3974\n",
      "Model Trained and Saved\n",
      "Epoch  14 Batch  101/133 train_loss = 6.559 time_per_batch = 0.346689 time_elapsed = 719.457   time_remaining = 4023\n",
      "Model Trained and Saved\n",
      "Epoch  15 Batch    1/133 train_loss = 6.480 time_per_batch = 0.352588 time_elapsed = 734.266   time_remaining = 4080\n",
      "Model Trained and Saved\n",
      "Epoch  15 Batch  101/133 train_loss = 6.560 time_per_batch = 0.339494 time_elapsed = 771.888   time_remaining = 3894\n",
      "Model Trained and Saved\n",
      "Epoch  16 Batch    1/133 train_loss = 6.478 time_per_batch = 0.347648 time_elapsed = 786.944   time_remaining = 3976\n",
      "Model Trained and Saved\n",
      "Epoch  16 Batch  101/133 train_loss = 6.559 time_per_batch = 0.344861 time_elapsed = 825.878   time_remaining = 3910\n",
      "Model Trained and Saved\n",
      "Epoch  17 Batch    1/133 train_loss = 6.482 time_per_batch = 0.343493 time_elapsed = 840.771   time_remaining = 3883\n",
      "Model Trained and Saved\n",
      "Epoch  17 Batch  101/133 train_loss = 6.561 time_per_batch = 0.341977 time_elapsed = 878.995   time_remaining = 3832\n",
      "Model Trained and Saved\n",
      "Epoch  18 Batch    1/133 train_loss = 6.481 time_per_batch = 0.341800 time_elapsed = 894.031   time_remaining = 3819\n",
      "Model Trained and Saved\n",
      "Epoch  18 Batch  101/133 train_loss = 6.558 time_per_batch = 0.333210 time_elapsed = 932.748   time_remaining = 3689\n",
      "Model Trained and Saved\n",
      "Epoch  19 Batch    1/133 train_loss = 6.478 time_per_batch = 0.340856 time_elapsed = 948.026   time_remaining = 3763\n",
      "Model Trained and Saved\n",
      "Epoch  19 Batch  101/133 train_loss = 6.561 time_per_batch = 0.339281 time_elapsed = 986.649   time_remaining = 3711\n",
      "Model Trained and Saved\n",
      "Epoch  20 Batch    1/133 train_loss = 6.478 time_per_batch = 0.334240 time_elapsed = 1001.606   time_remaining = 3645\n",
      "Model Trained and Saved\n",
      "Epoch  20 Batch  101/133 train_loss = 6.560 time_per_batch = 0.353458 time_elapsed = 1040.364   time_remaining = 3819\n",
      "Model Trained and Saved\n",
      "Epoch  21 Batch    1/133 train_loss = 6.478 time_per_batch = 0.344706 time_elapsed = 1055.588   time_remaining = 3714\n",
      "Model Trained and Saved\n",
      "Epoch  21 Batch  101/133 train_loss = 6.561 time_per_batch = 0.337796 time_elapsed = 1094.367   time_remaining = 3605\n",
      "Model Trained and Saved\n",
      "Epoch  22 Batch    1/133 train_loss = 6.479 time_per_batch = 0.359169 time_elapsed = 1109.630   time_remaining = 3822\n",
      "Model Trained and Saved\n",
      "Epoch  22 Batch  101/133 train_loss = 6.561 time_per_batch = 0.340268 time_elapsed = 1148.992   time_remaining = 3586\n",
      "Model Trained and Saved\n",
      "Epoch  23 Batch    1/133 train_loss = 6.479 time_per_batch = 0.360761 time_elapsed = 1164.600   time_remaining = 3791\n",
      "Model Trained and Saved\n",
      "Epoch  23 Batch  101/133 train_loss = 6.561 time_per_batch = 0.366767 time_elapsed = 1204.321   time_remaining = 3817\n",
      "Model Trained and Saved\n",
      "Epoch  24 Batch    1/133 train_loss = 6.479 time_per_batch = 0.355606 time_elapsed = 1220.150   time_remaining = 3689\n",
      "Model Trained and Saved\n",
      "Epoch  24 Batch  101/133 train_loss = 6.561 time_per_batch = 0.342108 time_elapsed = 1258.888   time_remaining = 3515\n",
      "Model Trained and Saved\n",
      "Epoch  25 Batch    1/133 train_loss = 6.480 time_per_batch = 0.349939 time_elapsed = 1274.960   time_remaining = 3584\n",
      "Model Trained and Saved\n",
      "Epoch  25 Batch  101/133 train_loss = 6.557 time_per_batch = 0.348498 time_elapsed = 1313.920   time_remaining = 3534\n",
      "Model Trained and Saved\n",
      "Epoch  26 Batch    1/133 train_loss = 6.478 time_per_batch = 0.356355 time_elapsed = 1329.767   time_remaining = 3602\n",
      "Model Trained and Saved\n",
      "Epoch  26 Batch  101/133 train_loss = 6.561 time_per_batch = 0.352849 time_elapsed = 1369.009   time_remaining = 3531\n",
      "Model Trained and Saved\n",
      "Epoch  27 Batch    1/133 train_loss = 6.478 time_per_batch = 0.368302 time_elapsed = 1385.350   time_remaining = 3674\n",
      "Model Trained and Saved\n",
      "Epoch  27 Batch  101/133 train_loss = 6.557 time_per_batch = 0.366205 time_elapsed = 1425.197   time_remaining = 3616\n",
      "Model Trained and Saved\n",
      "Epoch  28 Batch    1/133 train_loss = 6.478 time_per_batch = 0.338158 time_elapsed = 1441.485   time_remaining = 3328\n",
      "Model Trained and Saved\n",
      "Epoch  28 Batch  101/133 train_loss = 6.519 time_per_batch = 0.357155 time_elapsed = 1481.857   time_remaining = 3479\n",
      "Model Trained and Saved\n",
      "Epoch  29 Batch    1/133 train_loss = 6.386 time_per_batch = 0.364481 time_elapsed = 1497.785   time_remaining = 3539\n",
      "Model Trained and Saved\n",
      "Epoch  29 Batch  101/133 train_loss = 6.063 time_per_batch = 0.359897 time_elapsed = 1537.937   time_remaining = 3458\n",
      "Model Trained and Saved\n",
      "Epoch  30 Batch    1/133 train_loss = 5.900 time_per_batch = 0.365025 time_elapsed = 1554.107   time_remaining = 3495\n",
      "Model Trained and Saved\n",
      "Epoch  30 Batch  101/133 train_loss = 5.878 time_per_batch = 0.361116 time_elapsed = 1594.640   time_remaining = 3422\n",
      "Model Trained and Saved\n",
      "Epoch  31 Batch    1/133 train_loss = 5.736 time_per_batch = 0.352369 time_elapsed = 1610.795   time_remaining = 3327\n",
      "Model Trained and Saved\n",
      "Epoch  31 Batch  101/133 train_loss = 5.711 time_per_batch = 0.363849 time_elapsed = 1651.166   time_remaining = 3399\n",
      "Model Trained and Saved\n",
      "Epoch  32 Batch    1/133 train_loss = 5.563 time_per_batch = 0.365500 time_elapsed = 1667.777   time_remaining = 3403\n",
      "Model Trained and Saved\n",
      "Epoch  32 Batch  101/133 train_loss = 5.553 time_per_batch = 0.365010 time_elapsed = 1708.550   time_remaining = 3362\n",
      "Model Trained and Saved\n",
      "Epoch  33 Batch    1/133 train_loss = 5.411 time_per_batch = 0.351551 time_elapsed = 1725.070   time_remaining = 3226\n",
      "Model Trained and Saved\n",
      "Epoch  33 Batch  101/133 train_loss = 5.424 time_per_batch = 0.371788 time_elapsed = 1765.622   time_remaining = 3375\n",
      "Model Trained and Saved\n",
      "Epoch  34 Batch    1/133 train_loss = 5.283 time_per_batch = 0.360574 time_elapsed = 1782.350   time_remaining = 3261\n",
      "Model Trained and Saved\n",
      "Epoch  34 Batch  101/133 train_loss = 5.319 time_per_batch = 0.349742 time_elapsed = 1823.434   time_remaining = 3128\n",
      "Model Trained and Saved\n",
      "Epoch  35 Batch    1/133 train_loss = 5.192 time_per_batch = 0.384563 time_elapsed = 1840.699   time_remaining = 3427\n",
      "Model Trained and Saved\n",
      "Epoch  35 Batch  101/133 train_loss = 5.206 time_per_batch = 0.360130 time_elapsed = 1881.558   time_remaining = 3173\n",
      "Model Trained and Saved\n",
      "Epoch  36 Batch    1/133 train_loss = 5.090 time_per_batch = 0.353481 time_elapsed = 1898.447   time_remaining = 3103\n",
      "Model Trained and Saved\n",
      "Epoch  36 Batch  101/133 train_loss = 5.116 time_per_batch = 0.355554 time_elapsed = 1939.367   time_remaining = 3085\n",
      "Model Trained and Saved\n",
      "Epoch  37 Batch    1/133 train_loss = 4.999 time_per_batch = 0.364238 time_elapsed = 1956.508   time_remaining = 3149\n",
      "Model Trained and Saved\n",
      "Epoch  37 Batch  101/133 train_loss = 5.018 time_per_batch = 0.362574 time_elapsed = 1996.968   time_remaining = 3098\n",
      "Model Trained and Saved\n",
      "Epoch  38 Batch    1/133 train_loss = 4.919 time_per_batch = 0.343754 time_elapsed = 2013.532   time_remaining = 2926\n",
      "Model Trained and Saved\n",
      "Epoch  38 Batch  101/133 train_loss = 4.945 time_per_batch = 0.349412 time_elapsed = 2054.049   time_remaining = 2939\n",
      "Model Trained and Saved\n",
      "Epoch  39 Batch    1/133 train_loss = 4.843 time_per_batch = 0.359250 time_elapsed = 2070.986   time_remaining = 3010\n",
      "Model Trained and Saved\n",
      "Epoch  39 Batch  101/133 train_loss = 4.870 time_per_batch = 0.349547 time_elapsed = 2111.274   time_remaining = 2894\n",
      "Model Trained and Saved\n",
      "Epoch  40 Batch    1/133 train_loss = 4.774 time_per_batch = 0.340813 time_elapsed = 2128.082   time_remaining = 2810\n",
      "Model Trained and Saved\n",
      "Epoch  40 Batch  101/133 train_loss = 4.806 time_per_batch = 0.343539 time_elapsed = 2169.577   time_remaining = 2798\n",
      "Model Trained and Saved\n",
      "Epoch  41 Batch    1/133 train_loss = 4.720 time_per_batch = 0.350242 time_elapsed = 2187.171   time_remaining = 2842\n",
      "Model Trained and Saved\n",
      "Epoch  41 Batch  101/133 train_loss = 4.748 time_per_batch = 0.356659 time_elapsed = 2227.244   time_remaining = 2858\n",
      "Model Trained and Saved\n",
      "Epoch  42 Batch    1/133 train_loss = 4.668 time_per_batch = 0.367051 time_elapsed = 2244.257   time_remaining = 2929\n",
      "Model Trained and Saved\n",
      "Epoch  42 Batch  101/133 train_loss = 4.703 time_per_batch = 0.343588 time_elapsed = 2286.052   time_remaining = 2707\n",
      "Model Trained and Saved\n",
      "Epoch  43 Batch    1/133 train_loss = 4.613 time_per_batch = 0.366077 time_elapsed = 2304.322   time_remaining = 2873\n",
      "Model Trained and Saved\n",
      "Epoch  43 Batch  101/133 train_loss = 4.652 time_per_batch = 0.360854 time_elapsed = 2346.227   time_remaining = 2796\n",
      "Model Trained and Saved\n",
      "Epoch  44 Batch    1/133 train_loss = 4.564 time_per_batch = 0.341795 time_elapsed = 2365.398   time_remaining = 2637\n",
      "Model Trained and Saved\n",
      "Epoch  44 Batch  101/133 train_loss = 4.605 time_per_batch = 0.363777 time_elapsed = 2407.401   time_remaining = 2770\n",
      "Model Trained and Saved\n",
      "Epoch  45 Batch    1/133 train_loss = 4.520 time_per_batch = 0.347278 time_elapsed = 2426.122   time_remaining = 2633\n",
      "Model Trained and Saved\n",
      "Epoch  45 Batch  101/133 train_loss = 4.565 time_per_batch = 0.383017 time_elapsed = 2467.367   time_remaining = 2865\n",
      "Model Trained and Saved\n",
      "Epoch  46 Batch    1/133 train_loss = 4.498 time_per_batch = 0.339629 time_elapsed = 2485.531   time_remaining = 2530\n",
      "Model Trained and Saved\n",
      "Epoch  46 Batch  101/133 train_loss = 4.518 time_per_batch = 0.351776 time_elapsed = 2527.453   time_remaining = 2585\n",
      "Model Trained and Saved\n",
      "Epoch  47 Batch    1/133 train_loss = 4.450 time_per_batch = 0.350285 time_elapsed = 2545.678   time_remaining = 2562\n",
      "Model Trained and Saved\n",
      "Epoch  47 Batch  101/133 train_loss = 4.487 time_per_batch = 0.333503 time_elapsed = 2588.186   time_remaining = 2406\n",
      "Model Trained and Saved\n",
      "Epoch  48 Batch    1/133 train_loss = 4.416 time_per_batch = 0.363997 time_elapsed = 2606.574   time_remaining = 2614\n",
      "Model Trained and Saved\n",
      "Epoch  48 Batch  101/133 train_loss = 4.454 time_per_batch = 0.369993 time_elapsed = 2649.266   time_remaining = 2620\n",
      "Model Trained and Saved\n",
      "Epoch  49 Batch    1/133 train_loss = 4.383 time_per_batch = 0.348900 time_elapsed = 2667.639   time_remaining = 2459\n",
      "Model Trained and Saved\n",
      "Epoch  49 Batch  101/133 train_loss = 4.431 time_per_batch = 0.347844 time_elapsed = 2710.236   time_remaining = 2417\n",
      "Model Trained and Saved\n",
      "Epoch  50 Batch    1/133 train_loss = 4.342 time_per_batch = 0.346501 time_elapsed = 2728.329   time_remaining = 2396\n",
      "Model Trained and Saved\n",
      "Epoch  50 Batch  101/133 train_loss = 4.390 time_per_batch = 0.369047 time_elapsed = 2769.717   time_remaining = 2515\n",
      "Model Trained and Saved\n",
      "Epoch  51 Batch    1/133 train_loss = 4.315 time_per_batch = 0.356484 time_elapsed = 2788.064   time_remaining = 2418\n",
      "Model Trained and Saved\n",
      "Epoch  51 Batch  101/133 train_loss = 4.370 time_per_batch = 0.347520 time_elapsed = 2831.092   time_remaining = 2322\n",
      "Model Trained and Saved\n",
      "Epoch  52 Batch    1/133 train_loss = 4.285 time_per_batch = 0.344497 time_elapsed = 2850.427   time_remaining = 2291\n",
      "Model Trained and Saved\n",
      "Epoch  52 Batch  101/133 train_loss = 4.333 time_per_batch = 0.337000 time_elapsed = 2891.391   time_remaining = 2207\n",
      "Model Trained and Saved\n",
      "Epoch  53 Batch    1/133 train_loss = 4.263 time_per_batch = 0.344884 time_elapsed = 2909.586   time_remaining = 2248\n",
      "Model Trained and Saved\n",
      "Epoch  53 Batch  101/133 train_loss = 4.305 time_per_batch = 0.334997 time_elapsed = 2952.266   time_remaining = 2150\n",
      "Model Trained and Saved\n",
      "Epoch  54 Batch    1/133 train_loss = 4.230 time_per_batch = 0.363801 time_elapsed = 2970.980   time_remaining = 2323\n",
      "Model Trained and Saved\n",
      "Epoch  54 Batch  101/133 train_loss = 4.277 time_per_batch = 0.338886 time_elapsed = 3012.885   time_remaining = 2130\n",
      "Model Trained and Saved\n",
      "Epoch  55 Batch    1/133 train_loss = 4.200 time_per_batch = 0.352940 time_elapsed = 3031.401   time_remaining = 2206\n",
      "Model Trained and Saved\n",
      "Epoch  55 Batch  101/133 train_loss = 4.255 time_per_batch = 0.367615 time_elapsed = 3074.277   time_remaining = 2261\n",
      "Model Trained and Saved\n",
      "Epoch  56 Batch    1/133 train_loss = 4.186 time_per_batch = 0.360219 time_elapsed = 3093.521   time_remaining = 2204\n",
      "Model Trained and Saved\n",
      "Epoch  56 Batch  101/133 train_loss = 4.233 time_per_batch = 0.353359 time_elapsed = 3136.463   time_remaining = 2127\n",
      "Model Trained and Saved\n",
      "Epoch  57 Batch    1/133 train_loss = 4.156 time_per_batch = 0.345373 time_elapsed = 3155.393   time_remaining = 2067\n",
      "Model Trained and Saved\n",
      "Epoch  57 Batch  101/133 train_loss = 4.199 time_per_batch = 0.374448 time_elapsed = 3198.017   time_remaining = 2204\n",
      "Model Trained and Saved\n",
      "Epoch  58 Batch    1/133 train_loss = 4.140 time_per_batch = 0.338901 time_elapsed = 3217.917   time_remaining = 1983\n",
      "Model Trained and Saved\n",
      "Epoch  58 Batch  101/133 train_loss = 4.175 time_per_batch = 0.345848 time_elapsed = 3260.611   time_remaining = 1989\n",
      "Model Trained and Saved\n",
      "Epoch  59 Batch    1/133 train_loss = 4.110 time_per_batch = 0.342069 time_elapsed = 3279.218   time_remaining = 1956\n",
      "Model Trained and Saved\n",
      "Epoch  59 Batch  101/133 train_loss = 4.157 time_per_batch = 0.356866 time_elapsed = 3321.140   time_remaining = 2005\n",
      "Model Trained and Saved\n",
      "Epoch  60 Batch    1/133 train_loss = 4.094 time_per_batch = 0.353747 time_elapsed = 3340.512   time_remaining = 1976\n",
      "Model Trained and Saved\n",
      "Epoch  60 Batch  101/133 train_loss = 4.136 time_per_batch = 0.360508 time_elapsed = 3382.139   time_remaining = 1978\n",
      "Model Trained and Saved\n",
      "Epoch  61 Batch    1/133 train_loss = 4.069 time_per_batch = 0.354148 time_elapsed = 3401.820   time_remaining = 1931\n",
      "Model Trained and Saved\n",
      "Epoch  61 Batch  101/133 train_loss = 4.114 time_per_batch = 0.360383 time_elapsed = 3444.262   time_remaining = 1929\n",
      "Model Trained and Saved\n",
      "Epoch  62 Batch    1/133 train_loss = 4.040 time_per_batch = 0.347431 time_elapsed = 3463.757   time_remaining = 1848\n",
      "Model Trained and Saved\n",
      "Epoch  62 Batch  101/133 train_loss = 4.104 time_per_batch = 0.339320 time_elapsed = 3506.508   time_remaining = 1771\n",
      "Model Trained and Saved\n",
      "Epoch  63 Batch    1/133 train_loss = 4.039 time_per_batch = 0.343487 time_elapsed = 3525.505   time_remaining = 1782\n",
      "Model Trained and Saved\n",
      "Epoch  63 Batch  101/133 train_loss = 4.081 time_per_batch = 0.336092 time_elapsed = 3568.169   time_remaining = 1710\n",
      "Model Trained and Saved\n",
      "Epoch  64 Batch    1/133 train_loss = 4.005 time_per_batch = 0.342736 time_elapsed = 3587.600   time_remaining = 1732\n",
      "Model Trained and Saved\n",
      "Epoch  64 Batch  101/133 train_loss = 4.053 time_per_batch = 0.344032 time_elapsed = 3630.182   time_remaining = 1704\n",
      "Model Trained and Saved\n",
      "Epoch  65 Batch    1/133 train_loss = 4.000 time_per_batch = 0.336167 time_elapsed = 3649.669   time_remaining = 1654\n",
      "Model Trained and Saved\n",
      "Epoch  65 Batch  101/133 train_loss = 4.036 time_per_batch = 0.341892 time_elapsed = 3692.402   time_remaining = 1648\n",
      "Model Trained and Saved\n",
      "Epoch  66 Batch    1/133 train_loss = 3.975 time_per_batch = 0.359858 time_elapsed = 3712.524   time_remaining = 1723\n",
      "Model Trained and Saved\n",
      "Epoch  66 Batch  101/133 train_loss = 4.016 time_per_batch = 0.334001 time_elapsed = 3756.317   time_remaining = 1566\n",
      "Model Trained and Saved\n",
      "Epoch  67 Batch    1/133 train_loss = 3.950 time_per_batch = 0.333107 time_elapsed = 3775.254   time_remaining = 1551\n",
      "Model Trained and Saved\n",
      "Epoch  67 Batch  101/133 train_loss = 3.997 time_per_batch = 0.347566 time_elapsed = 3818.553   time_remaining = 1583\n",
      "Model Trained and Saved\n",
      "Epoch  68 Batch    1/133 train_loss = 3.942 time_per_batch = 0.344894 time_elapsed = 3838.452   time_remaining = 1560\n",
      "Model Trained and Saved\n",
      "Epoch  68 Batch  101/133 train_loss = 3.979 time_per_batch = 0.345636 time_elapsed = 3881.723   time_remaining = 1528\n",
      "Model Trained and Saved\n",
      "Epoch  69 Batch    1/133 train_loss = 3.934 time_per_batch = 0.349954 time_elapsed = 3902.413   time_remaining = 1536\n",
      "Model Trained and Saved\n",
      "Epoch  69 Batch  101/133 train_loss = 3.960 time_per_batch = 0.353599 time_elapsed = 3946.277   time_remaining = 1517\n",
      "Model Trained and Saved\n",
      "Epoch  70 Batch    1/133 train_loss = 3.903 time_per_batch = 0.335497 time_elapsed = 3966.612   time_remaining = 1428\n",
      "Model Trained and Saved\n",
      "Epoch  70 Batch  101/133 train_loss = 3.945 time_per_batch = 0.351983 time_elapsed = 4009.298   time_remaining = 1463\n",
      "Model Trained and Saved\n",
      "Epoch  71 Batch    1/133 train_loss = 3.880 time_per_batch = 0.342528 time_elapsed = 4030.059   time_remaining = 1412\n",
      "Model Trained and Saved\n",
      "Epoch  71 Batch  101/133 train_loss = 3.919 time_per_batch = 0.347638 time_elapsed = 4073.862   time_remaining = 1399\n",
      "Model Trained and Saved\n",
      "Epoch  72 Batch    1/133 train_loss = 3.875 time_per_batch = 0.343044 time_elapsed = 4094.166   time_remaining = 1369\n",
      "Model Trained and Saved\n",
      "Epoch  72 Batch  101/133 train_loss = 3.920 time_per_batch = 0.333102 time_elapsed = 4137.407   time_remaining = 1296\n",
      "Model Trained and Saved\n",
      "Epoch  73 Batch    1/133 train_loss = 3.859 time_per_batch = 0.335587 time_elapsed = 4157.539   time_remaining = 1294\n",
      "Model Trained and Saved\n",
      "Epoch  73 Batch  101/133 train_loss = 3.910 time_per_batch = 0.371785 time_elapsed = 4202.056   time_remaining = 1397\n",
      "Model Trained and Saved\n",
      "Epoch  74 Batch    1/133 train_loss = 3.842 time_per_batch = 0.347008 time_elapsed = 4222.531   time_remaining = 1292\n",
      "Model Trained and Saved\n",
      "Epoch  74 Batch  101/133 train_loss = 3.894 time_per_batch = 0.348730 time_elapsed = 4266.647   time_remaining = 1264\n",
      "Model Trained and Saved\n",
      "Epoch  75 Batch    1/133 train_loss = 3.833 time_per_batch = 0.355568 time_elapsed = 4287.651   time_remaining = 1277\n",
      "Model Trained and Saved\n",
      "Epoch  75 Batch  101/133 train_loss = 3.877 time_per_batch = 0.336415 time_elapsed = 4332.187   time_remaining = 1174\n",
      "Model Trained and Saved\n",
      "Epoch  76 Batch    1/133 train_loss = 3.811 time_per_batch = 0.336984 time_elapsed = 4353.135   time_remaining = 1165\n",
      "Model Trained and Saved\n",
      "Epoch  76 Batch  101/133 train_loss = 3.863 time_per_batch = 0.357480 time_elapsed = 4397.481   time_remaining = 1200\n",
      "Model Trained and Saved\n",
      "Epoch  77 Batch    1/133 train_loss = 3.790 time_per_batch = 0.340311 time_elapsed = 4417.427   time_remaining = 1132\n",
      "Model Trained and Saved\n",
      "Epoch  77 Batch  101/133 train_loss = 3.851 time_per_batch = 0.350351 time_elapsed = 4461.738   time_remaining = 1130\n",
      "Model Trained and Saved\n",
      "Epoch  78 Batch    1/133 train_loss = 3.785 time_per_batch = 0.369895 time_elapsed = 4482.830   time_remaining = 1181\n",
      "Model Trained and Saved\n",
      "Epoch  78 Batch  101/133 train_loss = 3.836 time_per_batch = 0.360241 time_elapsed = 4527.139   time_remaining = 1114\n",
      "Model Trained and Saved\n",
      "Epoch  79 Batch    1/133 train_loss = 3.771 time_per_batch = 0.352054 time_elapsed = 4547.653   time_remaining = 1077\n",
      "Model Trained and Saved\n",
      "Epoch  79 Batch  101/133 train_loss = 3.828 time_per_batch = 0.368205 time_elapsed = 4591.758   time_remaining = 1090\n",
      "Model Trained and Saved\n",
      "Epoch  80 Batch    1/133 train_loss = 3.751 time_per_batch = 0.354621 time_elapsed = 4612.793   time_remaining = 1038\n",
      "Model Trained and Saved\n",
      "Epoch  80 Batch  101/133 train_loss = 3.798 time_per_batch = 0.346584 time_elapsed = 4656.517   time_remaining = 979\n",
      "Model Trained and Saved\n",
      "Epoch  81 Batch    1/133 train_loss = 3.746 time_per_batch = 0.357213 time_elapsed = 4677.740   time_remaining = 998\n",
      "Model Trained and Saved\n",
      "Epoch  81 Batch  101/133 train_loss = 3.790 time_per_batch = 0.341330 time_elapsed = 4722.301   time_remaining = 919\n",
      "Model Trained and Saved\n",
      "Epoch  82 Batch    1/133 train_loss = 3.723 time_per_batch = 0.356228 time_elapsed = 4743.137   time_remaining = 948\n",
      "Model Trained and Saved\n",
      "Epoch  82 Batch  101/133 train_loss = 3.778 time_per_batch = 0.333971 time_elapsed = 4786.990   time_remaining = 855\n",
      "Model Trained and Saved\n",
      "Epoch  83 Batch    1/133 train_loss = 3.719 time_per_batch = 0.339274 time_elapsed = 4808.399   time_remaining = 857\n",
      "Model Trained and Saved\n",
      "Epoch  83 Batch  101/133 train_loss = 3.754 time_per_batch = 0.335351 time_elapsed = 4852.782   time_remaining = 814\n",
      "Model Trained and Saved\n",
      "Epoch  84 Batch    1/133 train_loss = 3.710 time_per_batch = 0.344597 time_elapsed = 4874.432   time_remaining = 825\n",
      "Model Trained and Saved\n",
      "Epoch  84 Batch  101/133 train_loss = 3.750 time_per_batch = 0.347916 time_elapsed = 4919.405   time_remaining = 798\n",
      "Model Trained and Saved\n",
      "Epoch  85 Batch    1/133 train_loss = 3.685 time_per_batch = 0.366520 time_elapsed = 4940.606   time_remaining = 829\n",
      "Model Trained and Saved\n",
      "Epoch  85 Batch  101/133 train_loss = 3.739 time_per_batch = 0.344789 time_elapsed = 4985.042   time_remaining = 745\n",
      "Model Trained and Saved\n",
      "Epoch  86 Batch    1/133 train_loss = 3.679 time_per_batch = 0.349837 time_elapsed = 5006.589   time_remaining = 744\n",
      "Model Trained and Saved\n",
      "Epoch  86 Batch  101/133 train_loss = 3.734 time_per_batch = 0.359769 time_elapsed = 5052.075   time_remaining = 730\n",
      "Model Trained and Saved\n",
      "Epoch  87 Batch    1/133 train_loss = 3.671 time_per_batch = 0.331826 time_elapsed = 5073.236   time_remaining = 662\n",
      "Model Trained and Saved\n",
      "Epoch  87 Batch  101/133 train_loss = 3.719 time_per_batch = 0.348323 time_elapsed = 5119.410   time_remaining = 660\n",
      "Model Trained and Saved\n",
      "Epoch  88 Batch    1/133 train_loss = 3.652 time_per_batch = 0.336497 time_elapsed = 5141.353   time_remaining = 627\n",
      "Model Trained and Saved\n",
      "Epoch  88 Batch  101/133 train_loss = 3.715 time_per_batch = 0.342726 time_elapsed = 5186.384   time_remaining = 604\n",
      "Model Trained and Saved\n",
      "Epoch  89 Batch    1/133 train_loss = 3.643 time_per_batch = 0.354055 time_elapsed = 5207.787   time_remaining = 612\n",
      "Model Trained and Saved\n",
      "Epoch  89 Batch  101/133 train_loss = 3.695 time_per_batch = 0.344120 time_elapsed = 5252.535   time_remaining = 561\n",
      "Model Trained and Saved\n",
      "Epoch  90 Batch    1/133 train_loss = 3.638 time_per_batch = 0.343060 time_elapsed = 5274.548   time_remaining = 548\n",
      "Model Trained and Saved\n",
      "Epoch  90 Batch  101/133 train_loss = 3.683 time_per_batch = 0.345540 time_elapsed = 5319.621   time_remaining = 517\n",
      "Model Trained and Saved\n",
      "Epoch  91 Batch    1/133 train_loss = 3.618 time_per_batch = 0.355155 time_elapsed = 5342.481   time_remaining = 520\n",
      "Model Trained and Saved\n",
      "Epoch  91 Batch  101/133 train_loss = 3.673 time_per_batch = 0.348028 time_elapsed = 5388.355   time_remaining = 474\n",
      "Model Trained and Saved\n",
      "Epoch  92 Batch    1/133 train_loss = 3.612 time_per_batch = 0.342551 time_elapsed = 5410.591   time_remaining = 456\n",
      "Model Trained and Saved\n",
      "Epoch  92 Batch  101/133 train_loss = 3.675 time_per_batch = 0.366666 time_elapsed = 5454.400   time_remaining = 451\n",
      "Model Trained and Saved\n",
      "Epoch  93 Batch    1/133 train_loss = 3.600 time_per_batch = 0.352150 time_elapsed = 5475.690   time_remaining = 422\n",
      "Model Trained and Saved\n",
      "Epoch  93 Batch  101/133 train_loss = 3.665 time_per_batch = 0.350140 time_elapsed = 5519.342   time_remaining = 384\n",
      "Model Trained and Saved\n",
      "Epoch  94 Batch    1/133 train_loss = 3.588 time_per_batch = 0.341089 time_elapsed = 5540.234   time_remaining = 363\n",
      "Model Trained and Saved\n",
      "Epoch  94 Batch  101/133 train_loss = 3.645 time_per_batch = 0.357621 time_elapsed = 5585.265   time_remaining = 345\n",
      "Model Trained and Saved\n",
      "Epoch  95 Batch    1/133 train_loss = 3.584 time_per_batch = 0.334080 time_elapsed = 5606.944   time_remaining = 311\n",
      "Model Trained and Saved\n",
      "Epoch  95 Batch  101/133 train_loss = 3.637 time_per_batch = 0.329750 time_elapsed = 5651.713   time_remaining = 274\n",
      "Model Trained and Saved\n",
      "Epoch  96 Batch    1/133 train_loss = 3.563 time_per_batch = 0.340241 time_elapsed = 5672.783   time_remaining = 272\n",
      "Model Trained and Saved\n",
      "Epoch  96 Batch  101/133 train_loss = 3.615 time_per_batch = 0.343235 time_elapsed = 5716.633   time_remaining = 240\n",
      "Model Trained and Saved\n",
      "Epoch  97 Batch    1/133 train_loss = 3.564 time_per_batch = 0.344260 time_elapsed = 5737.903   time_remaining = 229\n",
      "Model Trained and Saved\n",
      "Epoch  97 Batch  101/133 train_loss = 3.623 time_per_batch = 0.345497 time_elapsed = 5782.865   time_remaining = 195\n",
      "Model Trained and Saved\n",
      "Epoch  98 Batch    1/133 train_loss = 3.562 time_per_batch = 0.350653 time_elapsed = 5804.613   time_remaining = 187\n",
      "Model Trained and Saved\n",
      "Epoch  98 Batch  101/133 train_loss = 3.605 time_per_batch = 0.333413 time_elapsed = 5849.435   time_remaining = 144\n",
      "Model Trained and Saved\n",
      "Epoch  99 Batch    1/133 train_loss = 3.541 time_per_batch = 0.355442 time_elapsed = 5871.468   time_remaining = 142\n",
      "Model Trained and Saved\n",
      "Epoch  99 Batch  101/133 train_loss = 3.598 time_per_batch = 0.354584 time_elapsed = 5917.671   time_remaining = 106\n",
      "Model Trained and Saved\n",
      "Epoch 100 Batch    1/133 train_loss = 3.532 time_per_batch = 0.331551 time_elapsed = 5939.869   time_remaining = 88\n",
      "Model Trained and Saved\n",
      "Epoch 100 Batch  101/133 train_loss = 3.587 time_per_batch = 0.345711 time_elapsed = 5984.295   time_remaining = 57\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batches = get_batches(corpus_int, batch_size, seq_length)\n",
    "num_batches = len(batches)\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Num Batches per Epoche : {}, Total Epochs : {}\".format(num_batches, num_epochs))\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        \n",
    "        for batch_index, (x, y) in enumerate(batches):\n",
    "            \n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            feed_dict = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed_dict)\n",
    "            \n",
    "            if batch_index % 100 == 0:\n",
    "                time_elapsed   = time.time() - start_time\n",
    "                time_per_batch = time.time() - batch_start_time\n",
    "                num_batches_remaining = (num_epochs - epoch) * num_batches + num_batches - batch_index \n",
    "                print('Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f} time_per_batch = {:3f} time_elapsed = {:.3f}   time_remaining = {:.0f}'.format(\n",
    "                    epoch + 1,\n",
    "                    batch_index + 1,\n",
    "                    num_batches,\n",
    "                    train_loss,\n",
    "                    time_per_batch,\n",
    "                    time_elapsed,\n",
    "                    num_batches_remaining * time_per_batch))\n",
    "                \n",
    "                # save model every 100 batches\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, save_dir)\n",
    "                print('Model Trained and Saved')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text\n",
    "### Pick a Random Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word with some randomness\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Graph and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "一阵轻风吹过，屋檐下的铃铛迎风而响，绿色的衣角轻轻飘起，仿佛也带着几分笑意；清脆的铃声，随着风儿飘然而上，回荡在天地之间。\n",
      "\n",
      "    焚香谷一走的人亦出现，他看在禅室僧袍之中，却没有说话，却也有些赞叹，哪里今天听不懂这追面对下。\n",
      "\n",
      "    张小凡顿时吓了一跳，低头不语，不过是不了几个呢。\n",
      "\n",
      "    水月大师，对自己柔和的声音缓缓站起，道：“是啊只是……”\n",
      "\n",
      "    云易岚道：“什么大商尊者搬张肉花，你说的是奇怪的……”\n",
      "\n",
      "    李洵嘴角边有身形，笑容冲着田不易微笑道：“不用说了，我们狐族之中之强，就大此心夺命声中，我们生至了最后。你有没有交好，正是南疆”\n",
      "\n",
      "    陆雪琪点了点头，道∶是好说说，若是这黑棍非同小可，将小环面色鬼气森森？他这主峰黑衣人查看许多属于。不过远之后的尾端之夜\n",
      "\n",
      "    张小凡不言语，却浓烈的异样是成是惨白的冷傲，清亮的身子临死之后，道玄真人笑道：“世人怔，你们的意思是，我可虽然明白。”那青年缓缓向小环道：“不错，去这个风回峰重宝，你略知一二。张小凡道……吗？”\n",
      "\n",
      "    田不易怔了一下，道：“是奶让他傻问题，来这些家伙从小？就将这个老者蒙在鼓里\n",
      "\n",
      "    你说哪里话，法相、彭师兄知道“我们店家请跟我来过来。”\n",
      "\n",
      "    她摇了摇头，低声道：“师兄，我连使眼色，我竟要跟着她往来孽多日，许久之后所说的结果万人往，又摇了点头，道：“师父，你刚才等人拿来帮我们之命。此处进入我的时候，鬼厉自然不会妄言时的煞气，他你受前患难见真情的图，这修炼鬼王修炼前辈的大红芒就不下定决心，都在此地为小子送终，片刻了道童转身，走去玩而来，对面天空里钟声，悠悠从石壁散去了了。\n",
      "\n",
      "    他的隐隐，夹杂在高大轻烟的三叉路口，山野也慢慢挂着冒迸发出山前一把“万一齐师兄，爷爷这一路上中间一个人，一个身分被我们上场，能做到伤心的感觉，个个法相道：“那个年轻人，不入虎穴，焉得虎子？”\n",
      "\n",
      "    林惊羽的身子一动，道：“多谢尽心甫，说一个修道之这怪滴血仙剑，亦可能让人知晓这电光火石之际，若我在三人意图扯进来了，谁同意自己排队的？”\n",
      "\n",
      "    台上台下，合拢，鬼王衣衫紧咬，衣裳扭曲着，竟是在那七彩鉴上，又发亮的话与一寸：“怎么回事。”\n",
      "\n",
      "    这许多年来，人这份道行还是认真又是去里，但他们却是这一次，更冰冷，慢慢注视，向楼上向一旁处看去。\n",
      "\n",
      "    呵呵孟骥眉头，道：“难道就返身连法上，是残余的正道和齐昊和文敏抚养离开，急匆匆莫名其妙地身影明确的般的鬼王对了。文敏这一惊已然尚轻上前方的人，然后没有看他\n",
      "\n",
      "    ※※※\n",
      "\n",
      "    他田不易犯了很多，太极前头着不可思议。\n",
      "\n",
      "    远方，一片静默了。\n",
      "“公子那丫头不可谷主命您公子”这四字小盒，去看看了下去吧，还是看去。\n",
      "\n",
      "    欢喜间全部和人这妖与人蕴涵着坚持在看着也轻轻和他成婚多少到她的话。\n",
      "\n",
      "    就在残阳，顿时光芒闪动，那只古鼎却都是一抖，转眼间又再度合上了口气道。 \n",
      "\n",
      "    “哇”人，一物跟在自己的身边轻轻走了进去，在眼睛注视着不一次，但碍着、写关头，脸上眼角平淡神情，觉得急迫，却有丝毫肌肉实在明显万分得不屑之极，花容的轻喝，目光扫了一眼鬼先生，道：“施主愚鲁了不好，待道玄师兄盛怒一声，点了点头，低声道：“陆雪琪陆师妹不要对他横眉竖眼的出来，宗主也无意中将另注于”玄相貌异种，沉重的异种黑杖一角。但老人大竹仙倒有了一趟，洒在月光之中，突然一场败著了的腿，先是动了下来，但大师身子仍是看了李洵而过，但也似乎在后边找鬼厉用那么去的意思，这是人烟，为了不少，彷野猪弟子伤亡了…！\n",
      "\n",
      "    石头凛冽怒吼之下\n"
     ]
    }
   ],
   "source": [
    "gen_length = 1000\n",
    "prime_words = '一阵轻风吹过，屋檐下的铃铛迎风而响，绿色的衣角轻轻飘起，仿佛也带着几分笑意；清脆的铃声，随着风儿飘然而上，回荡在天地之间。'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load the saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "    \n",
    "    # Get tensors from loaded graph\n",
    "    input_text = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    gen_sentences = list(jieba.cut(prime_words)) if USE_SPLIT else prime_words.split()\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1 for word in gen_sentences]])})\n",
    "    \n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        # Get predict word\n",
    "        word_probs = probabilities[0][dyn_seq_length-1]\n",
    "        pred_word = pick_word(word_probs, int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "        \n",
    "    # Remove tokens\n",
    "    chapter_text = ''.join(gen_sentences)\n",
    "        \n",
    "    print(chapter_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open('generated_text.txt', \"w\") as text_file:\n",
    "    text_file.write(chapter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
