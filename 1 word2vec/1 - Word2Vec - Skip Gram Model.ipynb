{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using Unary Skip Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import collections\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_URL   = \"http://mattmahoney.net/dc/text8.zip\"      # 下载文件的URL\n",
    "DATA_FOLDER    = \"./data/\"                                  # 存放数据文件的文件夹路径\n",
    "FILE_NAME      = \"text8.zip\"                                # 数据文件的名称\n",
    "EXPECTED_BYTES = 31344016                                   # 文件的 bytes 大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个创建本地文件夹的函数\n",
    "# \n",
    "# 参数\n",
    "# path              : 创建路径\n",
    "#\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "# 获得当前文件的 bytes 大小\n",
    "# \n",
    "# 参数\n",
    "# file_path         : 文件的路径\n",
    "#\n",
    "# return            : 当前文件的大小\n",
    "# \n",
    "def get_bytes(file_path):\n",
    "    \n",
    "    # 获得文件的描述性数据\n",
    "    file_stats = os.stat(file_path)\n",
    "    \n",
    "    # 返回文件的大小\n",
    "    return file_stats.st_size\n",
    "    \n",
    "# 检查数据的大小是否正确，用来检查是否下载了 “ 完整 ” 的数据集\n",
    "# 如果文件大小不符合所期待的大小，则抛出异常\n",
    "# \n",
    "# 参数\n",
    "# file_path         : 文件路径\n",
    "# expected_bytes    : 所期待的文件的大小\n",
    "#\n",
    "def check_bytes(file_path, expected_bytes):\n",
    "    \n",
    "    # 如果不符合期待的文件大小，则抛出异常\n",
    "    assert get_bytes(file_path) == expected_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个下载数据的函数，并检查下载的数据是否完整的被下载了\n",
    "#\n",
    "# 参数\n",
    "# source_url        : 文件 URL 下载路径\n",
    "# download_folder   : 下载到本地 文件夹 的名字\n",
    "# file_name         : 文件名\n",
    "# expected_bytes    : 文件大小\n",
    "#\n",
    "# return            : 文件的路径\n",
    "#\n",
    "def download(download_url   = DOWNLOAD_URL, \n",
    "             data_folder    = DATA_FOLDER, \n",
    "             file_name      = FILE_NAME, \n",
    "             expected_bytes = EXPECTED_BYTES):\n",
    "    \n",
    "    # 如果下载数据的路径不存在的时候，则创建一个\n",
    "    if not os.path.exists(data_folder):\n",
    "        make_dir(data_folder)\n",
    "        \n",
    "    # 下载的数据的路径为文件夹的路径 + 文件名\n",
    "    file_path = data_folder + file_name\n",
    "    \n",
    "    # 如果文件已经存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 检查文件是否完整\n",
    "        if get_bytes(file_path) == expected_bytes:\n",
    "            # 如果完整，则返回该文件的路径，不做接下来的处理了\n",
    "            print(\"Dataset already downloaded.\")\n",
    "            return file_path\n",
    "        else:\n",
    "            # 如果文件不完整，则删除文件，接下来重新下载一次\n",
    "            os.remove(file_path)\n",
    "    \n",
    "    # 从网页上下载数据，下载文件可能会需要一段时间，请耐心等待\n",
    "    print(\"Start downloading the data, the process may take several minutes, please be patient...\")\n",
    "    file_name, _ = urllib.request.urlretrieve( url = download_url, filename = file_path )\n",
    "    \n",
    "    # 检查下载的数据是否完整\n",
    "    check_bytes( file_path, expected_bytes )\n",
    "    \n",
    "    # 返回文件的路径\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded.\n",
      "File downloaded at path ./data/text8.zip\n"
     ]
    }
   ],
   "source": [
    "# 下载数据\n",
    "file_path = download()\n",
    "\n",
    "# 检查数据的完整性\n",
    "check_bytes( file_path, EXPECTED_BYTES )\n",
    "\n",
    "# 下载成功\n",
    "print(\"File downloaded at path\", file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从zip中读取所有的单词\n",
    "# 参数\n",
    "# file_path         : ZIP 文件的路径\n",
    "#\n",
    "# return            : 该文件所包含的所有单词\n",
    "#\n",
    "def read_data(file_path):\n",
    "    with zipfile.ZipFile( file = file_path ) as f:\n",
    "        # namelist : 返回在压缩目录下的所有文件\n",
    "        # read     : 读出文件的 bytes\n",
    "        words = tf.compat.as_str( f.read( f.namelist()[0] ) ).split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole content contains 17005207 words.\n",
      "The first 5 words are : ['anarchism', 'originated', 'as', 'a', 'term'].\n"
     ]
    }
   ],
   "source": [
    "# 读取单词\n",
    "words = read_data(file_path)\n",
    "\n",
    "# 打印读取的单词的长度\n",
    "print( \"The whole content contains {} words.\".format( len(words) ) )\n",
    "\n",
    "# 打印最开始的5个单词\n",
    "print( \"The first 5 words are : {}.\".format( words[:5]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建数据集：将所有单词转换为index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000    # 定义词库的大小为 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建一个 word -> index 的 dictionary，\n",
    "# 以及一个 index -> word 的 reverse_dictionary\n",
    "# 参数\n",
    "# words             : 单词输入，用来创建dictionary\n",
    "# vocab_size        : 词库的大小\n",
    "# \n",
    "# return \n",
    "#   word_index          :  输入的单词序列 转换成的 index 的序列\n",
    "#                            [ 36, 1, 0, 998, ... , 12 ]\n",
    "#   count               :  一个长度为 词库大小 的数组，每个数组的元素为 [word, count]\n",
    "#                            [ ['UNK', 3456789], ['the', 12345], ['a',12344], ... , ]\n",
    "#   dictionary          :  word -> index 的 dictionary\n",
    "#   reverse_dictionary  :  index -> word 的 reverse_dictionary\n",
    "#   \n",
    "def build_dataset(words, vocab_size):\n",
    "    \n",
    "    dictionary = {}              # 初始化空词库\n",
    "    count = [['UNK',-1]]         # 初始化 Unknown 的单词计数为-1\n",
    "    \n",
    "    # 找出出现最频繁的一组单词，加入到count数组中\n",
    "    # 单词的个数为 词库的大小 减一，因为有一个位置已经被 unknown 占据了\n",
    "    count.extend(Counter(words).most_common(vocab_size-1))\n",
    "    \n",
    "    index = 0                    # 用来记录每个单词在词库中的 index\n",
    "    \n",
    "    # 创建一个目录来存放前 1000 个单词的\n",
    "    make_dir(\"processed\")\n",
    "    with open(\"processed/vocab_1000.tsv\", \"w\") as f:\n",
    "        for word, _ in count:\n",
    "            # 遍历 count 来生成 word -> index 的 dictionary\n",
    "            dictionary[word] = index\n",
    "            # 将前 1000 个单词写入文件中\n",
    "            if index < 1000:\n",
    "                f.write(word + '\\n')\n",
    "            index += 1\n",
    "    \n",
    "    # 将所有的从 zip 文件中读取的单词，转换成相对应的 index\n",
    "    # 如果单词存在于词库中，则返回它的index值\n",
    "    # 否则，返回 0 -- UNK\n",
    "    # 最终这个 len(word_index) == len(words)\n",
    "    word_index = [dictionary[word] if word in dictionary else 0 for word in words]\n",
    "    \n",
    "    # word_index 中 0 出现的次数，即是 UNK 单词出现的次数\n",
    "    count[0][1] = word_index.count(0)\n",
    "    \n",
    "    # zip  : zip 函数会对传入的 iterables 进行遍历和组合，\n",
    "    #         最终返回 合并到一起 的 tuple 的数组\n",
    "    #         # zip('ABCD', 'xy') --> Ax By\n",
    "    #\n",
    "    # dict : dict 函数可以从 tuple 的数组中生成一个新的dictionary\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    # 返回 单词的对应的 index 序列，每个单词对应出现的次数表，单词 - index 表， index - 单词 表\n",
    "    return word_index, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words are : [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)].\n",
      "\n",
      "10 first index and word are : 5234/anarchism, 3081/originated, 12/as, 6/a, 195/term, 2/of, 3134/abuse, 46/first, 59/used, 156/against.\n"
     ]
    }
   ],
   "source": [
    "# 计算 word_index, count, dictionary, reverse_dictionary\n",
    "word_index, count, dictionary, reverse_dictionary = build_dataset(words, VOCAB_SIZE)\n",
    "\n",
    "# 可以节省内存\n",
    "del words\n",
    "\n",
    "# 查看一下嘴常见的 5 个单词\n",
    "print(\"5 most common words are : {}.\\n\".format( count[:5] ) )\n",
    "\n",
    "# 看一下 word_index 中的前十个 index 的值 和 对应的单词\n",
    "print(\"10 first index and word are : {}.\".format( ', '.join(\n",
    "    str(index) + '/' +(reverse_dictionary[index]) for index in word_index[:10] ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成训练的 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在单词的 index 序列中，随机生成一些用来训练的 batch\n",
    "# 参数\n",
    "# word_index        : 单词序列的的 index 表示，从中进行取样来生成 batch\n",
    "# batch_size        : 训练 batch 的大小\n",
    "# num_skip          : Skip Gram 模型中，从整个窗口中选取多少个不同的词作为output word\n",
    "# window_size       : 取单词来预测的 window 大小\n",
    "#\n",
    "# return            :\n",
    "#   batch           : 一个 batch size 的 input 数据\n",
    "#   labels          : batch 中每个元素预测的单词\n",
    "#\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(word_index, \n",
    "                   batch_size   = 8, \n",
    "                   num_skip     = 2, \n",
    "                   skip_window  = 1):\n",
    "    \n",
    "    global data_index                             # data_index 为全局变量，每一次取样都会对它的值进行更新\n",
    "    \n",
    "    assert batch_size % num_skip == 0             # 要求从中取出的单词数为 batch 的整除数\n",
    "    assert num_skip <= 2 * skip_window            # 取出来的单词数要小于 2倍 window 的大小\n",
    "    \n",
    "    # shape 接收 int 的 tuple，生成多维的数组\n",
    "    batch  = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    \n",
    "    span = 2 * skip_window + 1                   # 左右两侧的大小为skip window 并 加上中间词 1\n",
    "    buffer = collections.deque(maxlen=span)      # 创建一个大小为 span 的 buffer\n",
    "    \n",
    "    # 如果此时 buffer 取样越界了，则重头开始取样\n",
    "    if data_index + span >= len(word_index):\n",
    "        data_index = ( data_index + span ) % len(word_index)\n",
    "        \n",
    "    # 从 word_index 中读取一个长度为 span 的词序列放入 buffer 中，用来生成 batch\n",
    "    buffer.extend(word_index[data_index:data_index+span])\n",
    "    \n",
    "    for i in range(batch_size // num_skip):        # batch_size/num_skip 是为了取到batch size的个数，我们所需中心词的个数\n",
    "        \n",
    "        # 左右两边的词为 context words\n",
    "        context_words = [w for w in range(span) if span != skip_window]\n",
    "        \n",
    "        # 重洗一下这个 index 序列的顺序，用来随机选取 num_skip 个单词进行训练\n",
    "        random.shuffle(context_words)\n",
    "        words_to_use = collections.deque(context_words)\n",
    "        for j in range(num_skip):\n",
    "            context_word = words_to_use.pop()\n",
    "            batch[i*num_skip+j]  = buffer[skip_window]\n",
    "            labels[i*num_skip+j,0] = buffer[context_word]\n",
    "        \n",
    "        if data_index == len(word_index):       # 如果data_index已经到这个文档的末尾了，我们就从头开始\n",
    "            buffer[:] = word_index[:span]\n",
    "            data_index = span\n",
    "        else:                                   # 把整个window往后面移一个位置\n",
    "            buffer.append(word_index[data_index])\n",
    "            data_index += 1\n",
    "        \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3081 originated -> [5234] anarchism\n",
      "1 3081 originated -> [3081] originated\n",
      "2 12 as -> [5234] anarchism\n",
      "3 12 as -> [3081] originated\n",
      "4 5234 anarchism -> [3081] originated\n",
      "5 5234 anarchism -> [12] as\n",
      "6 3081 originated -> [12] as\n",
      "7 3081 originated -> [5234] anarchism\n"
     ]
    }
   ],
   "source": [
    "# 测试生成一下 batch 和 labels\n",
    "batch, labels = generate_batch(word_index)\n",
    "\n",
    "# 打印出来 input -> output\n",
    "for i in range(8):\n",
    "    print(i, batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i], reverse_dictionary[labels[i,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBBED_SIZE     = 64                 # 词向量的维数\n",
    "NUM_SAMPLED     = 64                 # 进行 negative sampling 的取样个数\n",
    "LEARNING_RATE   = 1.0                # 梯度下降的学习速率\n",
    "BATCH_SIZE      = 128                # 每次训练的 batch 大小\n",
    "VALIDATION_SIZE = 16                 # validation 单词个数，用来看一下这些取样单词之间的 similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    \"\"\"\n",
    "        Placeholders\n",
    "    \"\"\"\n",
    "    \n",
    "    # 训练时的输入 placeholder ：输入为中心词的 index 的 batch\n",
    "    target_words  = tf.placeholder(name='target_words',  shape=[BATCH_SIZE],   dtype=tf.int32)\n",
    "    # 训练时的输出 placeholder ：输出为 context word 的 index 的 batch\n",
    "    context_words = tf.placeholder(name='context_words', shape=[BATCH_SIZE,1], dtype=tf.int32)\n",
    "    \n",
    "    \"\"\"\n",
    "        Variables\n",
    "    \"\"\"\n",
    "    \n",
    "    # 训练时不断的更新这个 embbding lookup table，得到最终的每个单词的词向量\n",
    "    embeddings = tf.Variable(                      # 输入的 Lookup Table\n",
    "        tf.random_uniform(                         # 随机生成的数据\n",
    "            [VOCAB_SIZE,EMBBED_SIZE],              # tensor 的大小为 VOCAB_SIZE * EMBBED_SIZE\n",
    "            -1.0, 1.0))                            # 范围在 -1.0 到 1.0 之间\n",
    "    # 通过在 lookup table 中匹配中心词，找到的 target words 的词向量\n",
    "    embed = tf.nn.embedding_lookup( embeddings, target_words )\n",
    "    # 在训练中会用来更新的权值矩阵 W\n",
    "    nce_weights = tf.Variable( \n",
    "        tf.truncated_normal(\n",
    "            shape=[VOCAB_SIZE, EMBBED_SIZE], \n",
    "            stddev=1.0/(EMBBED_SIZE ** 0.5)) )\n",
    "    # 在训练中用来更新的偏差矩阵 b\n",
    "    nce_bais = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "    \n",
    "    \"\"\"\n",
    "        Loss function\n",
    "    \"\"\"\n",
    "    \n",
    "    # 用 negative sampling 方法来简化计算\n",
    "    loss = tf.reduce_mean( \n",
    "        tf.nn.nce_loss(\n",
    "            inputs=embed,                         # 输入为由 target words 在当前 lookup table 中找到的词向量\n",
    "            labels=context_words,                 # 期待由 target words 能预测出来的 context words\n",
    "            weights=nce_weights,                  # 权值矩阵\n",
    "            biases=nce_bais,                      # 偏差矩阵\n",
    "            num_classes=VOCAB_SIZE,               # 最终分类会得到的类别总数为 所有单词的个数\n",
    "            num_sampled=NUM_SAMPLED) )            # 在 negative sampling 中的取样个数\n",
    "    \n",
    "    \"\"\"\n",
    "        Training step\n",
    "    \"\"\"\n",
    "    \n",
    "    # 利用梯度下降的方法来进行训练减小 loss\n",
    "    train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    \"\"\"\n",
    "        Normalized Embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    # 'x' is [[1, 1, 1]\n",
    "    #         [1, 1, 1]]\n",
    "    # tf.reduce_sum(x, 1, keep_dims=True) ==> [[3], [3]]\n",
    "    # 这里的reduce sum，对每一个单词的 embbeding 词向量进行了求和，得到的一个有 VOCAB SIZE 个元素的数组\n",
    "    # 其中每个元素的值为 开方 后的 所有元素的平方和 的一元矩阵\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    \n",
    "    # 进行归一化处理的词向量 look up table，处理后，每一行的元素的平方和都为 1\n",
    "    normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE      = 128           # 训练 batch 的大小\n",
    "NUM_SKIP        = 2             # Skip Gram 模型中，从整个窗口中选取多少个不同的词作为output word\n",
    "SKIP_WINDOW     = 1             # 取单词来预测的 window 大小\n",
    "NUM_TRAIN_STEPS = 40000         # 总共训练的次数\n",
    "WEIGHTS_FOLDER  = 'processed'   # 权值最后存储的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2000: 113.6\n",
      "Average loss at step 4000:  52.0\n",
      "Average loss at step 6000:  32.6\n",
      "Average loss at step 8000:  22.3\n",
      "Average loss at step 10000:  17.2\n",
      "Average loss at step 12000:  13.8\n",
      "Average loss at step 14000:  11.2\n",
      "Average loss at step 16000:   9.4\n",
      "Average loss at step 18000:   7.8\n",
      "Average loss at step 20000:   7.3\n",
      "Average loss at step 22000:   6.3\n",
      "Average loss at step 24000:   6.2\n",
      "Average loss at step 26000:   6.0\n",
      "Average loss at step 28000:   5.7\n",
      "Average loss at step 30000:   5.2\n",
      "Average loss at step 32000:   5.2\n",
      "Average loss at step 34000:   5.0\n",
      "Average loss at step 36000:   5.0\n",
      "Average loss at step 38000:   4.7\n",
      "Average loss at step 40000:   4.6\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    # 初始化在 graph 中的所有变量\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 总的损失\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # 一步一步的训练模型\n",
    "    for step in range(NUM_TRAIN_STEPS):\n",
    "        \n",
    "        # 生成 input - target batch\n",
    "        target_words_batch, context_words_batch = generate_batch(batch_size=BATCH_SIZE, word_index=word_index)\n",
    "        \n",
    "        # 运行一次 Gradient Descent 来降低 loss\n",
    "        _, batch_loss = sess.run([train_step, loss], feed_dict={\n",
    "            target_words: target_words_batch,\n",
    "            context_words: context_words_batch\n",
    "        })\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        # 每 2000 次运算，打印一下在之前 2000 次计算中的 loss 的平均值\n",
    "        if (step+1) % 2000 == 0:\n",
    "            print(\"Average loss at step {}: {:5.1f}\".format(step+1, total_loss / 2000))\n",
    "            total_loss = 0.0\n",
    "    \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 找到最相似的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 找到最接近的单词\n",
    "def find_nearest_words(word,                                          \n",
    "                       num_nearest_words=20,                          # 想要找到多少个最相近相似的词\n",
    "                       embeddings=final_embeddings,                   # embeddings 的 lookup table\n",
    "                       dictionary=dictionary,                         # word - index 字典\n",
    "                       reverse_dictionary=reverse_dictionary,         # index - word 字典反转\n",
    "                       only_comapre_with_frequent_word=True):         # 只在最常见的词中找相似词么\n",
    "    \n",
    "    # to lower case 输入 word\n",
    "    if word != word.lower():\n",
    "        print('{} will be treated as lower case {}'.format(word, word.lower()))\n",
    "        word = word.lower()\n",
    "    \n",
    "    # 处理异常单词\n",
    "    if word not in dictionary:\n",
    "        print('{} is not recognized in dictionary, treat as UNK'.format(word))\n",
    "        word = 'UNK'\n",
    "    \n",
    "    # Find Word Embedding of Target Word \n",
    "    embed = [ tf.nn.embedding_lookup( embeddings, dictionary[word] ) ]\n",
    "    \n",
    "    # Compute the similarity of this word with all other words in dictionary\n",
    "    similarity = tf.matmul(embeddings, embed, transpose_b=True)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sin = similarity.eval()\n",
    "        # 如果只考虑最频繁的1000个单词\n",
    "        if only_comapre_with_frequent_word:\n",
    "            sin = sin[:1000]\n",
    "        # 按降序进行排序相似度，并将排序后的index放到 sorted index 变量中\n",
    "        sorted_index = sorted(range(len(sin)), key=lambda k: sin[k], reverse=True)\n",
    "        # 将 单词 和 相似度 放在一个 array 中\n",
    "        nearest_words_similarity = [\n",
    "            [reverse_dictionary[index], sin[index][0]] for index in sorted_index[1:num_nearest_words]]\n",
    "    \n",
    "    # Print out result\n",
    "    print(\"For word '{}', the {} nearest words are : \\n\".format(word, num_nearest_words))\n",
    "    for word, similarity in nearest_words_similarity:\n",
    "        print(\"{} \\t: {}\".format(word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For word 'computer', the 20 nearest words are : \n",
      "\n",
      "designed \t: 0.5785603523254395\n",
      "blue \t: 0.5701479315757751\n",
      "function \t: 0.569938063621521\n",
      "great \t: 0.5639290809631348\n",
      "types \t: 0.5579832792282104\n",
      "zero \t: 0.530914843082428\n",
      "chinese \t: 0.5194635987281799\n",
      "revolution \t: 0.5166011452674866\n",
      "letters \t: 0.5131276249885559\n",
      "date \t: 0.5068237781524658\n",
      "collection \t: 0.5063639283180237\n",
      "release \t: 0.4990079998970032\n",
      "if \t: 0.4969874620437622\n",
      "players \t: 0.4943467974662781\n",
      "due \t: 0.4907388687133789\n",
      "southern \t: 0.48938414454460144\n",
      "left \t: 0.4888436794281006\n",
      "where \t: 0.48667147755622864\n",
      "he \t: 0.4859670400619507\n"
     ]
    }
   ],
   "source": [
    "find_nearest_words(\"computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 找到与单词对类似的另外一个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_similar_pair_word(word_pair,                                     # 用来寻找与此 word pair 相似的 word pair\n",
    "                           word,                                          # 给一个词，找与他对应的词\n",
    "                           embeddings=final_embeddings,                   # 词向量的 lookup table\n",
    "                           dictionary=dictionary,                         # word - index 字典\n",
    "                           reverse_dictionary=reverse_dictionary):        # index - word 字典反转)\n",
    "    \n",
    "    # 检查 word pair 的输入是否合理\n",
    "    assert type(word_pair) is list\n",
    "    assert len (word_pair) == 2\n",
    "    \n",
    "    # 寻找 word pair 中两个单词所对应的词向量\n",
    "    word_1 = word_pair[0]\n",
    "    word_2 = word_pair[1]\n",
    "    \n",
    "    if word_1 not in dictionary:\n",
    "        word_1 = \"UNK\"\n",
    "    if word_2 not in dictionary:\n",
    "        word_2 = \"UNK\"\n",
    "        \n",
    "    embedding_1 = tf.nn.embedding_lookup( embeddings, dictionary[word_1] )\n",
    "    embedding_2 = tf.nn.embedding_lookup( embeddings, dictionary[word_2] )\n",
    "    \n",
    "    # 计算 word pair 两个单词之间的 cosine 距离\n",
    "    word_pair_cosine_distance = tf.matmul( [embedding_1], [embedding_2], transpose_b=True )\n",
    "    \n",
    "    # 对于用来寻找 pair word 的单词，找到它的词向量，已经它与所有单词的 cosine 相似度\n",
    "    if word not in dictionary:\n",
    "        word = \"UNK\"\n",
    "    \n",
    "    embed = [tf.nn.embedding_lookup( embeddings, dictionary[word] )]\n",
    "    similarity = tf.matmul(embeddings, embed, transpose_b=True)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # 得到输入 word pair 的 cosine 距离\n",
    "        distance = word_pair_cosine_distance.eval()[0][0]\n",
    "        print(\"Word pair's cosine distance is :\",distance)\n",
    "        \n",
    "        # 求单词与词库所有单词的 similarity 与目标 distance 之间的差异的绝对值，绝对值越小，则说明该单词成对的可能性越大\n",
    "        pair_distance = tf.abs(similarity - distance).eval()\n",
    "        # 按升序排列\n",
    "        sorted_index = sorted(range(len(pair_distance)), key=lambda k: pair_distance[k])\n",
    "        # 取前 20 个 单词作为可能的 word pair 单词\n",
    "        prob_words = [reverse_dictionary[index] for index in sorted_index[:20]]\n",
    "        print(\"Possible {}'s pair words are :\".format(word))\n",
    "        for word in prob_words:\n",
    "            print(\"Word: {} \\t Different distance with word pair: {}\".format(word, pair_distance[dictionary[word]][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word pair's cosine distance is : 0.387206\n",
      "Possible go's pair words are :\n",
      "Word: subdued \t Different distance with word pair: 3.7550926208496094e-06\n",
      "Word: cristina \t Different distance with word pair: 8.07642936706543e-06\n",
      "Word: calibre \t Different distance with word pair: 2.5093555450439453e-05\n",
      "Word: parachute \t Different distance with word pair: 2.6285648345947266e-05\n",
      "Word: picked \t Different distance with word pair: 2.7418136596679688e-05\n",
      "Word: tartan \t Different distance with word pair: 3.3527612686157227e-05\n",
      "Word: abhidharma \t Different distance with word pair: 3.55839729309082e-05\n",
      "Word: physician \t Different distance with word pair: 3.93986701965332e-05\n",
      "Word: shift \t Different distance with word pair: 4.023313522338867e-05\n",
      "Word: magistrates \t Different distance with word pair: 4.51207160949707e-05\n",
      "Word: roadster \t Different distance with word pair: 4.8041343688964844e-05\n",
      "Word: fullness \t Different distance with word pair: 5.2928924560546875e-05\n",
      "Word: apl \t Different distance with word pair: 5.632638931274414e-05\n",
      "Word: reporter \t Different distance with word pair: 6.0439109802246094e-05\n",
      "Word: appellations \t Different distance with word pair: 6.139278411865234e-05\n",
      "Word: hashes \t Different distance with word pair: 6.604194641113281e-05\n",
      "Word: alumina \t Different distance with word pair: 8.106231689453125e-05\n",
      "Word: unwanted \t Different distance with word pair: 8.398294448852539e-05\n",
      "Word: thrown \t Different distance with word pair: 9.459257125854492e-05\n",
      "Word: sultanate \t Different distance with word pair: 0.00010010600090026855\n"
     ]
    }
   ],
   "source": [
    "find_similar_pair_word(['am','was'], 'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
