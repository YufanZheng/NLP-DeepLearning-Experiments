{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LSTM to Generate Next Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(data_file):\n",
    "    \"\"\" Read dataset as list of words\n",
    "    \n",
    "    Args:\n",
    "        data_file: string: path to the dataset\n",
    "        \n",
    "    Return:\n",
    "        words: list(str): list of words\n",
    "    \"\"\"\n",
    "    # Read the dataset as lines\n",
    "    with codecs.open(data_file, 'r', 'utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Convert the lines into words\n",
    "    sents = [line.split() for line in lines]\n",
    "    words = [word for sent in sents for word in sent]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, training dataset contains 71367 words.\n",
      "In total, validation dataset contains 8707 words.\n",
      "In total, test dataset contains 8809 words.\n"
     ]
    }
   ],
   "source": [
    "train_data = './bobsue-data/bobsue.lm.train.txt'\n",
    "test_data  = './bobsue-data/bobsue.lm.test.txt'\n",
    "eval_data  = './bobsue-data/bobsue.lm.dev.txt'\n",
    "\n",
    "train_words = get_words(train_data)\n",
    "test_words  = get_words(test_data)\n",
    "eval_words  = get_words(eval_data)\n",
    "\n",
    "# See how many words in dataset\n",
    "print(\"In total, training dataset contains {} words.\".  format(len(train_words)))\n",
    "print(\"In total, validation dataset contains {} words.\".format(len(eval_words))  )\n",
    "print(\"In total, test dataset contains {} words.\".      format(len(test_words)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole vocabulary contains 1498 words.\n"
     ]
    }
   ],
   "source": [
    "vocab_file = './bobsue-data/bobsue.voc.txt'\n",
    "\n",
    "# Read vocabulary file\n",
    "with codecs.open(vocab_file, 'r', 'utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Parse lines -> vocabulary \n",
    "vocabulary = [line.split()[0] for line in lines if line != '\\n']\n",
    "\n",
    "# Print out information about the vocabulary\n",
    "print(\"Whole vocabulary contains {} words.\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build index -> word and word -> index\n",
    "index_to_word = {key: word for key, word in enumerate(vocabulary)}\n",
    "word_to_index = {word: key for key, word in enumerate(vocabulary)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parsing, the first 10 words' index in training dataset are:\n",
      "[0, 16, 235, 372, 10, 60, 3, 75, 618, 39]\n"
     ]
    }
   ],
   "source": [
    "# Parse words list -> word index \n",
    "train_word_index = [word_to_index[word] for word in train_words]\n",
    "test_word_index  = [word_to_index[word] for word in test_words ]\n",
    "eval_word_index  = [word_to_index[word] for word in eval_words ]\n",
    "print(\"After parsing, the first 10 words' index in training dataset are:\")\n",
    "print(train_word_index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get number of batch data\n",
    "def get_batch(word_index, num_batches, seq_length):\n",
    "    \"\"\" Randomly get several batches of data from whole dataset\n",
    "    \n",
    "    Args:\n",
    "        word_index : list(int): List of index of words\n",
    "        num_batches: int: Number of batches\n",
    "        seq_length : int: sequence length\n",
    "        \n",
    "    Returns:\n",
    "        x_batches  : list(list(int)) :  shape = (num_batches, seq_length)\n",
    "        y_batches  : list(list(int)) :  shape = (num_batches, seq_length)\n",
    "    \"\"\"\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    max_start_index = len(word_index) - seq_length - 1\n",
    "    for _ in range(num_batches):\n",
    "        start = random.randint(0, max_start_index )\n",
    "        x_input  = word_index[ start   : start+seq_length   ]\n",
    "        y_output = word_index[ start+1 : start+seq_length+1 ]\n",
    "        \n",
    "        x_batches.append(x_input)\n",
    "        y_batches.append(y_output)\n",
    "    \n",
    "    return np.array(x_batches), np.array(y_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_units = 128\n",
    "dropout_keep_prob = 0.7\n",
    "num_layers = 2\n",
    "embed_dim = 128\n",
    "learning_rate = 0.002\n",
    "num_steps = 20000\n",
    "seq_length = 10\n",
    "num_batches = 20\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Placeholders\n",
    "    # Input  Shape = (num_batches, seq_length)\n",
    "    inputs  = tf.placeholder(tf.int64, [None, None], name=\"inputs\" )\n",
    "    # Output Shape = (num_batches, 1)\n",
    "    targets = tf.placeholder(tf.int64, [None, None], name=\"targets\")\n",
    "    \n",
    "    # Calculate Text Attributes\n",
    "    vocab_size = len(vocabulary)\n",
    "    input_shape = tf.shape(inputs)\n",
    "    \n",
    "    # Build LTSM Cells\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_units)\n",
    "    dropout_cell = tf.contrib.rnn.DropoutWrapper(lstm, \\\n",
    "                                                 output_keep_prob=dropout_keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([dropout_cell] * num_layers)\n",
    "    \n",
    "    # Set Initial State\n",
    "    initial_state = cell.zero_state(input_shape[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "    \n",
    "    # Create word embeddings as input of LSTM\n",
    "    embed = tf.contrib.layers.embed_sequence(inputs, vocab_size, embed_dim)\n",
    "    \n",
    "    # Build LSTM\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name=\"final_state\")\n",
    "    \n",
    "    # Take LSTM output and make logits\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    # Calculate the probability of generating each word\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "    \n",
    "    # Define loss\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], input_shape[1]])\n",
    "    )\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "    \n",
    "    # Gradient clipping to avoid exploding gradients\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(targets, tf.argmax(logits,2))\n",
    "    accuracy = tf.reduce_mean( tf.cast( correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Train Loss 7.311821460723877\n",
      "Step 0 Evaluation Loss 7.31191873550415\n",
      "Model Trained and Saved\n",
      "Step 100 Train Loss 7.311689376831055\n",
      "Step 100 Evaluation Loss 7.311807632446289\n",
      "Model Trained and Saved\n",
      "Step 200 Train Loss 7.311511039733887\n",
      "Step 200 Evaluation Loss 7.311671257019043\n",
      "Model Trained and Saved\n",
      "Step 300 Train Loss 7.311434268951416\n",
      "Step 300 Evaluation Loss 7.3114447593688965\n",
      "Model Trained and Saved\n",
      "Step 400 Train Loss 7.311280727386475\n",
      "Step 400 Evaluation Loss 7.3113579750061035\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-b5c58d97e134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         }\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    state = sess.run(initial_state, \\\n",
    "                     {inputs: np.array(train_word_index[:seq_length]).reshape(seq_length, 1)})\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        x_batches, y_batches = get_batch(train_word_index, num_batches, seq_length)\n",
    "\n",
    "        feed_dict = {\n",
    "            inputs: x_batches,\n",
    "            targets: y_batches,\n",
    "            initial_state: state\n",
    "        }\n",
    "\n",
    "        state, _ = sess.run([final_state, train_op], feed_dict)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            train_loss = sess.run(loss, feed_dict)\n",
    "            print(\"Step {} Train Loss {}\".format(step, train_loss))\n",
    "            \n",
    "            x_batches, y_batches = get_batch(eval_word_index, num_batches, seq_length)\n",
    "            feed_dict = {\n",
    "                inputs: x_batches,\n",
    "                targets: y_batches,\n",
    "                initial_state: state\n",
    "            }\n",
    "            eval_loss = sess.run(loss, feed_dict)\n",
    "            print(\"Step {} Evaluation Loss {}\".format(step, eval_loss))\n",
    "            \n",
    "            # save model\n",
    "            def make_dir(path):\n",
    "                try:\n",
    "                    os.mkdir(path)\n",
    "                except OSError:\n",
    "                    pass\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "            save_path = save_dir + '/' + str(step)\n",
    "            make_dir(save_path)\n",
    "            save_path = save_dir + '/' + str(step) + '/' + str(step)\n",
    "            saver.save(sess, save_path)\n",
    "            print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
